[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Preface\nThis is a collection of lecture notes on applied time series analysis and forecasting using the statistical programming language R. Many of these lectures are based on the original notes by Y. R. Gel and C. Cutler for the course STAT-443 Forecasting (University of Waterloo, Canada) adapted and expanded by V. Lyubchich for the course MEES-713 Environmental Statistics 2 (University of Maryland, USA).\nEach lecture starts by listing the learning objectives and required reading materials, with additional references in the text. The notes introduce the methods and give a few examples but are less detailed than the reading materials. The notes do not substitute a textbook.\nThe audience is expected to be familiar with R programming and the following statistical concepts and methods: probability distributions, sampling inference and hypothesis testing, correlation analysis, and regression analysis (including simple and multiple linear regression, mixed-effects models, generalized linear models, and generalized additive models).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Time Series Analysis",
    "section": "Authors",
    "text": "Authors\nVyacheslav Lyubchich is a Research Professor at the University of Maryland Center for Environmental Science.\nYulia R. Gel is a Professor at the Virginia Polytechnic Institute and State University.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Time Series Analysis",
    "section": "Citation",
    "text": "Citation\nLyubchich, V. and Gel, Y. R. (2025) Time Series Analysis: Lecture Notes with Examples in R. Edition 2025-10. https://vlyubchich.github.io/tsar/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Time Series Analysis",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "l01_regression.html",
    "href": "l01_regression.html",
    "title": "1  Review of Linear Regression",
    "section": "",
    "text": "1.1 Diagnostics for the simple linear regression: residual analysis\nAfter this lecture, you should be competent (again) in assessing violations of various assumptions of linear regression models, particularly assumptions about model residuals, by being able to apply visual assessments and formal statistical tests, and interpret the results and effects of the violations.\nObjectives\nReading materials\nAudio overview\nGiven a simple linear regression (SLR) model \\[\nY_{t} = \\beta_{0} + \\beta_{1} X_{t} + \\epsilon_{t},\n\\] where \\(Y_{t}\\) is the dependent variable and \\(X_{t}\\) is the regressor (independent, predictor) variable, \\(t = 1,\\dots,n\\), and \\(n\\) is the sample size.\nThe Gauss–Markov theorem\nIf \\(\\epsilon_{t}\\) are uncorrelated random variables with common variance, then of all possible estimators \\(\\beta^{\\ast}_{0}\\) and \\(\\beta^{\\ast}_{1}\\) that are linear functions of \\(Y_{t}\\), the least squares estimators have the smallest variance.\nThus, the ordinary least squares (OLS) assumptions are:\nA basic technique for investigating the aptness of a regression model is based on analyzing the residuals \\(\\epsilon_{t}\\). In a residual analysis, we attempt to assess the validity of the OLS assumptions by examining the estimated residuals \\(\\hat{\\epsilon}_{1}, \\dots, \\hat{\\epsilon}_{n}\\) to see if they satisfy the imposed conditions. If the model is apt, the observed residuals should reflect the assumptions listed above.\nWe perform our diagnostics analysis from a step-by-step verification of each assumption. We start with visual diagnostics, then proceed with formal tests. A lot of useful diagnostic information may be obtained from a residual plot.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "l01_regression.html#diagnostics-for-the-simple-linear-regression-residual-analysis",
    "href": "l01_regression.html#diagnostics-for-the-simple-linear-regression-residual-analysis",
    "title": "1  Review of Linear Regression",
    "section": "",
    "text": "the residuals \\(\\epsilon_{t}\\) have common variance (\\(\\epsilon_{t}\\) are homoskedastic);\n\nthe residuals \\(\\epsilon_{t}\\) are uncorrelated;\nto provide prediction intervals (PIs), confidence intervals (CIs), and to test hypotheses about the parameters in our model, we also need to assume that\n\nthe residuals \\(\\epsilon_{t}\\) are normally distributed (\\(\\epsilon_{t} \\sim N (0, \\sigma^{ 2} )\\)).\n\n\n\n\n\n\n\nNote\n\n\n\nIf the residuals are independent and identically distributed and normal (\\(\\epsilon_{t} \\sim\\) i.i.d. \\(N(0, \\sigma^{2}\\))), then all three above properties are automatically satisfied. In this case, \\(\\epsilon_{t}\\) are not only uncorrelated but are independent. To be independent is a much stronger property than to be uncorrelated.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile a given model may still have useful predictive value even when the OLS assumptions are violated, the confidence intervals, prediction intervals, and \\(p\\)-values associated with the \\(t\\)-statistics will generally be incorrect when the OLS assumptions do not hold.\n\n\n\n\n\n1.1.1 Homoskedasticity\nWe plot the residuals \\(\\hat{\\epsilon}_{t}\\) vs. time, fitted values \\(\\hat{Y}_{t}\\), and predictor values \\(X_t\\). If the assumption of constant variance is satisfied, \\(\\hat{\\epsilon}_{t}\\) fluctuate around the zero mean with more or less constant amplitude and this amplitude does not change with time, fitted values \\(\\hat{Y}_{t}\\), and predictor values \\(X_t\\).\nIf the (linear) model is not appropriate, the mean of the residuals may be non-constant, i.e., not always 0. Figure 1.1 shows an example of a random pattern that we would like the residuals to have (no systematic patterns).\n\nCodeset.seed(1)\nn = 26; m = 0; s = 522\nx &lt;- ts(rnorm(n, mean = m, sd = s))\nforecast::autoplot(x) + \n    geom_hline(yintercept = 0, lty = 2, col = 4)\n\n\n\n\n\n\nFigure 1.1: A time series plot of ‘ideal’ residuals. These residuals \\(x_t\\) are simulated i.i.d. normal.\n\n\n\n\nWhat can we notice in a residual plot?\n\nChange of variability with time indicates heterogeneity of variance of the residuals.\nObvious lack of symmetry (around 0) in the plot suggests a lack of normality or presence of outliers.\nSystematic trends in the residuals suggest correlations between the residuals or inadequateness of the proposed model.\n\nSometimes it is possible to transform the dependent or independent variables to remedy these problems, i.e., to get rid of the correlated residuals or to stabilize the variance (see Appendix A and Appendix B). Otherwise, we need to change (re-specify) the model.\nA useful technique that can guide us in this process is to plot \\(\\hat{\\epsilon}_{t}\\) vs. \\(\\hat{Y}_{t}\\) and \\(\\hat{\\epsilon}_{t}\\) vs. each predictor \\(X_t\\). Similarly to their time series plot, \\(\\hat{\\epsilon}_{t}\\) should fluctuate around the zero mean with more or less constant amplitude.\n\n\n\n\n\n\nNoteExample: Dishwasher shipments model and patterns in residuals\n\n\n\nFigure 1.2 shows the R code and residuals of a simple linear regression exploring dishwasher shipments (DISH) and private residential investments (RES) for several years.\nHow different are the patterns in Figure 1.2 from those in Figure 1.1?\n\nCodeD &lt;- read.delim(\"data/dish.txt\") %&gt;% \n    rename(Year = YEAR)\nmod1 &lt;- lm(DISH ~ RES, data = D)\np1 &lt;- ggplot(D, aes(x = Year, y = mod1$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ylab(\"Residuals\")\np2 &lt;- ggplot(D, aes(x = mod1$fitted.values, y = mod1$residuals)) + \n    geom_point() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    xlab(\"Fitted values\") +\n    ylab(\"Residuals\")\np3 &lt;- ggplot(D, aes(x = RES, y = mod1$residuals)) + \n    geom_point() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    xlab(\"Residential investments (RES)\") +\n    ylab(\"Residuals\")\np1 + p2 + p3 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 1.2: Estimated residuals plotted vs. time, fitted values, and predictor.\n\n\n\n\nIn Figure 1.2, we see a pattern of residuals increasing in time, and a pattern of lower variability for high fitted values or high residential investments. Thus, the assumption of homoskedasticity is violated. In Figure 1.1, no such patterns are observed.\n\n\n\n1.1.2 Uncorrelatedness\nIt is a deep topic that we shall discuss many times in different variations in the future. When observations are obtained in a time sequence (the topic of time series analysis and our course), there is a high possibility that the errors \\(\\epsilon_{t}\\) are correlated. For instance, if the residual is positive (or negative) for a given day \\(t\\), it is likely that the residual for the following day \\(t+1\\) is also positive (or negative). Such residuals are said to be autocorrelated (i.e., serially correlated). Autocorrelation of many environmental time series is positive.\nWhen the residuals \\(\\epsilon_{t}\\) are related over time, a model for the residuals frequently employed is the first-order autoregressive model, i.e., the AR(1) model.\nThe autoregressive model of the first order, AR(1), is defined as \\[\n\\epsilon_{t} = \\rho \\epsilon_{t - 1} + u_{t},\n\\] where \\(\\rho\\) is the autoregression coefficient (\\(- 1 &lt; \\rho &lt; 1\\)) and \\(u_{t}\\) is an uncorrelated \\(N (0, \\sigma^{2})\\) time series.\nThe model assumes that the residual \\(\\epsilon_{t}\\) at the time \\(t\\) contains a component resulting from the residual \\(\\epsilon_{t - 1}\\) at the time \\(t - 1\\) and a random disturbance \\(u_{t}\\) that is independent of the earlier periods.\nEffects of autocorrelation\nIf the OLS method is employed for the parameter estimation and the residuals \\(\\epsilon_{t}\\) are autocorrelated of the first order, then the consequences are:\n\nThe OLS estimators will still be unbiased, but they no longer have the minimum variance property (see the Gauss–Markov theorem); they tend to be relatively inefficient.\nThe residual mean square error (MSE) can seriously underestimate the true variance of the error terms in the model.\nStandard procedures for CI, PI, and tests using the \\(F\\) and Student’s \\(t\\) distributions are no longer strictly applicable.\n\nFor example, see Section 5.2 in Chatterjee and Simonoff (2013) for more details.\nDurbin–Watson test\nA widely used test for examining whether the residuals in a regression model are correlated is the Durbin–Watson test. This test is based on the AR(1) model for \\(\\epsilon_{t}\\). The one-tail test alternatives are \\[\n\\begin{align}\nH_{0}{:} ~ \\rho = 0 & ~~ vs. ~~ H_{1}{:} ~ \\rho &gt; 0,\\\\\nH_{0}{:} ~ \\rho = 0 & ~~ vs. ~~ H_{1}{:} ~ \\rho &lt; 0,\n\\end{align}\n\\] and the two-tail test is \\[\nH_{0}{:} ~ \\rho = 0 ~~ vs. ~~ H_{1}{:} ~ \\rho \\neq 0.\\\\\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWhen dealing with real data, positive autocorrelation is usually the case.\n\n\nThe Durbin–Watson test statistic DW is based on the differences between the adjacent residuals, \\(\\epsilon_{t} - \\epsilon_{t - 1}\\), and is of the following form: \\[\n\\text{DW} = \\frac{\\sum^{n}_{t = 2} \\left( \\epsilon_{t} - \\epsilon_{t - 1} \\right)^{2}}{\\sum^{n}_{t = 1} \\epsilon^{2}_{t}},\n\\] where \\(\\epsilon_{t}\\) is the regression residual at the time \\(t\\) and \\(n\\) is the number of observations.\nThe DW statistic takes on values in the range \\([0, 4]\\). In fact,\n\nWhen \\(\\epsilon_{t}\\) are positively correlated, adjacent residuals tend to be of similar magnitude so that the numerator of DW will be relatively small or 0.\nWhen \\(\\epsilon_{t}\\) are negatively correlated, adjacent residuals tend to be of similar magnitude but with the opposite sign so that the numerator of DW will be relatively large or equal to 4.\n\nHence, low DW corresponds to positive autocorrelation. Values of DW that tend towards 4 are in the region for negative autocorrelation.\nThe exact action limit for the Durbin–Watson test is difficult to calculate. Hence, the test is used with a lower bound \\(d_{L}\\) and an upper bound \\(d_{U}\\). We may use Table 1.1 as a rule of thumb.\n\n\nTable 1.1: Regions of rejection of the null hypothesis for the Durbin–Watson test\n\n\n\n\n\n\n\n\n\n\nfrom 0 to \\(d_{L}\\)\n\nfrom \\(d_{L}\\) to \\(d_{U}\\)\n\nfrom \\(d_{U}\\) to \\(4 - d_{U}\\)\n\nfrom \\(4 - d_{U}\\) to \\(4 - d_{L}\\)\n\nfrom \\(4 - d_{L}\\) to 4\n\n\nReject \\(H_{0}\\), positive autocorrelation\nNeither accept \\(H_{1}\\) or reject \\(H_{0}\\)\n\nDo not reject \\(H_{0}\\)\n\nNeither accept \\(H_{1}\\) or reject \\(H_{0}\\)\n\nReject \\(H_{0}\\), negative autocorrelation\n\n\n\n\n\nThe critical values \\(d_{L}\\) and \\(d_{U}\\) have been tabulated for combinations of various sample sizes, significance levels, and number of regressors in a model. For large samples, a normal approximation can be used (Chatterjee and Simonoff 2013): \\[\nz = \\left(\\frac{\\text{DW}}{2} - 1 \\right)\\sqrt{n}.\n\\] Statistical software packages usually provide exact \\(p\\)-values based on the null distribution of the test statistic (a linear combination of \\(\\chi^2\\) variables).\n\n\n\n\n\n\nNoteExample: Dishwasher residuals DW test\n\n\n\nApply the Durbin–Watson test to the residuals from the dishwashers example, i.e., DISH vs. RES, using the R package lmtest.\n\nlmtest::dwtest(D$DISH ~ D$RES, alternative = \"greater\")\n\n#&gt; \n#&gt;  Durbin-Watson test\n#&gt; \n#&gt; data:  D$DISH ~ D$RES\n#&gt; DW = 0.6, p-value = 3e-06\n#&gt; alternative hypothesis: true autocorrelation is greater than 0\n\n\nBased on the low \\(p\\)-value we can reject the \\(H_{0}\\): \\(\\rho = 0\\) at the 95% confidence level and accept the alternative \\(H_{1}\\): \\(\\rho &gt; 0\\).\n\n\nRuns test\nDepartures of randomness can take so many forms that no single test for randomness is best for all situations. For instance, one of the most common departures from randomness is the tendency of a sequence to persist in its direction of movement.\nWe can count the number of times a sequence of observations crossed a cut-off line, for example, the median line, and use this information to assess the randomness of \\(\\epsilon_t\\). Alternatively, we count successions of positive or negative differences (see Section 1.4 on the difference sign test). Each such succession is called a run.\nThe formal test is the following. When a sequence of \\(N\\) observations with \\(n\\) observations in positive runs and \\(m\\) observations in negative runs is a random process with independent values generated from a continuous distribution, then the sampling distribution of the number of runs \\(R\\) has the mean and variance \\[\n\\mathrm{E}(R) = \\frac{1 + 2nm}{N}, \\qquad \\sigma^2(R) = \\frac{2nm(2nm-n-m)}{N^2(N-1)},\n\\] where \\(N = n + m\\) is the total sample size.\nThe only assumption for this test is that all sample observations come from a continuous distribution.\nThe two-tail alternative is as follows\n\n\n\\(H_{0}\\): Sequence is generated by a random process;\n\n\\(H_{1}\\): Sequence is generated by a process containing either persistence or frequent changes in direction.\n\nWhen positive autocorrelation (or persistence) is present, \\(R\\) will be small. On the other hand, if the process involves frequent changes in direction (negative autocorrelation or anti-persistence), \\(R\\) will be too large.\nWhen the number of observations is sufficiently large, i.e., \\(N &gt; 30\\), the runs test statistic \\(R\\) is based on the standardized normal test statistic \\[\nz = \\frac{R - \\mathrm{E}(R)}{ \\sigma(R)}.\n\\] Here \\(z\\) follows approximately a standard normal distribution.\nRuns test is easy to interpret. Runs test allows assessing only the first-order serial correlation in the residuals, i.e., to test whether two residuals that are one lag apart are correlated.\n\n\n\n\n\n\nNoteExample: Dishwasher residuals runs test\n\n\n\n\npar(mfrow = c(1, 2))\nlawstat::runs.test(x, plot.it = TRUE)\n\n#&gt; \n#&gt;  Runs Test - Two sided\n#&gt; \n#&gt; data:  x\n#&gt; Standardized Runs Statistic = -0.4, p-value = 0.7\n\nlawstat::runs.test(mod1$residuals, plot.it = TRUE)\n\n#&gt; \n#&gt;  Runs Test - Two sided\n#&gt; \n#&gt; data:  mod1$residuals\n#&gt; Standardized Runs Statistic = -3, p-value = 0.005\n\n\n\n\n\n\n\nFigure 1.3: Runs tests and plots of independent normally distributed simulations \\(x_t\\) and the DISH residuals.\n\n\n\n\nThe \\(p\\)-value for the runs test for the residuals is very low (Figure 1.3), which supports the findings of the DW test that residuals are first-order serially correlated.\nAlternative versions of the runs test are available, e.g., in the package randtests.\n\nrandtests::runs.test(mod1$residuals)\n\n#&gt; \n#&gt;  Runs Test\n#&gt; \n#&gt; data:  mod1$residuals\n#&gt; statistic = -3, runs = 7, n1 = 13, n2 = 13, n = 26, p-value = 0.005\n#&gt; alternative hypothesis: nonrandomness\n\n\n\n\n\n1.1.3 Normality\nThere are two major ways of checking normality. Graphical methods visualize differences between empirical data and theoretical normal distribution. Numerical methods conduct statistical tests on the null hypothesis that the variable is normally distributed.\nGraphical methods\nGraphical methods visualize the data using graphs, such as histograms, stem-and-leaf plots, box plots, etc. For example, Figure 1.4 shows a histogram of the simulated normally distributed data and the residuals from the dishwasher example with superimposed normal curves with the corresponding mean and standard deviation.\n\nCodep1 &lt;- ggplot(data.frame(x = x), aes(x = x)) + \n    geom_histogram(aes(y = after_stat(density)), binwidth = 300, fill = \"grey50\") +\n    stat_function(fun = dnorm, \n                  args = list(mean = mean(x), sd = sd(x)),\n                  col = 1, lwd = 1.5) +\n    ylab(\"Density\") +\n    ggtitle(\"Random normal values\")\np2 &lt;- ggplot(x, aes(x = mod1$residuals)) + \n    geom_histogram(aes(y = after_stat(density)), binwidth = 300, fill = \"grey50\") +\n    stat_function(fun = dnorm, \n                  args = list(mean = mean(mod1$residuals), sd = sd(mod1$residuals)),\n                  col = 1, lwd = 1.5) +\n    ylab(\"Density\") +\n    ggtitle(\"Model residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 1.4: Histograms of the simulated normally distributed values and estimated regression residuals.\n\n\n\n\nAnother very popular graphical method of assessing normality is the quantile-quantile (Q-Q) plot. The Q-Q plot compares the ordered values of a variable with the corresponding ordered values of the normal distribution.\n\n\n\n\n\n\nNote\n\n\n\nQ-Q plots can also be used to compare sample quantiles with quantiles of other, not normal, distribution (e.g., \\(t\\) or gamma distribution), or to compare quantiles of two samples (to assess if both samples come from the same, unspecified, distribution).\n\n\nLet \\(X\\) be a random variable having the property that the equation \\[\n\\Pr \\left( X \\leqslant x \\right) = \\alpha\n\\] has a unique solution \\(x = x_{(\\alpha)}\\) for each \\(0 &lt; \\alpha &lt; 1\\). That is, there exists \\(x_{(\\alpha)}\\) such that \\[\n\\Pr \\left( X \\leqslant x_{(\\alpha)} \\right) = \\alpha\n\\tag{1.1}\\] and no other value of \\(x\\) satisfies Equation 1.1. Then we will call \\(x_{(\\alpha)}\\) the \\(\\alpha\\)th (population) quantile of \\(X\\). Note that any normal distribution has this uniqueness property. If we consider a standard normal \\(Z \\sim N(0, 1)\\), then some well-known quantiles are:\n\n\n\\(z_{(0.5)} = 0\\) (the median), qnorm(0.5, mean = 0, sd = 1)\n\n\n\\(z_{(0.05)} = -1.645\\) and \\(z_{(0.95)} = 1.645\\)\n\n\n\\(z_{(0.025)} = -1.96\\) and \\(z_{(0.975)} = 1.96\\)\n\n\nWe call the 0.25th, 0.5th, 0.75th quantiles the first, the second, and the third quartiles, respectively. The quartiles divide our data into 4 equal parts.\nNow suppose \\(X \\sim N (\\mu, \\sigma^{2})\\). By standardizing to \\(Z \\sim N(0, 1)\\), we obtain \\[\n\\alpha = \\Pr \\left( X \\leqslant x_{(\\alpha)} \\right) = \\Pr \\left( \\frac{X - \\mu}{ \\sigma} \\leqslant \\frac{x_{(\\alpha)} - \\mu}{\\sigma} \\right) = \\Pr \\left( Z \\leqslant \\frac{x_{(\\alpha)} - \\mu}{ \\sigma} \\right) .\n\\]\nWe also have \\(\\alpha = \\Pr (Z \\leqslant z_{(\\alpha)} )\\) by definition. It follows that \\[\nz_{(\\alpha)} = \\frac{x_{(\\alpha)} - \\mu}{ \\sigma} ~~~~ \\mbox{and hence} ~~~~ x_{(\\alpha)} = \\sigma z_{(\\alpha)} + \\mu.\n\\]\nThus, if \\(X\\) is truly normal, a plot of the quantiles of \\(X\\) vs. the quantiles of the standard normal distribution should yield a straight line. A plot of the quantiles of \\(X\\) vs. the quantiles of \\(Z\\) is called a Q-Q plot.\nEstimating quantiles from data\nLet \\(X_{1}, \\dots, X_{n}\\) be a sequence of observations. Ideally, \\(X_{1}, \\dots, X_{n}\\) should represent i.i.d. observations but we will be happy if preliminary tests indicate that they are homoskedastic and uncorrelated (see the previous sections). We order them from the smallest to the largest and indicate this using the notation \\[\nX_{(1/n)} &lt; X_{(2/n)} &lt; X_{(3/n)} &lt; \\dots &lt; X_{\\left((n - 1)/n\\right)} &lt; X_{(n/n)}.\n\\]\nThe above ordering assumes no ties, but ties can be quite common in data, even continuous data, because of rounding. As long as the proportion of ties is small, this method can be used.\nNote that the proportion of observations less than or equal to \\(X_{(k/n)}\\) is exactly \\(k/n\\). Hence \\(X_{(k/n)}\\), called the \\(k\\)th sample quantile, is an estimate of the population quantile \\(x_{(k/n)}\\).\nThe normal Q-Q plot is obtained by plotting the sample quantiles vs. the quantiles of the standard normal distribution. The base-R function qqnorm() produces a normal Q-Q plot of data and the function qqline() adds a line that passes through the first and third quartiles. The R package ggplot2 has analogous functions ggplot2::stat_qq() and ggplot2::stat_qq_line(). The R packages car and ggpubr also draw Q-Q plots, but also add a point-wise confidence envelope with their functions car::qqPlot() (base-R plot) and ggpubr::ggqqplot() (ggplot-type plot).\n\n\n\n\n\n\nNoteExample: Dishwasher residuals normal Q-Q plot\n\n\n\nFigure 1.5 shows the Q-Q plots of the residuals from the dishwasher example and the simulated normal data with the same mean and standard deviation.\n\nCodep1 &lt;- ggpubr::ggqqplot(x) + \n    ggtitle(\"Random normal values\") +\n    xlab(\"Standard normal quantiles\")\np2 &lt;- ggpubr::ggqqplot(mod1$residuals) + \n    ggtitle(\"Model residuals\") +\n    xlab(\"Standard normal quantiles\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 1.5: Normal Q-Q plots of the normally distributed simulated values \\(x_t\\) and the dishwasher residuals.\n\n\n\n\nBoth Q-Q plots in Figure 1.5 show a good correspondence of the sample quantiles with theoretical normal quantiles, providing no sufficient evidence against normality of the underlying distributions.\n\n\nAlthough visually appealing, these graphical methods do not provide objective criteria to determine the normality of variables.\nShapiro–Wilk normality test\nOne of the most popular numerical methods for assessing normality is the Shapiro–Wilk (SW) test:\n\n\n\\(H_0\\): the sample data come from a normally distributed population;\n\n\\(H_1\\): the population is not normally distributed).\n\nThe SW test is the ratio of the best estimator of the variance to the usual corrected sum of squares estimator of the variance. It has been originally constructed by considering the regression of ordered sample values on corresponding expected normal order statistics. The SW statistic is given by \\[\n\\mbox{SW} = \\frac{\\left(\\sum a_{i} x_{(i)} \\right)^{2}}{\\sum \\left(x_{i} - \\bar{x} \\right)^{2}},\n\\] where \\(x_{(i)}\\) are the ordered sample values (\\(x_{(1)}\\) is the smallest) and the \\(a_{i}\\) are constants generated from the means, variances, and covariances of the order statistics of a sample of size \\(n\\) from a normal distribution. The SW statistic lies between 0 and 1. If the SW statistic is close to 1, this indicates the normality of the data. The SW statistic requires the sample size \\(n\\) to be between 7 and 2000.\n\n\n\n\n\n\nNoteExample: Dishwasher residuals normality test\n\n\n\nBased on the \\(p\\)-values below, we cannot reject the null hypothesis of normality in both cases.\n\nshapiro.test(x)\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  x\n#&gt; W = 0.9, p-value = 0.08\n\nshapiro.test(mod1$residuals)\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  mod1$residuals\n#&gt; W = 1, p-value = 0.8\n\n\n\n\n\n1.1.4 Summary of the simple linear regression residual diagnostics\n\nThe residuals do not have a constant mean.\nThe residuals do not seem to have a constant variance.\nThe residuals are positively correlated.\nThe residuals look normally distributed (but the SW statistic might be affected by the serial correlation of the residuals).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "l01_regression.html#multiple-linear-regression",
    "href": "l01_regression.html#multiple-linear-regression",
    "title": "1  Review of Linear Regression",
    "section": "\n1.2 Multiple linear regression",
    "text": "1.2 Multiple linear regression\nHere we consider a case of \\(p\\) explanatory variables \\[\nY_{t} = \\beta_{0} + \\beta_{1} X_{t,1} + \\dots + \\beta_{p} X_{t,p} + \\epsilon_{t} \\quad (t = 1,\\dots,n).\n\\]\nThis can be expressed more compactly in a matrix notation as \\[\n\\boldsymbol{Y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\n\\] where \\(\\boldsymbol{Y} = (Y_{1}, \\dots, Y_{n})^{\\top}\\), \\(\\boldsymbol{\\beta} = (\\beta_{0} , \\dots, \\beta_{p})^{\\top}\\), \\(\\boldsymbol{\\epsilon} = (\\epsilon_{1} , \\dots, \\epsilon_{n})^{\\top}\\); \\(\\boldsymbol{X}\\) is an \\(n \\times (p + 1)\\) design matrix \\[\n\\boldsymbol{X} = \\left(\n\\begin{array}{cccc}\n1 & X_{1,1} & \\dots & X_{1,p} \\\\\n1 & X_{2,1}& \\dots & X_{2,p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & X_{n,1}& \\dots & X_{n,p}\n\\end{array}\n\\right).\n\\]\nHere the historical dataset for the dependent variable consists of the observations \\(Y_{1}, \\dots, Y_{n}\\); the historical dataset for the independent variables consists of the observations in the matrix \\(\\boldsymbol{X}\\).\nMinimizing \\(SSE =(\\boldsymbol{Y} - \\boldsymbol{X} \\hat{\\boldsymbol{\\beta}})^{\\top} (\\boldsymbol{Y} - \\boldsymbol{X} \\hat{\\boldsymbol{\\beta}})\\) yields the least squares solutions \\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^{\\top} \\boldsymbol{Y}\n\\] for non-singular \\(\\boldsymbol{X}^{\\top}\\boldsymbol{X}\\).\nThe forecast of a future value \\(Y_{t}\\) is then given by \\[\n\\hat{Y}_{t} = \\boldsymbol{x}^{\\top}_{t} \\hat{\\boldsymbol{\\beta}},\n\\] where \\(\\boldsymbol{x}_{t}\\) is a (column) vector at the time \\(t\\).\nUnder the OLS assumptions (recall them), we obtain \\[\n\\mathrm{var} \\left( \\hat{\\beta}_{j} \\right) = \\sigma^{2} \\left( \\boldsymbol{X }^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj},\n\\] where the \\(\\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj}\\) denotes the \\(j\\)th diagonal element of \\(\\left( \\boldsymbol{X }^{\\top} \\boldsymbol{X} \\right)^{-1}\\).\nThis yields \\[\ns.e. \\left( \\hat{\\beta}_{j} \\right) = \\hat{\\sigma} \\sqrt{ \\left( \\boldsymbol{X }^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj}}.\n\\]\nNote that here the degrees of freedom (d.f.) are \\(n - (p + 1) = n - p - 1\\). (The number of estimated parameters for the independent variables is \\(p\\), plus one for the intercept, i.e., \\(p + 1\\).)\nUnder the OLS assumptions, a \\(100(1 - \\alpha)\\)% confidence interval for the parameter \\(\\beta_{j}\\) (\\(j = 0, 1, \\dots, p\\)) is given by \\[\n\\begin{split}\n\\hat{\\beta}_{j} &\\pm t_{\\alpha / 2, n - (p+1)} s.e.\\left( \\hat{\\beta}_{j} \\right) \\text{ or} \\\\\n\\hat{\\beta}_{j} &\\pm t_{\\alpha / 2, n - (p+1)} \\hat{\\sigma} \\sqrt{\\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj}}.\n\\end{split}\n\\tag{1.2}\\]\nTypically, \\(s.e.(\\hat{\\beta}_{j})\\) is available directly from the R output, so Equation 1.2 is calculated automatically.\nUnder the OLS assumptions, it can be shown that \\[\n\\mathrm{var} \\left( Y_{t} - \\hat{Y}_{t} \\right) = \\sigma^{2} \\left( \\boldsymbol{x}^{\\top}_{t} \\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{- 1} \\boldsymbol{x}_{t} + 1 \\right),\n\\] yielding a \\(100(1 - \\alpha)\\)% prediction interval for \\(Y_{t}\\): \\[\n\\boldsymbol{x}^{\\top}_{t} \\hat{\\boldsymbol{\\beta}} \\pm t_{\\alpha / 2, n-(p+1)} \\hat{\\sigma} \\sqrt{ \\boldsymbol{x}^{\\top}_{t} \\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1}\\boldsymbol{x}_{t} + 1}.\n\\]\nWe usually never perform these calculations by hand and will use the corresponding software functions, e.g., using the function predict(), see an example code below.\nWhat else can we get from the regression output?\nAs in SLR, we will look for the \\(t\\)-statistics and \\(p\\)-values to get an idea about the statistical significance of each of the predictors \\(X_{t,1}, X_{t,2}, \\dots, X_{t,p}\\). The confidence intervals constructed above correspond to individual tests of hypothesis about a parameter, i.e., \\(H_{0}\\): \\(\\beta_{j} = 0\\) vs. \\(H_{1}\\): \\(\\beta_{j} \\neq 0\\).\nWe can also make use of the \\(F\\)-test. The \\(F\\)-test considers all parameters (other than the intercept \\(\\beta_{0}\\)) simultaneously, testing \\[\n\\begin{split}\nH_{0}{:} ~ \\beta_{1} &= \\dots = \\beta_{p} = 0 ~~~ \\text{vs.} \\\\\nH_{1}{:} ~ \\beta_{j} &\\neq 0 ~~~ \\mbox{for at least one} ~~~ j \\in \\{1, \\dots, p \\}.\n\\end{split}\n\\]\nFormally, \\(F_{\\rm{obs}} = \\rm{MSR/MSE}\\) (the ratio of the mean square due to regression and the mean square due to stochastic errors).\nWe reject \\(H_{0}\\) when \\(F_{\\rm{obs}}\\) is too large relative to a cut-off point determined by the degrees of freedom of the \\(F\\)-distribution. The \\(p\\)-value for this \\(F\\)-test is provided in the lm() output. Rejecting \\(H_{0}\\) is equivalent to stating that the model has some explanatory value within the range of the data set, meaning that changes in at least some of the explanatory \\(X\\)-variables correlate to changes in the average value of \\(Y\\).\nRecall that \\[\n\\begin{split}\n\\mathrm{SST} &= \\sum_{i=1}^n(Y_t-\\overline{Y})^2= \\mathrm{SSR} + \\mathrm{SSE},\\\\\n\\mathrm{SSE} &=\\sum_{t=1}^n(Y_t-\\hat{\\beta}_0 - \\hat{\\beta}_1 X_{t,1}-\\dots- \\hat{\\beta}_p X_{t,p})\n\\end{split}\n\\] and, hence, \\[\n\\rm{SSR}=\\rm{SST}-\\rm{SSE}.\n\\]\nTo conclude that the model has a reasonable fit, however, we would additionally like to see a high \\(R^{2}\\) value, where \\[\nR^{2} = \\rm{SSR/SST}\n\\] is the proportion of the total sum of squares explained by the regression.\nSmall \\(R^{2}\\) means that the stochastic fluctuations around the regression line (or prediction equation) are large, making the prediction task difficult, even though there may be a genuine explanatory relationship between the average value \\(\\mathrm{E}(Y)\\) and some of the \\(X\\)-variables.\nAnother criterion to judge the aptness of the obtained model is the adjusted \\(R^2\\): \\[\nR^2_{adj}=1-\\frac{n-1}{n-p}\\left( 1-R^2 \\right).\n\\]\nUnlike \\(R^2\\) itself, \\(R^2_{adj}\\) need not increase if an arbitrary (even useless) predictor is added to the model because of the correction \\((n-1)/(n-p)\\).\n\n\n\n\n\n\nNote\n\n\n\nThe intercept \\(\\beta_{0}\\) is not included in the \\(F\\)-test because there is no explanatory variable associated with it. In other words, \\(\\beta_{0}\\) does not contribute to the regression part of the model.\n\n\n\n\n\n\n\n\nNoteExample: Dishwasher shipments multiple linear regression\n\n\n\nLet us now extend the SLR model that we considered previously and include another potential predictor, the durable goods expenditures (billion of 1972 dollars). The goal is to build a model to predict the unit factory shipments of dishwashers (DISH) vs. private residential investment (RES) and durable goods expenditures (DUR) using a multiple linear regression model (MLR): \\[\nY_{t} = \\beta_{0} + \\beta_{1} X_{t,1} + \\beta_{2} X_{t,2} + \\epsilon_t,\n\\] where \\(X_{t,1}\\) is the private residential investment RES; \\(X_{t,2}\\) is the durable goods expenditures DUR.\nNow we apply the OLS method to estimate the coefficients \\(\\beta_{0}\\), \\(\\beta_{1}\\), and \\(\\beta_{2}\\) using the function lm().\n\nmod2 &lt;- lm(DISH ~ RES + DUR, data = D)\nsummary(mod2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = DISH ~ RES + DUR, data = D)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -643.9 -263.2  -22.1  190.2  920.0 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -1603.09     391.31   -4.10  0.00044 ***\n#&gt; RES            50.97      11.37    4.48  0.00017 ***\n#&gt; DUR            13.77       2.78    4.95  5.2e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 378 on 23 degrees of freedom\n#&gt; Multiple R-squared:  0.877,  Adjusted R-squared:  0.867 \n#&gt; F-statistic: 82.3 on 2 and 23 DF,  p-value: 3.31e-11\n\n\nWe have a high \\(R^2_{adj} =\\) 0.867 and both predictors are statistically significant, but can we already wholeheartedly trust these results? We need to perform the residual diagnostics.\nPlot the estimated residuals \\(\\hat{\\epsilon}_{t} = \\hat{Y}_{t} - Y_{t}\\) vs. the observed \\(Y_{t}\\) (\\(t = 1, 2, \\dots, 26\\) or Year) – see Figure 1.6. The plots show a remaining pattern, with residuals peaking, then declining. The assumption of homoskedasticity is violated. We should update the model so that this pattern is modeled or removed.\n\nCodep1 &lt;- ggplot(D, aes(x = Year, y = mod2$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ylab(\"Residuals\")\np2 &lt;- ggplot(D, aes(x = mod2$fitted.values, y = mod2$residuals)) + \n    geom_point() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    xlab(\"Fitted values\") +\n    ylab(\"Residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 1.6: Residuals vs. time and vs. fitted values.\n\n\n\n\nCheck that the residuals \\(\\epsilon_{t}\\) are uncorrelated (the Durbin–Watson and the runs tests):\n\nlmtest::dwtest(D$DISH ~ D$RES + D$DUR)\n\n#&gt; \n#&gt;  Durbin-Watson test\n#&gt; \n#&gt; data:  D$DISH ~ D$RES + D$DUR\n#&gt; DW = 0.4, p-value = 7e-09\n#&gt; alternative hypothesis: true autocorrelation is greater than 0\n\n\n\nlawstat::runs.test(mod2$residuals, plot.it = FALSE)\n\n#&gt; \n#&gt;  Runs Test - Two sided\n#&gt; \n#&gt; data:  mod2$residuals\n#&gt; Standardized Runs Statistic = -4, p-value = 3e-04\n\n\nBoth the tests reject \\(H_0\\) of no autocorrelation; hence the assumption of uncorrelatedness is violated.\nCheck that the residuals \\(\\epsilon_{t}\\) are normally distributed using the Q-Q plot (Figure 1.7) and Shapiro–Wilk test.\n\nshapiro.test(mod2$residuals)\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  mod2$residuals\n#&gt; W = 1, p-value = 0.9\n\n\n\nCodeggpubr::ggqqplot(mod2$residuals) + \n    xlab(\"Standard normal quantiles\")\n\n\n\n\n\n\nFigure 1.7: Normal Q-Q plot of the multiple regression residuals.\n\n\n\n\nFigure 1.7 and \\(p\\)-value of the Shapiro–Wilk test do not provide evidence against the null hypothesis of normality. The assumption of normality is satisfied.\n\n\n\n1.2.1 Summary of the multiple linear regression residual diagnostics\n\nThe \\(R^{2}\\) has been improved.\nWe do not see a visible improvement in terms of the mean and variance of the residuals.\nThe residuals are still positively correlated.\nThe residuals look normally distributed.\n\nEven though not all OLS assumptions are satisfied we shall consider how to predict the future values of \\(Y\\) and to construct the prediction intervals using R.\nFor example, assume that we need to predict the future unit factory shipments of dishwashers (DISH) based on the private residential investment of 100 billion USD and durable goods expenditures of 150 billion USD.\nSupply new values of independent variables and use the function predict().\n\nnewData &lt;- data.frame(RES = c(100), DUR = c(150))\npredict(mod2, newData, se.fit = TRUE, interval = \"prediction\")\n\n#&gt; $fit\n#&gt;    fit  lwr  upr\n#&gt; 1 5559 4227 6891\n#&gt; \n#&gt; $se.fit\n#&gt; [1] 521\n#&gt; \n#&gt; $df\n#&gt; [1] 23\n#&gt; \n#&gt; $residual.scale\n#&gt; [1] 378",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "l01_regression.html#conclusion",
    "href": "l01_regression.html#conclusion",
    "title": "1  Review of Linear Regression",
    "section": "\n1.3 Conclusion",
    "text": "1.3 Conclusion\nWe have recalled the standard assumptions about residuals of linear regression models. Remember that there are some other assumptions (e.g., about linear independence of predictors) that must be verified. Refer to the reading materials for a complete list.\nThe methods we have used to test the homogeneity of residuals included various residual plots. The normality of residuals can be assessed using histograms or Q-Q plots and statistical tests such as the Shapiro–Wilk normality test.\nThe use of time series in regression presents additional ways to assess patterns in the regression residuals. A plot of residuals vs. time is assessed for homogeneity and absence of trends. Less obvious patterns, such as autocorrelation, can be tested with parametric and nonparametric tests, such as the Durbin–Watson and runs tests.\nThe statistical techniques we will learn aim to model or extract as much information from time series (including the autocorrelation of regression residuals) as possible, such that the remaining series are completely random.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "l01_regression.html#sec-diffsign",
    "href": "l01_regression.html#sec-diffsign",
    "title": "1  Review of Linear Regression",
    "section": "\n1.4 Appendix",
    "text": "1.4 Appendix\nDifference sign test\nThe logic behind the difference sign test is that in a random process, there will be roughly the same number of ups (positive differences between consecutive values, i.e., \\(X_t-X_{t-1}\\)) and downs (negative differences).\nBrockwell and Davis (2002): “The difference-sign test must be used with caution. A set of observations exhibiting a strong cyclic component will pass the difference-sign test for randomness since roughly half of the observations will be points of increase.” We may see (Figure 1.2 and Figure 1.6), it is the case for our residuals, even though we have no cyclic component, but have a rise followed by a decline.\n\nrandtests::difference.sign.test(x)\n\n#&gt; \n#&gt;  Difference Sign Test\n#&gt; \n#&gt; data:  x\n#&gt; statistic = -2, n = 26, p-value = 0.1\n#&gt; alternative hypothesis: nonrandomness\n\nrandtests::difference.sign.test(mod1$residuals)\n\n#&gt; \n#&gt;  Difference Sign Test\n#&gt; \n#&gt; data:  mod1$residuals\n#&gt; statistic = 0.3, n = 26, p-value = 0.7\n#&gt; alternative hypothesis: nonrandomness\n\nrandtests::difference.sign.test(mod2$residuals)\n\n#&gt; \n#&gt;  Difference Sign Test\n#&gt; \n#&gt; data:  mod2$residuals\n#&gt; statistic = -0.3, n = 26, p-value = 0.7\n#&gt; alternative hypothesis: nonrandomness\n\n\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn. John Wiley & Sons, Hoboken, NJ, USA\n\n\nChatterjee S, Simonoff JS (2013) Handbook of regression analysis. John Wiley & Sons, Hoboken, NJ, USA",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "l02_tsintro.html",
    "href": "l02_tsintro.html",
    "title": "\n2  Introduction to Time Series Analysis\n",
    "section": "",
    "text": "2.1 Time series and its components\nThe goal of this lecture is to introduce time series and their common components. You should become confident in identifying and characterizing trends, seasonal and other variability based on visual analysis of time series plots and plots of autocorrelation functions.\nObjectives\nReading materials\nAudio overview\n‘A time series is a set of observations \\(Y_t\\), each one being recorded at a specific time \\(t\\)’ (Brockwell and Davis 2002). The index \\(t\\) will typically refer to some standard unit of time, e.g., seconds, hours, days, weeks, months, or years.\n‘A time series is a collection of observations made sequentially through time’ (Chatfield 2000).\nA stochastic process is a sequence of random variables \\(Y_t\\), \\(t = 1, 2, \\dots\\) indexed by time \\(t\\), which can be written as \\(Y_t\\), \\(t \\in [1,T]\\). A time series is a realization of a stochastic process.\nWe shall frequently use the term time series to mean both the data and the process.\nLet’s try to describe the patterns we see in Figure 2.1, Figure 2.2, and Figure 2.3.\nCodep &lt;- forecast::autoplot(TSstudio::Michigan_CS) + \n    xlab(\"\") + \n    ylab(\"Index\")\nplotly::ggplotly(p)\n\n\n\n\n\n\nFigure 2.1: Monthly index of consumer sentiment, the University of Michigan Consumer Survey (1966:Q1 = 100).\nCodeggplot2::autoplot(JohnsonJohnson) + \n    xlab(\"\") + \n    ylab(\"Earnings per share (USD)\")\n\n\n\n\n\n\nFigure 2.2: Quarterly earnings (dollars) per Johnson & Johnson share, 1960–1980.\nCodeggplot2::autoplot(MASS::accdeaths) + \n    xlab(\"\") + \n    ylab(\"Number of accidental deaths\")\n\n\n\n\n\n\nFigure 2.3: Monthly totals of accidental deaths in the USA, 1973–1978.\nIn this course, we will be interested in modeling time series to learn more about their properties and to forecast (predict) future values of \\(Y_t\\), i.e., values of \\(Y_t\\) for \\(t\\) beyond the end of the data set. Typically, we will use the historical data (or some appropriate subset of it) to build our forecasting models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "l02_tsintro.html#decomposition-of-time-series",
    "href": "l02_tsintro.html#decomposition-of-time-series",
    "title": "\n2  Introduction to Time Series Analysis\n",
    "section": "\n2.2 Decomposition of time series",
    "text": "2.2 Decomposition of time series\nA time series can generally be expressed as a sum or product of four distinct components: \\[\nY_t = M_t + S_t + C_t + \\epsilon_t\n\\] or \\[\nY_t = M_t \\cdot S_t \\cdot C_t \\cdot \\epsilon_t,\n\\] where\n\n\n\\(M_t\\) is the trend, representing the average change (change in the mean) in the time series over time. Examples of trends are:\\(M_t = \\beta_0\\) (constant over time, we usually refer to this case as ‘no trend’);\\(M_t = \\beta_0 + \\beta_1t\\) (linear increase or decrease over time);\\(M_t = \\beta_0 + \\beta_1t + \\beta_2t^2\\) (quadratic over time).\n\n\n\\(S_t\\) represents regular periodic fluctuations (a.k.a. seasonality) in the time series. \\(S_t\\) has the property that it is not constant but there exists an integer \\(m \\geqslant 2\\) and scaling factors \\(\\lambda_k &gt; 0\\) such that \\(S_{t+km} = \\lambda_kS_t\\) for \\(1 \\leqslant t \\leqslant m\\) and each \\(k \\geqslant 1\\). The smallest such \\(m\\) is called the period. Periodic time series are quite common and include seasonal variability (the period is 1 year), diurnal (the period is 24 hours), and other cycles such as tides (the period is 12 hours 25 minutes). This component can be modeled using sinusoidal functions or indicator variables.\n\n\\(C_t\\) represents irregular cyclical fluctuations, i.e., sinusoidal-type movements that are of irregular length and are not as predictable as the seasonal component, e.g., El Niño Southern Oscillation (ENSO) and macroeconomic business cycles. We do not explicitly model \\(C_t\\) in this course.\n\n\\(\\epsilon_t\\) is the residual or error and represents the remaining unexplained variation in \\(Y_t\\). In other words, it is a random or stochastic component.\n\nFigure 2.4 illustrates an automatic decomposition, however, because the user has too little control over how the decomposition is done, this function is not recommended. We will talk about the alternatives in the next lecture.\n\nCode# births &lt;- scan(\"http://robjhyndman.com/tsdldata/data/nybirths.dat\")\nbirths &lt;- scan(\"data/nybirths.dat\")\nbirths &lt;- ts(births, frequency = 12, start = c(1946, 1))\nggplot2::autoplot(decompose(births))\n\n\n\n\n\n\nFigure 2.4: Time-series decomposition of the monthly number of births in New York City (thousands), 1946–1959.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "l02_tsintro.html#general-approach-to-time-series-modeling",
    "href": "l02_tsintro.html#general-approach-to-time-series-modeling",
    "title": "\n2  Introduction to Time Series Analysis\n",
    "section": "\n2.3 General approach to time series modeling",
    "text": "2.3 General approach to time series modeling\n\nPlot the series and examine the main features of the graph, checking whether there is\n\na trend,\na seasonal or another periodic component,\nany apparent sharp changes in behavior,\nany outlying observations.\n\n\nRemove the trend and periodic components to get stationary residuals (this step is called detrending and deseasonalizing). Broadly speaking, a time series is said to be stationary if there is\n\nno systematic change in the mean (no trend),\nno systematic change in the variance, and\nno strictly periodic variations.\n\n\nChoose a model to fit the residuals, making use of various sample statistics, including the sample autocorrelation function (ACF).\nForecast the residuals and then invert the transformations to arrive at forecasts of the original series.\n\nThere are two general classes of forecasting models.\nUnivariate time series models include different types of exponential smoothing, trend models, autoregressive models, etc. The characteristic feature of these models is that we need only one time series to start with (\\(Y_t\\)), then we can build a regression of this time series over time (\\(Y_t \\sim t\\)) for estimating the trend or, for example, an autoregressive model (‘auto’ \\(=\\) self). In the autoregressive approach, the current value \\(Y_t\\) is modeled as a function of the past values: \\[\nY_t = f(Y_{t-1}, Y_{t-2}, \\dots, Y_{t-p}) + \\epsilon_t.\n\\tag{2.1}\\] A linear autoregressive model has the following form (assume that function \\(f(\\cdot)\\) is a linear parametric function, with parameters \\(\\phi_0, \\phi_1, \\dots, \\phi_p\\)): \\[\nY_t = \\phi_0 + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\dots +\\phi_p Y_{t-p} + \\epsilon_t.\n\\]\nMultivariate models involve additional covariates (a.k.a. regressors, predictors, or independent variables) \\(X_{1,t}, X_{2,t}, \\dots, X_{k,t}\\). A multivariate time series model can be as simple as the multiple regression \\[\nY_t = f(X_{1,t}, X_{2,t}, \\dots, X_{k,t}) + \\epsilon_t,\n\\] or involve lagged values of the response and predictors: \\[\n\\begin{split}\nY_t = f(&X_{1,t}, X_{2,t}, \\dots, X_{k,t}, \\\\\n&X_{1,t-1}, X_{1,t-2}, \\dots, X_{1,t-q1}, \\\\\n&\\dots\\\\\n&X_{k,t-1}, X_{k,t-2}, \\dots, X_{k,t-qk}, \\\\\n&Y_{t-1}, Y_{t-2}, \\dots, Y_{t-p}) + \\epsilon_t,\n\\end{split}\n\\tag{2.2}\\] where \\(p\\), \\(q1, \\dots, qk\\) are the lags. We can start the analysis with many variables and build a forecasting model as complex as Equation 2.2, but remember that a simpler univariate model may also work well. We should create an appropriate univariate model (like in Equation 2.1) to serve as a baseline, then compare the models’ performance on some out-of-sample data, as described in Chapter 10.\n\n\n\n\n\n\nNoteExample: Identify the time-series components in these plots\n\n\n\n\nCodeggplot2::autoplot(sunspots) + \n    xlab(\"\") + \n    ylab(\"Sunspot number\")\n\n\n\n\n\n\nFigure 2.5: Monthly mean relative sunspot numbers from 1749 to 1983. Collected at Swiss Federal Observatory, Zurich until 1960, then Tokyo Astronomical Observatory. Notice the apparent periodicity, large variability when the series is at a high level, and small variability when the series is at a low level. This series is likely to have no trend over the time we have been able to observe it, though it may have a nearly perfectly periodic component.\n\n\n\n\n\nCodeggplot2::autoplot(lynx) + \n    xlab(\"\") + \n    ylab(\"Number of lynx trappings\")\n\n\n\n\n\n\nFigure 2.6: Annual numbers of lynx trappings for 1821–1934 in Canada. There is a clear cycle of about 10 years in length (the period is 10 years). There is potentially also a longer-term cycle.\n\n\n\n\n\nCodeggplot2::autoplot(Nile) + \n    xlab(\"\") + \n    ylab(bquote('Annual flow  '(10^8~m^3)))\n\n\n\n\n\n\nFigure 2.7: Annual flow of the river Nile at Aswan, 1871–1970. There are signs of nonlinear dynamics or a changepoint.\n\n\n\n\n\nCodeggplot2::autoplot(co2) + \n    xlab(\"\") + \n    ylab(bquote(CO[2]~'concentration (ppm)'))\n\n\n\n\n\n\nFigure 2.8: Monthly Mauna Loa atmospheric CO\\(_2\\) concentration, 1959–1997. There is a strong trend and a strong annual cycle.\n\n\n\n\n\nCodeggplot2::autoplot(Ecdat::Consumption[,\"yd\"]/1000) + \n    xlab(\"\") + \n    ylab(\"Income (thousand 1986 dollars)\")\n\n\n\n\n\n\nFigure 2.9: Quarterly personal disposable income (Canada, 1947–1996). There is a clear increasing trend of a relatively complex shape. Also, the variability increased in the later years.\n\n\n\n\n\nCodeforecast::autoplot(as.ts(Ecdat::SP500[,1])) + \n    xlab(\"\") + \n    ylab(\"Daily return\")\n\n\n\n\n\n\nFigure 2.10: Daily returns (change in log index) on Standard & Poor’s 500 Index, 1981-01 to 1991-04. Business days are used as the time index to avoid data gaps during non-trading days. The mean and variance are constant overall (not increasing or decreasing), but there are clusters of volatility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "l02_tsintro.html#some-simple-time-series-models",
    "href": "l02_tsintro.html#some-simple-time-series-models",
    "title": "\n2  Introduction to Time Series Analysis\n",
    "section": "\n2.4 Some simple time series models",
    "text": "2.4 Some simple time series models\nRecall that a random process is a sequence of random variables, so its model would be the joint distribution of these random variables \\[\nf_1(Y_{t_1}), \\; f_2(Y_{t_1}, Y_{t_2}), \\; f_3(Y_{t_1}, Y_{t_2}, Y_{t_3}), \\dots\n\\]\nWith sample data, usually, we cannot estimate so many parameters. Instead, we use only first- and second-order moments of the joint distributions, i.e., \\(\\mathrm{E}(Y_t)\\) and \\(\\mathrm{E}(Y_tY_{t+h})\\). In the case when all the joint distributions are multivariate normal (MVN), these second-order properties completely describe the sequence (we do not need any other information beyond the first two moments in this case).\n\n\ni.i.d. noise – independent and identically distributed random variables with zero mean. There is no dependence between observations, at any moment. The joint distribution function is \\[\n\\Pr[Y_1\\leqslant y_1, \\dots, Y_n\\leqslant y_n] = \\Pr[Y_1\\leqslant y_1]\\dots \\Pr[Y_n\\leqslant y_n].\n\\] and the forecast is 0 (zero).\n\nbinary process is a case of i.i.d. process with \\(0 &lt; p &lt; 1\\), and \\[\n\\begin{split}\n\\Pr[X_t=1]& = p, \\\\\n\\Pr[X_t=-1]& = 1-p.\n\\end{split}\n\\]\n\n\nrandom walk is a cumulative sum of i.i.d. noise: \\[\nS_t=X_1+X_2+\\dots +X_t = \\sum_{i=1}^t X_i,\n\\] where \\(t=1,2,\\dots\\), and \\(X_t\\) is i.i.d. noise.\n\nwhite noise (WN) is a sequence of uncorrelated random variables, each with zero mean and finite variance \\(\\sigma^2\\). An i.i.d.(0,\\(\\sigma^2\\)) series is also WN(0,\\(\\sigma^2\\)), but not conversely.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "l02_tsintro.html#autocorrelation-function",
    "href": "l02_tsintro.html#autocorrelation-function",
    "title": "\n2  Introduction to Time Series Analysis\n",
    "section": "\n2.5 Autocorrelation function",
    "text": "2.5 Autocorrelation function\nIt is time to give more formal definitions of stationarity. Loosely speaking, a time series \\(X_t\\) (\\(t=0,\\pm 1, \\dots\\)) is stationary if it has statistical properties similar to those of the time-shifted series \\(X_{t+h}\\) for each integer \\(h\\).\nLet \\(X_t\\) be a time series with \\(\\mathrm{E}(X^2_t)&lt;\\infty\\), then the mean function of \\(X_t\\) is \\[\n\\mu_X(t)=\\mathrm{E}(X_t).\n\\]\nThe autocovariance function of \\(X_t\\) is \\[\n\\gamma_X(r,s) = \\mathrm{cov}(X_r,X_s) = \\mathrm{E}[(X_r-\\mu_X(r))(X_s-\\mu_X(s))]\n\\tag{2.3}\\] for all integers \\(r\\) and \\(s\\).\nWeak stationarity\n\\(X_t\\) is (weakly) stationary if \\(\\mu_X(t)\\) is independent of \\(t\\), and \\(\\gamma_X(t+h,t)\\) is independent of \\(t\\) for each \\(h\\) (consider only the first two moments): \\[\n\\begin{split}\n\\mathrm{E}(X_t) &= \\mu, \\\\\n\\mathrm{cov}(X_t, X_{t+h}) &= \\gamma_X(h) &lt; \\infty.\n\\end{split}\n\\tag{2.4}\\]\nStrong stationarity\n\\(X_t\\) is strictly stationary if (\\(X_1, \\dots, X_n\\)) and (\\(X_{1+h}, \\dots, X_{n+h}\\)) have the same joint distribution (consider all moments).\nStrictly stationary \\(X_t\\) with finite variance \\(\\mathrm{E}(X_t^2) &lt;\\infty\\) for all \\(t\\) is also weakly stationary.\nIf \\(X_t\\) is a Gaussian process, then strict and weak stationarity are equivalent (i.e., one form of stationarity implies the other).\n\n\n\n\n\n\nNote\n\n\n\nIn applications, it is usually very difficult if not impossible to verify strict stationarity. So in the vast majority of cases, we are satisfied with the weak stationarity. Moreover, we usually omit the word ‘weak’ and simply talk about stationarity (but have the weak stationary in mind).\n\n\nGiven the second condition of weak stationarity in Equation 2.4, we can write for stationary(!) time series the autocovariance function (ACVF) for the lag \\(h\\): \\[\n\\gamma_X(h)= \\gamma_X(t+h,t) = \\mathrm{cov}(X_{t+h},X_t)\n\\] and the autocorrelation function (ACF), which is the normalized autocovariance: \\[\n\\rho_X(h)=\\frac{\\gamma_X(h)}{\\gamma_X(0)}=\\mathrm{cor}(X_{t+h},X_t).\n\\]\nThe sample autocovariance function is defined as: \\[\n\\hat{\\gamma}_X(h)= n^{-1}\\sum_{t=1}^{n-k}(x_{t+h}- \\bar{x})(x_t - \\bar{x}),\n\\] with \\(\\hat{\\gamma}_X(h) = \\hat{\\gamma}_X(-h)\\) for \\(h = 0,1,\\dots, n-1\\). In R, use acf(X, type = \"covariance\").\nThe sample autocorrelation function is defined as (in R, use acf(X)): \\[\n\\hat{\\rho}_X(h)=\\frac{\\hat{\\gamma}_X(h)}{\\hat{\\gamma}_X(0)}.\n\\]\n\n2.5.1 Properties of autocovariance and autocorrelation functions\n\nLinearity: if \\(\\mathrm{E}(X^2) &lt; \\infty\\), \\(\\mathrm{E}(Y^2) &lt; \\infty\\), \\(\\mathrm{E}(Z^2) &lt; \\infty\\) and \\(a\\), \\(b\\), and \\(c\\) are any real constants, then \\[\n\\mathrm{cov}(aX + bY + c, Z) = a\\mathrm{cov}(X,Z) + b\\mathrm{cov}(Y,Z).\n\\]\n\n\n\\(\\gamma(0) \\geqslant 0\\).\n\n\\(|\\gamma(h)| \\leqslant \\gamma(0)\\) for all \\(h\\).\n\n\\(\\gamma(\\cdot)\\) is even: \\(\\gamma(h) = \\gamma(-h)\\) for all \\(h\\).\n\n\\(\\gamma(\\cdot)\\) is nonnegative definite: \\[\n\\sum_{i,j=1}^na_i \\gamma(i-j)a_j\\geqslant 0,\n\\] for all positive integers \\(n\\) and vectors \\(\\boldsymbol{a} = (a_1, \\dots, a_n)^{\\top}\\) with real-valued elements \\(a_i\\).\nThe autocorrelation function \\(\\rho(\\cdot)\\) has all the properties of autocovariance function plus \\(\\rho(0)=1\\).\nFor i.i.d. noise with finite variance, the sample autocorrelations \\(\\hat{\\rho}(h)\\), \\(h&gt;0\\), are approximately \\(N(0,1/n)\\) for large sample size \\(n\\). Hence, approximately 95% of the sample autocorrelations should fall between the bounds \\(\\pm1.96/\\sqrt{n}\\) (1.96 is the 0.975th quantile of the standard normal distribution) – this is the bound automatically drawn by the R function acf(). Autocorrelations that reach outside this bound are then statistically significant. Note that in R as a rule of thumb the default maximal lag is \\(10 \\log_{10}(n)\\), and the same \\(n\\) is used for the confidence bounds at all lags (however, in reality, samples of different sizes are used for each lag).\n\n\n\n\n\n\n\nNote\n\n\n\nThe last property is yet another tool for testing autocorrelation in a time series, in addition to those listed in Chapter 1. Also, see the Ljung–Box test described below.\n\n\nLjung–Box test\nInstead of testing autocorrelation at each lag, we can apply an overall test by Ljung and Box (1978):\n\\(H_0\\): \\(\\rho(1) = \\rho(2) = \\dots = \\rho(h) = 0\\)\\(H_1\\): \\(\\rho(j) \\neq 0\\) for some \\(j \\in \\{1, 2, \\dots, h\\}\\), where \\(n &gt; h\\).\nThe Ljung–Box test statistic is given by \\[\nQ_h = n(n + 2) \\sum_{j = 1}^h\\frac{\\hat{\\rho}_j^2}{n - j}.\n\\] Under the null hypothesis, \\(Q_h\\) has a \\(\\chi^2\\) distribution with \\(h\\) degrees of freedom. In R, this test is implemented in the function Box.test() with the argument type = \"Ljung-Box\" and in the function tsdiag().\n\n\n\n\n\n\nNoteExample: Compare ACFs of time series with and without a strong trend\n\n\n\nThe time series plot of births in Figure 2.11 shows a trend. The time series is not stationary. Thus, the calculated ACF is mostly useless (it is calculated under the assumption of stationarity), but we notice some periodicity in the ACF – it is a sign of periodicity in the time series itself (seasonality). Need to remove the trend (and seasonality) and repeat the ACF analysis.\nCompare with the plots for accidental deaths in Figure 2.12.\n\nCodep1 &lt;- ggplot2::autoplot(births) + \n    xlab(\"\") +\n    ylab(\"Number of births (thousands)\")\np2 &lt;- forecast::ggAcf(births) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 2.11: Time series plot and sample ACF of the monthly number of births in New York City, 1946–1959.\n\n\n\n\n\nCodep1 &lt;- ggplot2::autoplot(MASS::accdeaths) + \n    xlab(\"\") +\n    ylab(\"Number of accidental deaths\")\np2 &lt;- forecast::ggAcf(MASS::accdeaths) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 2.12: Time series plot and sample ACF of monthly totals of accidental deaths in the USA, 1973–1978.\n\n\n\n\nFigure 2.12 shows a slight trend of declining, then increasing deaths; the seasonal component in this time series is more dominant. The seasonal cycle of 12 months is clearly visible in the ACF.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn base-R plots, the lags in ACF plots are measured in the number of periods. The period information (or frequency, i.e., the number of observations per period) is saved in the format ts in R:\n\nis.ts(births)\n\n#&gt; [1] TRUE\n\nfrequency(births)\n\n#&gt; [1] 12\n\n\nWe don’t have to convert data to this format. If we plot the ACF of just a vector without these attributes, the values are the same, just the labels on the horizontal axis are different (Figure 2.13). For monthly data, as in the example here, the frequency is 12. Hence, here the ACF at lag 0.5 (Figure 2.13 A) means autocorrelation with the lag \\(h=6\\) months (Figure 2.13 B); lag 1 (Figure 2.13 A) corresponds to one whole period of \\(h=12\\) months (Figure 2.13 B), and so on.\n\nCodepar(mar = c(4, 4, 3, 1) + 0.1, mfrow = c(1, 2))\n\nacf(births, lag.max = 37, las = 1, main = \"A) Input is a ts object\")\nacf(as.numeric(births), lag.max = 37, las = 1, main = \"B) Input is a numeric vector\")\n\n\n\n\n\n\nFigure 2.13: In base-R graphics, the x-axis labels in ACF plots differ depending on the input format.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe ts objects in R cannot incorporate varying periods (e.g., different numbers of days or weeks because of leap years), therefore, we recommend using this format only for monthly or annual data. Setting frequency = 365.25 for daily data could be an option (although not very accurate) to accommodate leap years.\nConverting to the ts format usually is not required for analysis. Most of the time we use plain numeric vectors and data frames in R.\nContributed R packages offer additional formats for time series, e.g., see the packages xts and zoo.\n\n\nACF measures the correlation between \\(X_t\\) and \\(X_{t+h}\\). The correlation can be due to a direct connection or through the intermediate steps \\(X_{t+1}, X_{t+2}, \\dots, X_{t+h-1}\\). Partial ACF looks at the correlation between \\(X_t\\) and \\(X_{t+h}\\) once the effect of intermediate steps is removed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "l02_tsintro.html#partial-autocorrelation-function",
    "href": "l02_tsintro.html#partial-autocorrelation-function",
    "title": "\n2  Introduction to Time Series Analysis\n",
    "section": "\n2.6 Partial autocorrelation function",
    "text": "2.6 Partial autocorrelation function\nShumway and Stoffer (2011) and Shumway and Stoffer (2014) provide the following explanation of the concept.\nRecall that if \\(X\\), \\(Y\\), and \\(Z\\) are random variables, then the partial correlation between \\(X\\) and \\(Y\\) given \\(Z\\) is obtained by regressing \\(X\\) on \\(Z\\) to obtain \\(\\hat{X}\\), regressing \\(Y\\) on \\(Z\\) to obtain \\(\\hat{Y}\\), and then calculating \\[\n\\rho_{XY|Z} = \\mathrm{cor}(X - \\hat{X}, Y - \\hat{Y}).\n\\] The idea is that \\(\\rho_{XY|Z}\\) measures the correlation between \\(X\\) and \\(Y\\) with the linear effect of \\(Z\\) removed (or partialled out). If the variables are multivariate normal, then this definition coincides with \\(\\rho_{XY|Z} = \\mathrm{cor}(X,Y | Z)\\).\nThe partial autocorrelation function (PACF) of a stationary process, \\(X_t\\), denoted \\(\\rho_{hh}\\), for \\(h = 1,2,\\dots\\), is \\[\n\\rho_{11} = \\mathrm{cor}(X_t, X_{t+1}) = \\rho(1)\n\\] and \\[\n\\rho_{hh} = \\mathrm{cor}(X_t - \\hat{X}_t, X_{t+h} - \\hat{X}_{t+h}), \\; h\\geqslant 2.\n\\]\nBoth \\((X_t - \\hat{X}_t)\\) and \\((X_{t+h} - \\hat{X}_{t+h})\\) are uncorrelated with \\(\\{ X_{t+1}, \\dots, X_{t+h-1}\\}\\). The PACF, \\(\\rho_{hh}\\), is the correlation between \\(X_{t+h}\\) and \\(X_t\\) with the linear dependence of everything between them (intermediate lags), namely \\(\\{ X_{t+1}, \\dots, X_{t+h-1}\\}\\), on each, removed.\nIn other notations, the PACF for the lag \\(h\\) and predictions \\(P\\) is \\[\n\\rho_{hh} =\n\\begin{cases}\n1 & \\text{if } h = 0 \\\\\n\\mathrm{cor}(X_t, X_{t+1}) = \\rho(1) & \\text{if } h = 1 \\\\\n\\mathrm{cor}(X_t - P(X_t | X_{t+1}, \\dots, X_{t+h-1}), \\\\\n\\quad\\;\\; X_{t+h} - P(X_{t+h} | X_{t+1}, \\dots, X_{t+h-1})) & \\text{if } h &gt; 1.\n\\end{cases}\n\\]\nTo obtain sample estimates of PACF in R, use pacf(X) or acf(X, type = \"partial\").\nCorrelation and partial correlation coefficients measure the strength and direction of the relationship, changing within \\([-1, 1]\\). The percent of explained variance for the case of two variables is measured by squared correlation (r-squared; \\(R^2\\); coefficient of determination) changing within \\([0, 1]\\), so a correlation of 0.2 means only 4% of variance explained by the simple linear relationship (regression). To report a partial correlation, for example, of 0.2 at lag 3, one could say something like ‘The partial autocorrelation at lag 3 (after removing the influence of the intermediate lags 1 and 2) is 0.2’ (depending on the application, the correlation of 0.2 can be considered weak or moderate strength) or ‘After accounting for autocorrelation at intermediate lags 1 and 2, the linear relationship at lag 3 can explain 4% of the remaining variability.’\nThe partial autocorrelation coefficients of i.i.d. time series are asymptotically distributed as \\(N(0,1/n)\\). Hence, for lags \\(h &gt; p\\), where \\(p\\) is the optimal or true order of the partial autocorrelation, \\(\\rho_{hh} &lt; 1.96/\\sqrt{n}\\) (assuming the confidence level 95%). This suggests using as a preliminary estimator of the order \\(p\\) the smallest value \\(m\\) such that \\(\\rho_{hh} &lt; 1.96/\\sqrt{n}\\) for \\(h &gt; m\\).\n\n\n\n\n\n\nNoteExample: ACF and PACF of accdeaths\n\n\n\nBy plotting ACF and PACF (Figure 2.14), we observe that most of the temporal dependence (autocorrelation) in this time series is due to correlation with the immediately preceding values (see PACF at lag 1).\n\nCodep1 &lt;- forecast::ggAcf(MASS::accdeaths) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\np2 &lt;- forecast::ggAcf(MASS::accdeaths, type = \"partial\") + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 2.14: ACF and PACF plots of the monthly accidental deaths.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "l02_tsintro.html#conclusion",
    "href": "l02_tsintro.html#conclusion",
    "title": "\n2  Introduction to Time Series Analysis\n",
    "section": "\n2.7 Conclusion",
    "text": "2.7 Conclusion\nWe have defined components of the time series including trend, periodic component, and unexplained variability (errors, residuals). Our goal will be to model trends and periodicity, extract them from the time series, then extract as much information as possible from the remainder, so the ultimate residuals become white noise.\nWhite noise is a sequence of uncorrelated random variables with zero mean and finite variance. White noise is also an example of a weakly stationary time series. The i.i.d. noise is strictly stationary (all moments of the distribution stay identical through time), and i.i.d. noise with finite variance is also white noise.\nTime-series dependence can be quantified using a (partial) autocorrelation function. We defined (P)ACF for stationary series; R functions also assume stationarity of the time series when calculating ACF or PACF.\nAfter developing several models for modeling and forecasting time series, we can compare them quantitatively in cross-validation. For time series, it is typical to have the testing set (or a validation fold) to be after the training set. Models can be compared based on the accuracy of their point forecasts and interval forecasts.\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton, FL, USA\n\n\nLjung GM, Box GEP (1978) On a measure of lack of fit in time series models. Biometrika 65:297–303. https://doi.org/10.1093/biomet/65.2.297\n\n\nShumway RH, Stoffer DS (2011) Time series analysis and its applications with R examples, 3rd edn. Springer, New York, NY, USA\n\n\nShumway RH, Stoffer DS (2014) Time series analysis and its applications with R examples, 3-EZ. Free Texts in Statistics",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "l03_smoothing.html",
    "href": "l03_smoothing.html",
    "title": "\n3  Smoothing, Detrending, and Deseasonalizing\n",
    "section": "",
    "text": "3.1 Introduction\nThe goal of this lecture is to learn a variety of methods used for trend visualization (such that make a trend in noisy data more apparent), trend modeling (such that can help us to come up with ‘an equation’ for the trend, which could be further studied or used for interpolation, forecasting, or detrending), and detrending (just to remove trend from the series, with or without modeling it). You should be able to choose a method based on the goals of your analysis. We will start with simple and widely used methods that can be further combined into more elaborate algorithms.\nObjectives\nReading materials\nAudio overview\nRecall the classical decomposition \\[\nY_t = M_t + S_t + \\epsilon_t,\n\\tag{3.1}\\] where \\(M_t\\) is a slowly changing function (trend component), \\(\\epsilon_t\\) is stationary random noise component, and \\(S_t\\) is the periodic term with the period \\(m\\geqslant 2\\) (seasonal component) and scaling factors \\(\\lambda_k &gt; 0\\) such that \\(S_{t+km} = \\lambda_kS_t\\) for \\(1 \\leqslant t \\leqslant m\\) and each \\(k \\geqslant 1\\). For identification, we need \\(\\sum_{t=1}^m S_t = 0\\) and \\(\\mathrm{E}(\\epsilon_t)=0\\).\nOur goal is to estimate and extract \\(M_t\\) and \\(S_t\\) with the hope that the residual or noise component \\(\\epsilon_t\\) will turn out to be a stationary time series (Section 3.2–Section 3.5). Alternatively, Box–Jenkins models use difference operators repeatedly to the series \\(Y_t\\) until a stationary time series is obtained (see Section 3.7 on differencing).\nThe seasonal component is said to have constant seasonal variation if the scaling factors satisfy \\(\\lambda_{k} = 1\\) for all \\(k\\). In other words, \\(S_{t+m} = S_{t}\\) for all \\(t\\). The constant variation is an assumption of most modeling techniques we will be using in this course. Unfortunately, many real seasonal time series do not have this constancy property and it is necessary to first perform a variance-stabilizing transformation to modify the original time series into one with constant variation. To some extent, this is a matter of trial and error, where we work from ‘weaker’ transformations to ‘stronger’ transformations as needed. Typically, power transformations are used \\(Y_{t} \\rightarrow Y^{\\lambda}_{t}\\), where \\(0 &lt; \\lambda &lt; 1\\) or, log or even log log transformations, e.g., \\(Y_{t} \\rightarrow \\log Y_{t}\\).\nThe log transformation of \\(Y_t\\) is convenient when we assume not an additive model as Equation 3.1 is, but a multiplicative model as \\[\nY_t = M_t \\cdot S_t \\cdot \\epsilon_t.\n\\tag{3.2}\\] When applying variance-stabilizing log transformation to Equation 3.2, we get an additive result: \\[\n\\log Y_t = \\log M_t + \\log S_t + \\log \\epsilon_t,\n\\tag{3.3}\\] which is now similar to Equation 3.1. We will refer to Equation 3.1 as additive seasonality, and to Equation 3.2 as multiplicative seasonality. Only a few methods can deal with the multiplicative case Equation 3.2 directly; most methods we consider in this course will require us to apply a transformation as in Equation 3.3.\nIn the following sections, we consider methods for the estimation and elimination of \\(M_t\\) (for non-seasonal data, when \\(S_t = 0\\) in the additive case Equation 3.1 or \\(S_t = 1\\) in the multiplicative case Equation 3.2) and of both \\(M_t\\) and \\(S_t\\) (for seasonal data).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Smoothing, Detrending, and Deseasonalizing</span>"
    ]
  },
  {
    "objectID": "l03_smoothing.html#sec-movavg",
    "href": "l03_smoothing.html#sec-movavg",
    "title": "\n3  Smoothing, Detrending, and Deseasonalizing\n",
    "section": "\n3.2 Finite moving average smoothing",
    "text": "3.2 Finite moving average smoothing\nThe non-seasonal model with a trend: \\[\nY_t = M_t + \\epsilon_t,\n\\] where \\(\\mathrm{E}(\\epsilon_t)=0\\) and \\(t=1,\\dots,n\\).\nLet \\(q\\) be a nonnegative integer (\\(q \\in \\mathbb{N}^{+}\\)) and consider the two-sided moving average of the series \\(Y_t\\): \\[\n\\begin{split}\nW_t &= \\frac{1}{2q+1}\\sum_{j=-q}^q Y_{t-j}\\\\\n&= \\frac{1}{2q+1}\\sum_{j=-q}^q M_{t-j} + \\frac{1}{2q+1}\\sum_{j=-q}^q \\epsilon_{t-j} \\\\\n&\\approx M_t,\n\\end{split}\n\\tag{3.4}\\] where \\(w = 2q+1\\) is the size of the moving window. Note that the above approximation is correct if the average value of \\(\\epsilon_{t}\\) within each window is close to 0 (important for selecting \\(q\\)).\n\n\n\n\n\n\nNoteExample: Shampoo plots\n\n\n\nFigure 3.1 shows a time series plot of monthly shampoo sales. While the data are monthly, the seasonal component is not visible:\n\nno strong periodicity in the time series plot;\nthe ACF at seasonal lag (1 year) is not significant.\n\nTherefore, we apply Equation 3.4 treating the shampoo time series as non-seasonal data.\n\n# Get the data from the package fma\nshampoo &lt;- fma::shampoo\nshampoo\n\n#&gt;   Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n#&gt; 1 266 146 183 119 180 168 232 224 193 123 336 186\n#&gt; 2 194 150 210 273 191 287 226 304 290 422 264 342\n#&gt; 3 340 440 316 439 401 437 576 408 682 475 581 647\n\n\n\nCodepshampoo &lt;- ggplot2::autoplot(shampoo, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Sales\")\np2 &lt;- forecast::ggAcf(shampoo) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\npshampoo + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.1: Monthly shampoo sales over three years and a corresponding sample ACF.\n\n\n\n\n\n\nThere are multiple ways to calculate moving averages in R, and formulas for \\(W_t\\) may vary. If using someone’s else functions, remember to read the help files or source code to find out the exact formula in place of Equation 3.4. Below are some examples.\n\n\n\n\n\n\nNote\n\n\n\nThe package forecast is now retired in favor of the package fable.\n\n\nFor example, forecast::ma() has the default option centre = TRUE, and the order argument represents the window size. Odd order and centre = TRUE correspond exactly to our definitions in Equation 3.4:\n\nforecast::ma(shampoo, order = 5)\n\n#&gt;   Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n#&gt; 1  NA  NA 179 159 177 185 200 188 222 213 206 198\n#&gt; 2 215 203 204 222 238 256 260 306 301 324 332 362\n#&gt; 3 341 376 387 407 434 452 501 516 544 559  NA  NA\n\n\nFor example, pracma::movavg() averages the last n data points:\n\npracma::movavg(shampoo, n = 5)\n\n#&gt;  [1] 266 206 198 179 179 159 177 185 200 188 222 213 206 198 215 203 204 222 238\n#&gt; [20] 256 260 306 301 324 332 362 341 376 387 407 434 452 501 516 544 559\n\n\nThe base-R function stats::filter() can also be used. Its odd window size and sides = 2 correspond to our definitions in Equation 3.4:\n\nwindow_size &lt;- 5\nstats::filter(shampoo, filter = 1/rep(window_size, window_size), sides = 2)\n\n#&gt;   Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n#&gt; 1  NA  NA 179 159 177 185 200 188 222 213 206 198\n#&gt; 2 215 203 204 222 238 256 260 306 301 324 332 362\n#&gt; 3 341 376 387 407 434 452 501 516 544 559  NA  NA\n\n\nThe sides = 1 in stats::filter() corresponds to the pracma::movavg() results above:\n\nstats::filter(shampoo, filter = 1/rep(window_size, window_size), sides = 1)\n\n#&gt;   Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n#&gt; 1  NA  NA  NA  NA 179 159 177 185 200 188 222 213\n#&gt; 2 206 198 215 203 204 222 238 256 260 306 301 324\n#&gt; 3 332 362 341 376 387 407 434 452 501 516 544 559\n\n\nSee Figure 3.2 and Figure 3.3 for the effects of the window size.\n\n\n\n\n\n\nNoteExample: Shampoo moving average smoothing\n\n\n\n\nCodeps &lt;- lapply(c(1, 2, 3, 5), function(q){\n    w &lt;- 2 * q + 1\n    Wt &lt;- forecast::ma(shampoo, order = w)\n    pshampoo + \n        geom_line(aes(y = Wt), col = 4, lwd = 1.5) + \n        ggtitle(paste0(\"q = \", q, \" (window = \", w, \")\"))\n})\nwrap_plots(ps, ncol = 2) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.2: Shampoo sales over three years smoothed with centered moving average filters, where the window size is \\(w = 2q+1\\).\n\n\n\n\n\nCodeps &lt;- lapply(c(1, 2, 3, 5), function(q){\n    w &lt;- 2 * q + 1\n    Wt &lt;- stats::filter(shampoo, filter = 1/rep(w, w), sides = 1)\n    pshampoo + \n        geom_line(aes(y = Wt), col = 4, lwd = 1.5) + \n        ggtitle(paste0(\"q = \", q, \" (window = \", w, \")\"))\n})\nwrap_plots(ps, ncol = 2) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.3: Shampoo sales over three years smoothed with non-centered moving average filters, where the window size is \\(w = 2q+1\\).\n\n\n\n\n\n\n\n3.2.1 Moving average smoothing for seasonal data\nCome back to the trend-seasonal model Equation 3.1: \\[\nY_t = M_t + S_t + \\epsilon_t.\n\\]\nWe apply the following steps for estimating seasonality and trend:\n\nApply a moving average filter specially chosen to eliminate the seasonal component and dampen the noise – get \\(\\hat{M}_t\\). Remember that the sum of \\(S_t\\) within each period is 0. Hence, to smooth out the seasonal component (and noise) and get an estimate of just \\(M_t\\), we will use a moving window \\(w\\) sized as the seasonal period \\(m\\) (more generally, \\(w = km\\), where \\(k\\in \\mathbb{N}^{+}\\), if a greater smoothness is desired). It will often lead to the even window size \\(w\\). For even \\(w\\), Equation 3.4 is modified as follows (we now use \\(km + 1\\) elements so the window is still centered, but the elements on the ends receive half the weight): \\[\n\\hat{M}_t = \\frac{0.5Y_{t-km/2} + Y_{t-km/2 + 1} +\\dots+Y_{t+km/2-1} + 0.5Y_{t+km/2}}{km}.\n\\tag{3.5}\\]\n\nUse \\(Y_t - \\hat{M}_t\\) to estimate the seasonal component – get \\(\\hat{S}_t\\). If needed, correct the estimates, to sum up to 0 in each period. Let \\(\\hat{S}^*_t\\) be the corrected values.\nUse deseasonalized data \\(Y_t - \\hat{S}^*_t\\) to re-estimate the trend. Let \\(\\hat{M}^*_t\\) be the corrected trend estimate.\nThe estimated random noise is then: \\(\\hat{\\epsilon}_t = Y_t - \\hat{M}^*_t - \\hat{S}^*_t\\).\n\n\n\n\n\n\n\nNote\n\n\n\nFor the multiplicative case, replace subtractions with divisions and let the product of \\(\\hat{S}^*_t\\) be 1 within each seasonal period.\n\n\nThe above algorithm is implemented in stats::decompose(), but its main disadvantage is in the centered moving average filter used to estimate \\(M_t\\) that does not produce smoothed values for the most recent observations. We can elaborate this algorithm, first, by using different estimators for \\(M_t\\); second, by also replacing estimators for \\(S_t\\) (e.g., see the next section).\n\n\n\n\n\n\nNoteExample: Secchi depth automatic (simple moving average-based) decomposition\n\n\n\nConsider monthly data of Secchi disk depth measured at station CB1.1 in Chesapeake Bay (Figure 3.4), with the decomposition in Figure 3.5.\n\nCodesecchi_data &lt;- read.csv(\"data/Secchi_CB1.1.csv\", header = TRUE)\nSecchi &lt;- ts(secchi_data$Value,\n             start = c(min(secchi_data$Year), min(secchi_data$Month)),\n             frequency = 12)\npSecchi &lt;- ggplot2::autoplot(Secchi, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Secchi depth (m)\")\np2 &lt;- forecast::ggAcf(Secchi) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\npSecchi + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.4: Monthly average Secchi disk depth at station CB1.1 and its ACF.\n\n\n\n\n\nCodeggplot2::autoplot(stats::decompose(Secchi))\n\n\n\n\n\n\nFigure 3.5: Trend-seasonal decomposition of monthly average Secchi disk depth at station CB1.1.\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Secchi depth manual simple moving average smoothing\n\n\n\nTo replicate the behavior of the function stats::decompose() or tune the outputs, use the following code. For example, Figure 3.5 shows a wiggly trend that we might want to smooth more, using a bigger window, such as 24 months instead of 12 months.\n\nYt &lt;- Secchi\nt &lt;- as.vector(time(Yt))\nmonth &lt;- as.factor(cycle(Yt))\n\n# 1. Initial trend estimate\nwindow_size = 24\nMt &lt;- stats::filter(Yt, \n                    filter = 1/c(2*window_size, rep(window_size, window_size - 1), \n                                 2*window_size), \n                    sides = 2)\n\n# 2. Estimate of the seasonality, corrected to sum up to 0\nSt &lt;- tapply(Yt - Mt, month, mean, na.rm = TRUE)\nSt\n\n#&gt;       1       2       3       4       5       6       7       8       9      10 \n#&gt; -0.0225  0.0869 -0.2469 -0.2791 -0.0748 -0.0379  0.1256  0.1831  0.1234  0.1150 \n#&gt;      11      12 \n#&gt;  0.0368 -0.0164\n\nsum(St)\n\n#&gt; [1] -0.00673\n\nSt_star &lt;- St - sum(St)/12\nsum(St_star)\n\n#&gt; [1] -4.16e-17\n\n# 3. Refined trend estimate\nMt_star &lt;- stats::filter(Yt - St_star[month], \n                    filter = 1/c(2*window_size, rep(window_size, window_size - 1), \n                                 2*window_size), \n                    sides = 2)\n\n# 4. Noise\net &lt;- Yt - Mt_star - St_star[month]\n\n# Convert back to ts format for plotting\net &lt;- ts(as.vector(et), start = start(Secchi), frequency = 12)\n\nSee Figure 3.6 for the results.\n\nCodep1 &lt;- pSecchi + \n    geom_line(aes(y = Mt_star), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 &lt;- pSecchi + \n    geom_line(aes(y = Mt_star + St_star[month]), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 &lt;- ggplot2::autoplot(et) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\")\np4 &lt;- forecast::ggAcf(et) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\")\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.6: Detrending using simple moving average and deseasonalizing the Secchi depth data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Smoothing, Detrending, and Deseasonalizing</span>"
    ]
  },
  {
    "objectID": "l03_smoothing.html#sec-lowess",
    "href": "l03_smoothing.html#sec-lowess",
    "title": "\n3  Smoothing, Detrending, and Deseasonalizing\n",
    "section": "\n3.3 Lowess smoothing",
    "text": "3.3 Lowess smoothing\nOne of the alternatives for estimating trends \\(M_t\\) is locally weighted regression (shortened as ‘lowess’ or ‘loess’) (Cleveland 1979). We can apply lowess as a regression of \\(Y_t\\) on time, for example, using the algorithm outlined by Berk (2016):\n\nChoose the smoothing parameter such as bandwidth, \\(f\\), which is a proportion between 0 and 1.\nChoose a point \\(t_0\\) and its \\(w = f n\\) nearest points on the time axis.\nFor these \\(w\\) nearest neighbor points, compute a weighted least squares regression line for \\(Y_t\\) on \\(t\\). The coefficients \\(\\boldsymbol{\\beta}\\) of such regression are estimated by minimizing the residual sum of squares \\[\n\\text{RSS}^*(\\boldsymbol{\\beta}) = (\\boldsymbol{Y}^* - \\boldsymbol{X}^* \\boldsymbol{\\beta})^{\\top} \\boldsymbol{W}^* (\\boldsymbol{Y}^* - \\boldsymbol{X}^* \\boldsymbol{\\beta}),\n\\] where the asterisk indicates that only the observations in the window are included. The regressor matrix \\(\\boldsymbol{X}^*\\) can contain polynomial terms of time, \\(t\\). The \\(\\boldsymbol{W}^*\\) is a diagonal matrix conforming to \\(\\boldsymbol{X}^*\\), with diagonal elements being a function of distance from \\(t_0\\) (observations closer to \\(t_0\\) receive higher weights).\nCalculate the fitted value \\(\\tilde{Y}_t\\) for that single \\(t_0\\).\nRepeat Steps 2–4 for each \\(t_0 = 1,\\dots,n\\).\n\nBy selecting large bandwidth \\(f\\) in the lowess algorithm, we can obtain greater smoothing (see Figure 3.7).\n\nCodedata(WWWusage)\nps &lt;- lapply(c(0.1, 0.3, 0.5, 0.75), function(w){\n    ggplot2::autoplot(WWWusage, colour = \"grey50\")  + \n        ggtitle(paste0(\"span = \", w)) +\n        geom_smooth(method = \"loess\", span = w, se = FALSE)\n})\nwrap_plots(ps, ncol = 2) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.7: A lowess illustration, adapted from Berk (2016).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function ggplot2::geom_smooth() has the default setting se = TRUE that produces a confidence interval around the smooth. In time series applications, autocorrelated residuals may lead to underestimated standard errors and incorrect (typically, too narrow) confidence intervals. Hence, we set se = FALSE. For testing the significance of the identified trends, see lectures on trend testing (detection).\n\n\n\n3.3.1 Lowess smoothing for seasonal data\nNow use the Secchi time series (Figure 3.4) and implement the procedure of detrending and deseasonalizing from Section 3.2.1, but replace the simple moving average estimates of \\(M_t\\) with lowess estimates.\n\n\n\n\n\n\nNoteExample: Secchi depth lowess-based decomposition\n\n\n\n\nYt &lt;- Secchi\nt &lt;- as.vector(time(Yt))\nmonth &lt;- as.factor(cycle(Yt))\n\n# 1. Initial trend estimate\nMt &lt;- loess(Yt ~ t, span = 0.25)$fitted\n\n# 2. Estimate of the seasonality, corrected to sum up to 0\nSt &lt;- tapply(Yt - Mt, month, mean)\nSt\n\n#&gt;       1       2       3       4       5       6       7       8       9      10 \n#&gt; -0.0359  0.0457 -0.2483 -0.2683 -0.0716 -0.0129  0.1411  0.2024  0.1276  0.1108 \n#&gt;      11      12 \n#&gt;  0.0391 -0.0165\n\nsum(St)\n\n#&gt; [1] 0.0131\n\nSt_star &lt;- St - sum(St)/12\nsum(St_star)\n\n#&gt; [1] 3.47e-18\n\n# 3. Refined trend estimate\nMt_star &lt;- loess((Yt - St_star[month]) ~ t, span = 0.25)$fitted\n\n# 4. Noise\net &lt;- Yt - Mt_star - St_star[month]\n\n# Convert back to ts format for plotting\net &lt;- ts(as.vector(et), start = start(Secchi), frequency = 12)\n\nSee Figure 3.8 for the results.\n\nCodep1 &lt;- pSecchi + \n    geom_line(aes(y = Mt_star), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 &lt;- pSecchi + \n    geom_line(aes(y = Mt_star + St_star[month]), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 &lt;- ggplot2::autoplot(et) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\")\np4 &lt;- forecast::ggAcf(et) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\")\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.8: Detrending using lowess and deseasonalizing the Secchi depth data.\n\n\n\n\nIn Figure 3.9, we use the decomposition function stl() forced to have approximately the same values as we used in the steps above and as shown in Figure 3.8. Hence, Figure 3.8 and Figure 3.9 are similar. However, the function stl() is more flexible – it also automatically smooths the seasonal component (across Januaries, across Februaries, etc.) – and can provide finer estimates (see Figure 3.10).\n\nCode# Span we used above (the fraction)\nspan = 0.25\n\n# Window size in number of lags (observations)\nw &lt;- span * length(Secchi)/2 \nD &lt;- stl(Yt, s.window = \"periodic\", t.window = w)\nMt_star &lt;- D$time.series[,\"trend\"]\nSt_star &lt;- D$time.series[,\"seasonal\"]\net &lt;- D$time.series[,\"remainder\"]\n\np1 &lt;- pSecchi +\n    geom_line(aes(y = Mt_star), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 &lt;- pSecchi + \n    geom_line(aes(y = Mt_star + St_star), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 &lt;- ggplot2::autoplot(et) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\")\np4 &lt;- forecast::ggAcf(et) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\")\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.9: Detrending and deseasonalizing the Secchi depth data using lowess within the function stl() forced to behave similarly to Figure 3.8.\n\n\n\n\n\nCodeD &lt;- stl(Yt, s.window = 24, s.degree = 1, t.window = w)\nMt_star &lt;- D$time.series[,\"trend\"]\nSt_star &lt;- D$time.series[,\"seasonal\"]\net &lt;- D$time.series[,\"remainder\"]\n\np1 &lt;- pSecchi +\n    geom_line(aes(y = Mt_star), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 &lt;- pSecchi + \n    geom_line(aes(y = Mt_star + St_star), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 &lt;- ggplot2::autoplot(et) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\")\np4 &lt;- forecast::ggAcf(et) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\")\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.10: Detrending and deseasonalizing the Secchi depth data using lowess within the function stl() with default settings.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Smoothing, Detrending, and Deseasonalizing</span>"
    ]
  },
  {
    "objectID": "l03_smoothing.html#exponential-smoothing",
    "href": "l03_smoothing.html#exponential-smoothing",
    "title": "\n3  Smoothing, Detrending, and Deseasonalizing\n",
    "section": "\n3.4 Exponential smoothing",
    "text": "3.4 Exponential smoothing\nExponential smoothing (ES) is a successful forecasting technique. It turns out that ES can be modified to be used effectively for time series with\n\nslowly drifting trend (double exponential smoothing);\ntrends (Holt’s method);\nseasonal patterns;\ncombination of trend and seasonality (Holt–Winters method).\n\nES is easy to adjust for past errors and easy to prepare follow-on forecasts. ES is ideal for situations where many forecasts must be prepared. Several different functional forms of ES are used depending on the presence of trend or cyclical variations.\nIn short, an ES is an averaging technique that uses unequal weights while the weights applied to past observations decline in an exponential manner.\nSingle exponential smoothing calculates the smoothed series as a damping coefficient \\(\\alpha\\) (\\(\\alpha \\in [0, 1]\\)) times the actual series plus \\(1 - \\alpha\\) times the lagged value of the smoothed series. For the model \\[\nY_{t} = M_t + \\epsilon_{t},\n\\] the updating equation is \\[\n\\begin{split}\n\\hat{M}_1 &= Y_1\\\\\n\\hat{M}_t &= \\alpha Y_t + (1 - \\alpha) \\hat{M}_{t - 1}\n\\end{split}\n\\] and the forecast is \\[\n\\hat{Y}_{t+1} = \\hat{M}_t.\n\\]\nAn exponential smoothing over an already smoothed time series is called double exponential smoothing. In some cases, it might be necessary to extend it even to a triple exponential smoothing.\nHolt’s linear exponential smoothing Suppose that the series \\(Y_t\\) is non-seasonal but displays a trend. Now we need to estimate both the current mean (a.k.a. level) and the current trend.\nThe updating equations express ideas similar to those for exponential smoothing. However, now we have two smoothing parameters, \\(\\alpha\\) and \\(\\beta\\) (\\(\\alpha \\in [0, 1]\\); \\(\\beta \\in [0, 1]\\)).\nThe updating equations are \\[\na_{t} = \\alpha Y_{t} + \\left( 1- \\alpha \\right) \\left( a_{t - 1} + b_{t - 1} \\right)\n\\] for the mean and \\[\nb_{t} = \\beta \\left( a_{t} - a_{t-1} \\right) + \\left( 1 - \\beta \\right) b_{t-1}\n\\] for the trend.\nThen the forecasting for \\(k\\) steps into the future is \\[\n\\hat{Y}_{t+k} = a_{t} + kb_{t}.\n\\]\nUsually, the initial (starting) values are \\[\n\\begin{split}\na_{1} & = Y_{2}, \\\\\nb_{1} & = Y_{2} - Y_{1}.\n\\end{split}\n\\]\n\n\n\n\n\n\nNoteExample: Shampoo exponential smoothing\n\n\n\nSee an example in Figure 3.11.\n\nCodeclrs &lt;- c(\"0.3\" = 4, \"0.7\" = 2)\nm1 &lt;- HoltWinters(shampoo, alpha = 0.3, beta = FALSE, gamma = FALSE)\nm2 &lt;- HoltWinters(shampoo, alpha = 0.7, beta = FALSE, gamma = FALSE)\np1 &lt;- pshampoo + \n    geom_line(data = fitted(m1)[,1], aes(col = \"0.3\"), lwd = 1.5) + \n    geom_line(data = fitted(m2)[,1], aes(col = \"0.7\"), lwd = 1.5) +\n    ggtitle(\"Simple exponential smoothing\") +\n    labs(color = \"\\u03b1\") +\n    scale_color_manual(values = clrs)\nclrs &lt;- c(\"(0.3, 0.3)\" = 4, \"(0.7, 0.7)\" = 2)\nm1 &lt;- HoltWinters(shampoo, alpha = 0.3, beta = 0.3, gamma = FALSE)\nm2 &lt;- HoltWinters(shampoo, alpha = 0.7, beta = 0.7, gamma = FALSE)\np2 &lt;- pshampoo + \n    geom_line(data = fitted(m1)[,1], aes(col = \"(0.3, 0.3)\"), lwd = 1.5) + \n    geom_line(data = fitted(m2)[,1], aes(col = \"(0.7, 0.7)\"), lwd = 1.5) +\n    ggtitle(\"Holt's method\") +\n    labs(color = \"(\\u03b1, \\u03b2)\") +\n    scale_color_manual(values = clrs)\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.11: Shampoo sales over three years smoothed with different exponential smoothing filters.\n\n\n\n\n\n\n\n3.4.1 Exponential smoothing for seasonal data\nMultiplicative Holt–Winters procedure\nNow in addition to the Holt parameters, suppose that the series exhibits multiplicative seasonality and let \\(S_{t}\\) be the multiplicative seasonal factor at the time \\(t\\).\nSuppose also that there are \\(m\\) observations in one period (in a year). For example, \\(m = 4\\) for quarterly data, and \\(m = 12\\) for monthly data.\nIn some time series, seasonal variation is so strong it obscures any trends or other cycles, which are important for our understanding of the observed process. Holt–Winters smoothing method can remove seasonality and makes long-term fluctuations in the series stand out more clearly.\nA simple way of detecting a trend in seasonal data is to take averages over a certain period. If these averages change with time we can say that there is evidence of a trend in the series.\nWe now use three smoothing parameters: \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) (\\(\\alpha \\in [0, 1]\\); \\(\\beta \\in [0, 1]\\); \\(\\gamma \\in [0, 1]\\)).\nThe updating equations for the level (\\(a_t\\)), local trend (\\(b_t\\)), and seasonal factor (\\(S_t\\)) are: \\[\n\\begin{split}\na_{t} & = \\alpha Y_{t} / S_{t - m} + (1 - \\alpha) ( a_{t - 1} + b_{t - 1}), \\\\\nb_{t} & = \\beta (a_{t} - a_{t - 1}) + (1 - \\beta) b_{t - 1}, \\\\\nS_{t} & = \\gamma Y_{t} / a_{t} + (1 - \\gamma) S_{t - m}.\n\\end{split}\n\\]\nThen the forecasting for \\(k\\) steps into the future is \\[\n\\hat{Y}_{t+k} = (a_{t} + kb_{t}) S_{t+k-m},\n\\] where \\(k = 1, 2, \\dots, m\\).\nTo obtain starting values, one may use an average over the first few periods (years) of the data.\nThe smoothing parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) are estimated by minimizing the sum of the squared one-step prediction errors.\nAdditive Holt–Winters procedure\nThe updating equations are \\[\n\\begin{split}\na_{t} & = \\alpha (Y_{t} - S_{t - m}) + (1 - \\alpha) (a_{t - 1} + b_{t - 1}), \\\\\nb_{t} & = \\beta (a_{t} - a_{t - 1}) + (1 - \\beta) b_{t - 1} \\\\\nS_{t} & = \\gamma (Y_{t} - a_{t}) + (1 - \\gamma) S_{t - m}.\n\\end{split}\n\\]\nThen the forecasting for \\(k\\) steps into the future is \\[\n\\hat{Y}_{t+k} = a_{t} + kb_{t} + S_{t + k - m},\n\\] where \\(k = 1, 2, \\dots, m\\).\n\n\n\n\n\n\nNoteExample: Airline passengers non-seasonal and seasonal exponential smoothing\n\n\n\nMonthly totals of international airline passengers, 1949–1960 (see Figure 3.12). This is a classical example in time series analysis. Note that the seasonal variability increases with the increasing mean, hence, we deal with the multiplicative seasonality.\n\nCodepAirPassengers &lt;- ggplot2::autoplot(AirPassengers, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Airline passengers (thousand)\")\np2 &lt;- forecast::ggAcf(AirPassengers) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\npAirPassengers + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.12: Monthly AirPassengers data and a corresponding sample ACF.\n\n\n\n\nComparison of various exponential smoothing techniques for the AirPassengers data:\n\nm1 &lt;- HoltWinters(AirPassengers, beta = FALSE, gamma = FALSE)\nm1$SSE\n\n#&gt; [1] 162511\n\nc(m1$alpha, m1$beta, m1$gamma)\n\n#&gt; [1] 1 0 0\n\nm2 &lt;- HoltWinters(AirPassengers, gamma = FALSE)\nm2$SSE\n\n#&gt; [1] 163634\n\nc(m2$alpha, m2$beta, m2$gamma)\n\n#&gt;   alpha    beta         \n#&gt; 1.00000 0.00322 0.00000\n\nm3 &lt;- HoltWinters(AirPassengers)\nm3$SSE\n\n#&gt; [1] 21860\n\nc(m3$alpha, m3$beta, m3$gamma)\n\n#&gt;  alpha   beta  gamma \n#&gt; 0.2480 0.0345 1.0000\n\nm4 &lt;- HoltWinters(AirPassengers, seasonal = \"multiplicative\")\nm4$SSE\n\n#&gt; [1] 16571\n\nc(m4$alpha, m4$beta, m4$gamma)\n\n#&gt;  alpha   beta  gamma \n#&gt; 0.2756 0.0327 0.8707\n\n\nThe last multiplicative model is the best one, based on the sum of squared errors (SSE) in the training set (see Figure 3.13). For a more thorough comparison, consider using out-of-sample data in cross-validation.\n\nCodek = 12\nfm1 &lt;- predict(m1, n.ahead = k)\nfm2 &lt;- predict(m2, n.ahead = k)\nfm3 &lt;- predict(m3, n.ahead = k)\nfm4 &lt;- predict(m4, n.ahead = k)\np1 &lt;- pAirPassengers + \n    geom_line(data = m1$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm1) +\n    ggtitle(\"Exponential smoothing\") +\n    theme(legend.position = \"none\")\np2 &lt;- pAirPassengers + \n    geom_line(data = m2$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm2) +\n    ggtitle(\"Holt's method\") +\n    theme(legend.position = \"none\")\np3 &lt;- pAirPassengers + \n    geom_line(data = m3$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm3) +\n    ggtitle(\"Additive Holt-Winters\") +\n    theme(legend.position = \"none\")\np4 &lt;- pAirPassengers + \n    geom_line(data = m4$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm4) +\n    ggtitle(\"Multiplicative Holt-Winters\") +\n    theme(legend.position = \"none\")\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.13: Plots of the observed, fitted, and predicted AirPassengers data, using various smoothing procedures.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Smoothing, Detrending, and Deseasonalizing</span>"
    ]
  },
  {
    "objectID": "l03_smoothing.html#sec-regressionTime",
    "href": "l03_smoothing.html#sec-regressionTime",
    "title": "\n3  Smoothing, Detrending, and Deseasonalizing\n",
    "section": "\n3.5 Polynomial regression on time",
    "text": "3.5 Polynomial regression on time\nThis is a very intuitive procedure for fitting parametric trend functions to time series:\n\nMake a parametric model assumption about \\(M_t\\) as a function of time \\(t\\), for example, quadratic trend: \\[\nM_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2\n\\]\n\nFit the model using the usual regression estimators (least squares or maximum likelihood)\n\n\n\n\n\n\n\nNote\n\n\n\nA nonstationary time series with a trend represented by a parametric function is a typical example of a trend-stationary time series or a time series with a deterministic trend. It is easy to make such a time series stationary by modeling and extracting the trend.\n\n\n\n\n\n\n\n\nNoteExample: Airline passengers non-seasonal polynomial smoothing\n\n\n\nContinue using the AirPassengers data and fit linear and quadratic trends.\n\nt &lt;- as.vector(time(AirPassengers))\nt2 &lt;- t^2\ntm1 &lt;- lm(AirPassengers ~ t)\ntm2.1 &lt;- lm(AirPassengers ~ t + t2)\ntm2.2 &lt;- lm(AirPassengers ~ t + I(t^2))\ntm2.3 &lt;- lm(AirPassengers ~ poly(t, degree = 2))\n\nAfter estimating the trend coefficients with OLS, visualize the results (Figure 3.14).\n\nCodep1 &lt;- pAirPassengers + \n    geom_line(aes(y = tm1$fitted.values), col = 4, lwd = 1.5) + \n    ggtitle(\"Linear trend\")\np2 &lt;- pAirPassengers + \n    geom_line(aes(y = tm2.3$fitted.values), col = 4, lwd = 1.5) + \n    ggtitle(\"Quadratic trend\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.14: Plots of the AirPassengers data with estimated parametric linear and quadratic trends.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe models tm2.1 and tm2.2 are equivalent. Model tm2.1 uses a pre-calculated quadratic transformation, while model tm2.2 applies the transformation on the fly within the R formula call, using the I() syntax (without the I() wrapper, the output of tm2.2 would be the same as of tm1, not tm2.1). However, both models tm2.1 and tm2.2 can easily violate the assumption of independence of predictors in linear regression, especially when values of the time index \\(t\\) are large. It applies to our example because the sequence \\(t\\) of decimal years goes from 1949 to 1960.917, and \\(\\widehat{\\mathrm{cor}}(t, t^2) =\\) 1. Therefore, model tm2.3 is preferred, because it is evaluated on orthogonal polynomials (centering and normalization are applied to \\(t\\) and \\(t^2\\)).\n\n\n\n3.5.1 Seasonal regression with dummy variables\nTo each time \\(t\\) and each season \\(k\\) we will assign an indicator that ‘turns on’ if and only if \\(t\\) falls into that season. In principle, a seasonal period of any positive integer length \\(m \\geqslant 2\\) is possible, however, in most cases, the length of the seasonal cycle is one year and so the seasonal period of the time series depends on the number of observations per year. Some common periods are \\(m = 12\\) (monthly data) and \\(m = 4\\) (quarterly data). For each \\(k = 1, \\dots, m\\), we define the indicator \\[\nX_{k,t} = \\left\\{\n\\begin{array}{cl}\n1, & \\mbox{if} ~ t ~ \\text{corresponds to season} ~ k, \\\\\n0, & \\mbox{if} ~ t ~ \\text{does not correspond to season} ~ k. \\\\\n\\end{array}\n\\right.\n\\]\nNote that for each \\(t\\) we have \\(X_{1,t} + X_{2,t} + \\dots + X_{m,t} = 1\\) since each \\(t\\) corresponds to exactly one season. Thus, given any \\(m - 1\\) of the variables, the remaining variable is known and thus redundant. Because of this linear dependence, we must drop one of the indicator variables (seasons) from the model. It does not matter which season is dropped although sometimes there are choices that are simpler from the point of view of constructing or labeling the design matrix. Thus the general form of a seasonal model looks like \\[\n\\begin{split}\nf (Y_{t}) &= M_{t} + \\beta_{2} X_{2,t} + \\beta_{3} X_{3,t} + \\dots + \\beta_{m} X_{m,t} + \\epsilon_{t} \\\\\n&= M_{t} + \\sum_{i=2}^{m}\\beta_{i} X_{i,t} + \\epsilon_{t},\n\\end{split}\n\\] where \\(M_{t}\\) is a trend term which may also include some \\(\\beta\\) parameters and explanatory variables. The function \\(f\\) represents the appropriate variance-stabilizing transformations as needed. (Here the 1st season has been dropped from the model.)\n\n\n\n\n\n\nNoteExample: Airline passengers polynomial smoothing with dummy variables for the seasons\n\n\n\nConsider adding the dummy variables to model seasonality together with the quadratic trend in the AirPassengers time series. We noticed multiplicative seasonality in this time series before, therefore we apply a logarithmic transformation to the original data to transform the multiplicative seasonality into additive. That is, we need to estimate the model \\[\n\\ln(Y_{t}) = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\sum_{i=2}^{12}\\beta_{i} X_{i,t} + \\epsilon_{t},\n\\] where \\(X_{i,t}\\) are the dummy variables for the months. (Here the first month has been dropped from the model, the same as in R the first factor level is dropped.)\nIf we were doing the regression analysis by hand, we would create a design matrix with its few top rows looking like this:\n\n\n#&gt;          t      t2 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12\n#&gt;  [1,] 1949 3798601  0  0  0  0  0  0  0  0   0   0   0\n#&gt;  [2,] 1949 3798926  1  0  0  0  0  0  0  0   0   0   0\n#&gt;  [3,] 1949 3799251  0  1  0  0  0  0  0  0   0   0   0\n#&gt;  [4,] 1949 3799576  0  0  1  0  0  0  0  0   0   0   0\n#&gt;  [5,] 1949 3799900  0  0  0  1  0  0  0  0   0   0   0\n#&gt;  [6,] 1949 3800225  0  0  0  0  1  0  0  0   0   0   0\n#&gt;  [7,] 1950 3800550  0  0  0  0  0  1  0  0   0   0   0\n#&gt;  [8,] 1950 3800875  0  0  0  0  0  0  1  0   0   0   0\n#&gt;  [9,] 1950 3801200  0  0  0  0  0  0  0  1   0   0   0\n#&gt; [10,] 1950 3801525  0  0  0  0  0  0  0  0   1   0   0\n#&gt; [11,] 1950 3801850  0  0  0  0  0  0  0  0   0   1   0\n#&gt; [12,] 1950 3802175  0  0  0  0  0  0  0  0   0   0   1\n#&gt; [13,] 1950 3802500  0  0  0  0  0  0  0  0   0   0   0\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe way the months are represented above as separate columns of 0s and 1s is called one-hot encoding in the field of machine learning.\n\n\nIn R, it is enough to have one variable representing the seasons. This categorical variable should be saved as a factor. Then we apply the OLS method to find the model coefficients (see results in Figure 3.15):\n\nMonth &lt;- as.factor(cycle(AirPassengers))\nm &lt;- lm(log(AirPassengers) ~ poly(t, degree = 2) + Month)\nsummary(m)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(AirPassengers) ~ poly(t, degree = 2) + Month)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.12748 -0.03709  0.00418  0.03197  0.11529 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           5.45716    0.01392  391.91  &lt; 2e-16 ***\n#&gt; poly(t, degree = 2)1  5.02251    0.04837  103.84  &lt; 2e-16 ***\n#&gt; poly(t, degree = 2)2 -0.39837    0.04820   -8.26  1.4e-13 ***\n#&gt; Month2               -0.02227    0.01968   -1.13  0.25984    \n#&gt; Month3                0.10779    0.01968    5.48  2.2e-07 ***\n#&gt; Month4                0.07639    0.01968    3.88  0.00016 ***\n#&gt; Month5                0.07393    0.01968    3.76  0.00026 ***\n#&gt; Month6                0.19603    0.01968    9.96  &lt; 2e-16 ***\n#&gt; Month7                0.29997    0.01969   15.24  &lt; 2e-16 ***\n#&gt; Month8                0.29072    0.01969   14.77  &lt; 2e-16 ***\n#&gt; Month9                0.14617    0.01969    7.42  1.3e-11 ***\n#&gt; Month10               0.00814    0.01970    0.41  0.67991    \n#&gt; Month11              -0.13540    0.01970   -6.87  2.4e-10 ***\n#&gt; Month12              -0.02132    0.01971   -1.08  0.28129    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.0482 on 130 degrees of freedom\n#&gt; Multiple R-squared:  0.989,  Adjusted R-squared:  0.988 \n#&gt; F-statistic:  913 on 13 and 130 DF,  p-value: &lt;2e-16\n\n\n\nCodep1 &lt;- pAirPassengers + \n    geom_line(aes(y = exp(m$fitted.values)), col = 4, lwd = 1.5)\np2 &lt;- forecast::ggAcf(m$residuals) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.15: The AirPassengers data, with estimated parametric quadratic trend and seasonality (modeled using dummy variables), and ACF of the residuals. Since the model was fitted on the log scale, the trend-cycle estimates need to be exponentiated to match the original scale of the data.\n\n\n\n\n\n\n\n3.5.2 Seasonal regression with Fourier series\nIt is a spoiler for the future lecture on spectral analysis of time series, but seasonal regression modeling would not be complete without introducing the trigonometric Fourier series.\nWe can fit a linear regression model using several pairs of trigonometric functions as predictors. For example, for monthly observations \\[\n\\begin{split}\ncos_{k} (i) &= \\cos (2 \\pi ki / 12), \\\\\nsin_{k} (i) &= \\sin (2 \\pi ki / 12),\n\\end{split}\n\\] where \\(i\\) is the month within the year, and the trigonometric function has \\(k\\) cycles per year.\n\n\n\n\n\n\nNoteExample: Airline passengers polynomial smoothing with Fourier series for the seasons\n\n\n\nNow let us apply the method of sinusoids to the AirPassengers time series. We have one prominent and, possibly, one less prominent peak per year. Hence, we can test the model with \\(k = 1\\) and \\(k = 2\\). We will construct the following trigonometric predictors: \\[\n\\begin{split}\ncos_{1,t} &= \\cos(2 \\pi \\text{month}_t/12),\\\\\nsin_{1,t} &= \\sin(2 \\pi \\text{month}_t/12),\\\\\ncos_{2,t} &= \\cos(4 \\pi \\text{month}_t/12),\\\\\nsin_{2,t} &= \\sin(4 \\pi \\text{month}_t/12)\n\\end{split}\n\\] to use in the model \\[\n\\ln(Y_{t}) = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\beta_{1}cos_{1,t} + \\beta_{2}sin_{1,t} + \\beta_{3}cos_{2,t} + \\beta_{4}sin_{2,t} + \\epsilon_{t}.\n\\]\nCalculate the predictors and estimate the model in R (see results in Figure 3.16):\n\nmonth &lt;- as.numeric(cycle(AirPassengers))\ncos1 &lt;- cos(2 * pi * month / 12)\nsin1 &lt;- sin(2 * pi * month / 12)\ncos2 &lt;- cos(4 * pi * month / 12)\nsin2 &lt;- sin(4 * pi * month / 12)\nm &lt;- lm(log(AirPassengers) ~ poly(t, degree = 2) + cos1 + sin1 + cos2 + sin2)\nsummary(m)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(AirPassengers) ~ poly(t, degree = 2) + cos1 + \n#&gt;     sin1 + cos2 + sin2)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.20078 -0.03539 -0.00056  0.04065  0.13771 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           5.54218    0.00493 1123.46  &lt; 2e-16 ***\n#&gt; poly(t, degree = 2)1  5.02920    0.05936   84.72  &lt; 2e-16 ***\n#&gt; poly(t, degree = 2)2 -0.39819    0.05920   -6.73  4.3e-10 ***\n#&gt; cos1                 -0.14152    0.00698  -20.28  &lt; 2e-16 ***\n#&gt; sin1                 -0.04923    0.00699   -7.04  8.3e-11 ***\n#&gt; cos2                 -0.02276    0.00698   -3.26   0.0014 ** \n#&gt; sin2                  0.07874    0.00698   11.28  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.0592 on 137 degrees of freedom\n#&gt; Multiple R-squared:  0.983,  Adjusted R-squared:  0.982 \n#&gt; F-statistic: 1.3e+03 on 6 and 137 DF,  p-value: &lt;2e-16\n\n\n\nCodep1 &lt;- pAirPassengers + \n    geom_line(aes(y = exp(m$fitted.values)), col = 4, lwd = 1.5)\np2 &lt;- forecast::ggAcf(m$residuals) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.16: The AirPassengers data, with estimated parametric quadratic trend and seasonality (modeled using trigonometric functions), and ACF of the residuals. Since the model was fitted on the log scale, the trend-cycle estimates need to be exponentiated to match the original scale of the data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Smoothing, Detrending, and Deseasonalizing</span>"
    ]
  },
  {
    "objectID": "l03_smoothing.html#sec-GAM",
    "href": "l03_smoothing.html#sec-GAM",
    "title": "\n3  Smoothing, Detrending, and Deseasonalizing\n",
    "section": "\n3.6 Generalized additive models (GAMs)",
    "text": "3.6 Generalized additive models (GAMs)\nAn alternative way of modeling nonlinearities in trends, seasonality, and other patterns is by replacing the original variables with those individually transformed using smooth nonparametric functions, such as in a generalized additive model (GAM, Wood 2006). The model is called ‘generalized’ because the distribution of the response variable \\(Y_t\\) is not just normal but can be from a family of exponential distributions, such as Poisson, binomial, and gamma. A smooth monotonic function \\(g(\\cdot)\\) called the ‘link function’ is applied to transform the response variable. However, our interest currently is in the additive nature of these models.\nInstead of fitting a global nonlinear function like in a polynomial regression (Section 3.5) or fitting simpler local polynomials directly like in lowess smoothing (Section 3.3), we can construct a basis from basic splines (B-splines). B-splines are simple functions, which vanish outside the intervals defined by time points called the ‘knots.’ These splines are defined recursively, from the lowest to higher orders, starting from a simple indicator function for the intervals between the knots (see Appendix to Chapter 5 in Hastie et al. 2009; or Chapter 4.1 in Wood 2006 for the recursive formulas). Here we show the results of the recursive computations, for times \\(t = 1, \\dots,\\) 100 and knots 1, 10, 20, 50 spaced unequally for illustration purposes (uniformly-spaced knots give us cardinal B-splines). Figure 3.17 shows a basis formed by B-splines of degree one, and Figure 3.18 shows splines of degree two.\n\n\n\n\n\n\n\nFigure 3.17: B-splines of degree one. The vertical dashed lines denote locations of the knots.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.18: B-splines of degree two. The vertical dashed lines denote locations of the knots.\n\n\n\n\nIn the next step, the basis is used in regression, for example, to model the expected value \\(\\mu_t\\) of the WWWusage data introduced in Section 3.3: \\[\ng(\\mu_t) = \\alpha_0 + f(t),\n\\tag{3.6}\\] where \\(\\alpha_0\\) is the intercept, and \\(f(t)\\) represents the added effects of the \\(M\\) basis functions \\(B_m(t)\\) (hence the word ‘additive’ in GAM): \\[\nf(t) = \\sum_{m = 1}^M \\beta_m B_m(t).\n\\] Here we may assume that the response WWWusage is normally distributed. The link function \\(g(\\cdot)\\) for normal distribution is identity (no transformation to the original data), hence we can omit it and rewrite Equation 3.6 for our data as follows: \\[\n\\mu_t = \\alpha_0 + f(t).\n\\] We are not interested in the individual regression coefficients \\(\\beta_m\\) (regression parameters) we estimated, hence the function \\(f(t)\\) is nonparametric even though many parameters have been estimated to obtain \\(f(t)\\), similar to lowess smoothing.\nThe model in Equation 3.6 can handle deviations from normality and can accommodate nonlinearity and nonmonotonicity of individual relationships, however, the model does not address the potential issue of remaining dependencies in the errors (e.g., see Kohn et al. 2000). We skip the topic of selecting an appropriate distribution, testing statistical significance of estimated patterns, or using the reported confidence intervals for inference until later in the course when we are familiar with various tests for autocorrelation and trends (e.g., see Chapter 7 on trend tests and Section 9.5 using additional covariates, specifying structure of the regression errors, and extending GAM to several parameters of an exponential distribution beyond just the mean).\nFigure 3.19 shows the results of fitting the model in Equation 3.6 to the bases shown in Figure 3.17 and Figure 3.18. The results are not so bad given the knots were set without considering the data.\n\n\n\n\n\n\n\nFigure 3.19: Results of regressing WWWusage on the linear, quadratic, and cubic B-spline bases. The fit in each case is suboptimal because the knots were set arbitrarily.\n\n\n\n\nThe number and locations of the knots are important tuning parameters for achieving the desired smoothness. Fortunately, there exist cross-validation procedures and penalization techniques that allow us to set these parameters rather automatically. When a penalty is applied to the basis coefficients, B-splines are called P-splines.\nThe R packages mgcv and gamlss provide several types of smoothing splines. In this lecture, we demonstrate the package mgcv but we will use the package gamlss in Section 9.5.\n\n\n\n\n\n\nNoteExample: GAM of WWWusage\n\n\n\nConsider fitting the GAM specified in Equation 3.6 to the WWWusage time series. Here we use B-splines again (see ?mgcv::smooth.terms and ?mgcv::b.spline for different options), but allow the parameters to be estimated each time using cross-validation (see ?mgcv::gam for more details).\nIn the code below, we estimate the model four times, with different spline orders m[1] (quadratic or cubic) and basis dimensions k (k controls the overall smoothness as it sets the upper limit on the degrees of freedom associated with a smooth, see ?mgcv::choose.k). The models can be compared using the Akaike information criterion (AIC).\n\nlibrary(mgcv)\nt &lt;- 1:length(WWWusage)\nmwww_q1 &lt;- gam(WWWusage ~ s(t, bs = \"bs\", m = c(2, 2)))\nmwww_c1 &lt;- gam(WWWusage ~ s(t, bs = \"bs\", m = c(3, 2)))\nmwww_c2 &lt;- gam(WWWusage ~ s(t, bs = \"bs\", m = c(3, 2), k = 5))\nmwww_c3 &lt;- gam(WWWusage ~ s(t, bs = \"bs\", m = c(3, 2), k = 15))\nAIC(mwww_q1, mwww_c1, mwww_c2, mwww_c3)\n\n#&gt;            df AIC\n#&gt; mwww_q1 10.92 701\n#&gt; mwww_c1 10.91 726\n#&gt; mwww_c2  5.97 887\n#&gt; mwww_c3 15.20 641\n\n\nFrom the results above and Figure 3.20, we may conclude that when the smoothing parameters are allowed to be selected automatically, restricting the degrees of freedom leads to more dramatic changes than switching the order of splines.\n\nCodep1 &lt;- p0 + \n    geom_line(aes(y = mwww_q1$fitted.values), col = 4, lwd = 1.5) +\n    ggtitle(\"Quadratic splines, automatic k\")\np2 &lt;- p0 + \n    geom_line(aes(y = mwww_c1$fitted.values), col = 4, lwd = 1.5) +\n    ggtitle(\"Cubic splines, automatic k\")\np3 &lt;- p0 + \n    geom_line(aes(y = mwww_c2$fitted.values), col = 4, lwd = 1.5) +\n    ggtitle(\"Cubic splines, k = 5\")\np4 &lt;- p0 + \n    geom_line(aes(y = mwww_c3$fitted.values), col = 4, lwd = 1.5) +\n    ggtitle(\"Cubic splines, k = 15\")\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.20: Results of regressing WWWusage on time, using the smoothing splines.\n\n\n\n\nSummary of the last model using cubic smoothing splines:\n\nsummary(mwww_c3)\n\n#&gt; \n#&gt; Family: gaussian \n#&gt; Link function: identity \n#&gt; \n#&gt; Formula:\n#&gt; WWWusage ~ s(t, bs = \"bs\", m = c(3, 2), k = 15)\n#&gt; \n#&gt; Parametric coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  137.080      0.552     248   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Approximate significance of smooth terms:\n#&gt;       edf Ref.df   F p-value    \n#&gt; s(t) 13.2   13.8 369  &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; R-sq.(adj) =  0.981   Deviance explained = 98.3%\n#&gt; GCV = 35.568  Scale est. = 30.519    n = 100\n\n\n\n\n\n\n\n\n\n\nNoteExample: GAM of the airline passengers time series\n\n\n\nConsider fitting a GAM to the airline passengers time series. Similar to our previous analyses, we need to apply logarithmic transformations to stabilize the variance and transform the multiplicative seasonality to additive, but now we have a few more options to choose from. We may continue assuming a lognormal distribution of the response with the identity link \\(g(\\mathrm{E}(Y_t)) = \\mathrm{E}(Y_t)\\) \\[\nY_t \\sim LN(\\mu_t, \\sigma^2) \\text{ or equivalently } \\ln(Y_t) ~ N(\\mu_t, \\sigma^2)\n\\tag{3.7}\\] or use a normal distribution with the logarithmic link function \\(g(\\mathrm{E}(Y_t)) = \\ln(\\mathrm{E}(Y_t))\\): \\[\nY_t \\sim N(\\mu_t, \\sigma^2).\n\\tag{3.8}\\] The main difference between these options is how the errors are interpreted.\nFor the expected values, we will use a GAM with smoothing splines applied to the numeric time and month: \\[\ng(\\mu_t) = \\alpha_0 + f_1(t) + f_2(Month_t).\n\\] This model will capture a potentially non-monotonic trend and seasonality. We will use cyclical splines for \\(f_2(\\cdot)\\). Cyclical splines have an additional penalty to ensure a smooth transition between the last and the first month of the annual cycle (from December to January).\nBelow we fit the model for both distribution families, lognormal with the identity link function (Equation 3.7) and normal with the logarithmic link (Equation 3.8):\n\nt &lt;- as.vector(time(AirPassengers))\nmonth &lt;- as.numeric(cycle(AirPassengers))\n\nm_lno &lt;- gam(log(AirPassengers) ~ s(t) + s(month, bs = \"cp\"), \n         family = gaussian(link = \"identity\"))\n\nm_no &lt;- gam(AirPassengers ~ s(t) + s(month, bs = \"cp\"), \n         family = gaussian(link = \"log\"))\n\nFigure 3.21 shows the results of smoothing are quite similar. In both cases, we were not able to fully remove seasonality from the data, based on the residual autocorrelation at the seasonal lag of 12 months.\n\nCodep1 &lt;- pAirPassengers + \n    geom_line(aes(y = exp(m_lno$fitted.values)), col = 4, lwd = 1.5) + \n    ggtitle(\"GAM smoothing using lognormal distribution\")\np2 &lt;- forecast::ggAcf(m_lno$residuals) + \n    ggtitle(\"ACF of residuals, using lognormal distribution\") +\n    xlab(\"Lag (months)\")\np3 &lt;- pAirPassengers + \n    geom_line(aes(y = m_no$fitted.values), col = 4, lwd = 1.5) + \n    ggtitle(\"GAM smoothing using normal distribution with log link\")\np4 &lt;- forecast::ggAcf(m_no$residuals) + \n    ggtitle(\"ACF of residuals, using normal distribution with log link\") +\n    xlab(\"Lag (months)\")\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.21: Plots of the AirPassengers data smoothed with GAMs and ACFs of the residuals.\n\n\n\n\nWe might prefer using the formulation in Equation 3.8 since it captures the most recent peaks better (Figure 3.21 C) than the alternative (Figure 3.21 A), however, more rigorous numeric comparisons can be performed.\nThe fitted smooth functions can be visualized using plot(m_no) or using the ggplot2-type graphics as in Figure 3.22. Note that the smooths are centered at 0 for the identifiability. This model used approximately 8.069 degrees of freedom to fit the trend, which is much more than the polynomial regression used. To fit the seasonality, the GAM used approximately 8.922 degrees of freedom, which is between the numbers for the regressions with Fourier series (4) and categorical months (11). Note that the quality of removing seasonality by GAM in this case is also between the regressions with Fourier series (strong autocorrelation at the seasonal lag remains, Figure 3.16) and categorical months (no significant autocorrelation at the seasonal lag, Figure 3.15).\n\nCodelibrary(mgcViz)\nb &lt;- getViz(m_no)\nprint(plot(b, allTerms = TRUE) + \n          theme_light(),\n      pages = 1)\n\n\n\n\n\n\nFigure 3.22: GAM smoothers. Solid curves are the function estimates, dashed curves are 2 standard errors above and below the estimate. The y-labels show the variable to which the smoothing splines were applied and the approximate degrees of freedom used for the smooth.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Smoothing, Detrending, and Deseasonalizing</span>"
    ]
  },
  {
    "objectID": "l03_smoothing.html#sec-differencing",
    "href": "l03_smoothing.html#sec-differencing",
    "title": "\n3  Smoothing, Detrending, and Deseasonalizing\n",
    "section": "\n3.7 Differencing",
    "text": "3.7 Differencing\nA big subset of time series models (particularly, models dealing with random walk series) applies the differencing to eliminate trends.\nLet \\(\\Delta\\) be the difference operator (\\(\\nabla\\) notation is also used sometimes), then the simple first-order differences are: \\[\n\\begin{split}\n\\Delta Y_t &= Y_t - Y_{t-1} \\\\\n&=(1-B)Y_t,\n\\end{split}\n\\] where \\(B\\) is a backshift operator.\nThe backshift operator and difference operator are useful for a convenient representation of higher-order differences. We will often use backshift operators in future lectures: \\[\n\\begin{split}\nB^0Y_{t} &= Y_{t} \\\\\nBY_{t} &= Y_{t-1} \\\\\nB^{2}Y_{t} &= Y_{t-2} \\\\\n& \\vdots \\\\\nB^{k}Y_{t} &= Y_{t-k}.\n\\end{split}\n\\] The convenience is because the powers of the operators can be treated as powers of elements of usual polynomials, so we can make different operations for more complex cases.\nFor example, if we took second-order differences, \\(\\Delta^2Y_t = (1-B)^2Y_t\\), to remove a trend and then used differences with the seasonal lag of 12 months to remove a strong seasonal component, what would be the final form of the transformed series? The transformed series, \\(Y^*_t\\), will be: \\[\n\\begin{split}\nY^*_t & = (1-B)^2 (1 - B^{12})Y_t \\\\\n& = (1 - 2B + B^2) (1 - B^{12})Y_t \\\\\n& = (1 - 2B + B^2 - B^{12} + 2B^{13} - B^{14})Y_t \\\\\n& = Y_t - 2Y_{t-1} + Y_{t-2} - Y_{t-12} + 2Y_{t-13}-Y_{t-14},\n\\end{split}\n\\] but we will often use just the top-row notations.\nWe will discuss formal tests to identify an appropriate order of differences in future lectures (unit-root tests), but for now use the rule of thumb: for time trends looking linear use 1st order differences (\\(\\Delta Y_t = Y_t - Y_{t-1}\\)), for parabolic shapes use differences of the 2nd order, etc. Apply the differencing of higher orders until the time series looks stationary (plot the transformed series and its ACF at each step). In practice, differences of order 3 or higher are rarely needed.\n\n\n\n\n\n\nNote\n\n\n\nA nonstationary time series that can be converted to stationary by taking differences is also sometimes called a difference-stationary time series or a time series with a stochastic trend. Very rarely, a time series may contain both deterministic and stochastic trends that need to be modeled or removed for further analysis.\n\n\n\n\n\n\n\n\nNoteExample: Shampoo detrending by differencing\n\n\n\nRemove the trend from the shampoo series by applying the differences.\nBased on Figure 3.23, differencing once is enough to remove the trend. The resulting detrended series is \\[\nY^*_t = \\Delta Y_t = (1 - B)Y_t = Y_t - Y_{t-1}.\n\\]\n\nCodep1 &lt;- pshampoo +\n    ggtitle(\"Yt\")\np2 &lt;- forecast::ggAcf(shampoo) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\np3 &lt;- ggplot2::autoplot(diff(shampoo)) + \n    xlab(\"Year\") + \n    ylab(\"Sales\") + \n    ggtitle(\"D1\")\np4 &lt;- forecast::ggAcf(diff(shampoo)) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\np5 &lt;- ggplot2::autoplot(diff(shampoo, differences = 2)) + \n    xlab(\"Year\") + \n    ylab(\"Sales\") +\n    ggtitle(\"D2\")\np6 &lt;- forecast::ggAcf(diff(shampoo, differences = 2)) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\")\n(p1 + p2) / (p3 + p4) / (p5 + p6) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.23: Time series plot of shampoo sales with an estimated ACF and the differenced series with their ACFs.\n\n\n\n\n\n\nA deterministic linear trend can be also eliminated by differencing. For example, consider a time series \\(X-t\\) with a deterministic linear trend: \\[\nX_t = a + b t + Z_t,\n\\] where \\(Z_t \\sim \\mathrm{WN}(0,\\sigma^2)\\). The first-order differences of \\(X_t\\) remove the linear trend: \\[\n\\begin{align}\n(1 - B)X_t &= X_t - X_{t-1} \\\\\n&= a + b t + Z_t - (a + b (t - 1) + Z_{t-1}) \\\\\n&= a + b t + Z_t - a - b t + b - Z_{t-1} \\\\\n&= b + Z_t + Z_{t-1}.\n\\end{align}\n\\]\n\n\n\n\n\n\nNoteExample: Airline passengers detrending and deseasonalizing by differencing\n\n\n\nRemove trend and seasonality from the log-transformed AirPassengers series by applying the differences.\nBased on Figure 3.24, differencing once with the non-seasonal and the seasonal lags is enough to remove the trend and strong seasonality. The final series (denoted as D1D12) is \\[\n\\begin{split}\nD1D12_t & = (1 - B)(1 - B^{12})\\lg Y_t = (1 - B - B^{12} + B^{13})\\lg Y_t\\\\\n& = \\lg Y_t - \\lg Y_{t-1} - \\lg Y_{t-12} + \\lg Y_{t-13}.\n\\end{split}\n\\]\n\nCodeYt &lt;- log10(AirPassengers)\n\n# Apply first-order (non-seasonal) differences\nD1 &lt;- diff(Yt)\n\n# Additionally, apply first-order seasonal differences\nD1D12 &lt;- diff(D1, lag = 12)\n\np1 &lt;- ggplot2::autoplot(Yt) + \n    xlab(\"Year\") + \n    ylab(\"log10(Air passangers)\") + \n    ggtitle(\"Yt\")\np2 &lt;- forecast::ggAcf(Yt) + \n    ggtitle(\"Yt\") +\n    xlab(\"Lag (months)\")\np3 &lt;- ggplot2::autoplot(D1) + \n    xlab(\"Year\") + \n    ylab(\"log10(Air passangers)\") + \n    ggtitle(\"(1-B)Yt\")\np4 &lt;- forecast::ggAcf(D1) + \n    ggtitle(\"(1-B)Yt\") +\n    xlab(\"Lag (months)\")\np5 &lt;- ggplot2::autoplot(D1D12) + \n    xlab(\"Year\") + \n    ylab(\"log10(Air passangers)\") + \n    ggtitle(\"(1-B)(1-B12)Yt\")\np6 &lt;- forecast::ggAcf(D1D12) + \n    ggtitle(\"(1-B)(1-B12)Yt\") +\n    xlab(\"Lag (months)\")\n(p1 + p2) / (p3 + p4) / (p5 + p6) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 3.24: Time series plot of the airline passenger series with an estimated ACF and the detrended (differenced) series with their ACFs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Smoothing, Detrending, and Deseasonalizing</span>"
    ]
  },
  {
    "objectID": "l03_smoothing.html#conclusion",
    "href": "l03_smoothing.html#conclusion",
    "title": "\n3  Smoothing, Detrending, and Deseasonalizing\n",
    "section": "\n3.8 Conclusion",
    "text": "3.8 Conclusion\nWe have implemented various methods for trend visualization, modeling, and trend removal. In each case, the method could be extended to also remove a strong seasonal signal. The choice of a method depends on many things, such as:\n\nThe methods require different skill levels (and time commitment) for their implementation and can be categorized as automatic and non-automatic.\nSome methods are very flexible when dealing with seasonality (e.g., Holt–Winters), while others are not.\nThe goal of smoothing and desired degree of smoothness in the output.\nSome of the methods have a convenient way of getting forecasts (e.g., exponential smoothing and polynomial regression), while other methods are more suitable for interpolation (polynomial regression) and simple visualization (simple moving average), or just trend elimination (all of the considered methods are capable of eliminating trends, but differencing is the method that does nothing else but the elimination).\n\nRemember that forecasted values (point forecasts) are not valuable if their uncertainty is not quantified. To quantify the uncertainty, we provide prediction intervals that are based on the past behavior of residuals and careful residual diagnostics (such as described in the previous lectures).\nThe evaluation of the methods may involve splitting the time series into training and testing periods. Overall, this lecture focused on presenting the methods themselves, while residual diagnostics and evaluation on a testing set were omitted due to time constraints.\nWhile the considered methods are among the most common, there are many more smoothing methods that can be applied to time series, including machine learning techniques.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Smoothing, Detrending, and Deseasonalizing</span>"
    ]
  },
  {
    "objectID": "l03_smoothing.html#appendix",
    "href": "l03_smoothing.html#appendix",
    "title": "\n3  Smoothing, Detrending, and Deseasonalizing\n",
    "section": "\n3.9 Appendix",
    "text": "3.9 Appendix\n\n\n\n\n\n\nNoteExample: Smoothing blocks\n\n\n\nA student was concerned about applying smoothing to data with a block-like structure. Without a real dataset on hand, below we simulate a series \\(X_t\\) with the mentioned structure.\n\nCodeset.seed(123)\n\n# Set number of blocks\nn &lt;- 10 \n\n# Let the values of blocks come from standard normal distribution,\n# and length of block be a random variable generated from Poisson distribution:\nXt &lt;- rep(rnorm(n), times = rpois(n, 5))\n\n\nFigure 3.25 shows an example of lowess applied to \\(X_t\\) to get a smoothed series \\(M_t\\).\nWe see that lowess estimates smooth trends. If we assume that trends are not smooth, then some other techniques (e.g., piecewise linear estimation) should be used.\n\nCodet &lt;- c(1:length(Xt))\nggplot(data.frame(t = t, Xt = Xt), aes(x = t, y = Xt)) + \n    geom_line() + \n    geom_smooth(se = FALSE, span = 0.25)\n\n\n\n\n\n\nFigure 3.25: Smoothing of the block-like data \\(X_t\\).\n\n\n\n\n\n\n\n\n\n\nBerk RA (2016) Statistical learning from a regression perspective, 2nd edn. Springer, Switzerland\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nCleveland WS (1979) Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association 74:829–836. https://doi.org/10.1080/01621459.1979.10481038\n\n\nHastie TJ, Tibshirani RJ, Friedman JH (2009) The elements of statistical learning: Data mining, inference, and prediction, 2nd edn. Springer, New York, NY, USA\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern time series analysis. Springer-Verlag, Berlin, Germany\n\n\nKohn R, Schimek MG, Smith M (2000) Spline and kernel regression for dependent data. In: Schimek MG (ed) Smoothing and regression: Approaches, computation, and application. John Wiley & Sons, Inc., New York, pp 135–158\n\n\nWood SN (2006) Generalized additive models: An introduction with r. Chapman; Hall/CRC, New York, NY, USA",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Smoothing, Detrending, and Deseasonalizing</span>"
    ]
  },
  {
    "objectID": "l04_arma.html",
    "href": "l04_arma.html",
    "title": "\n4  Autoregressive Moving Average (ARMA) Models\n",
    "section": "",
    "text": "4.1 Introduction\nThe goal of this lecture is to introduce a broad class of models for stationary time series – autoregressive moving average (ARMA) models. You should recognize the difference between the AR and MA components and learn how to implement these models in practice.\nObjectives\nReading materials\nAudio overview\nAfter removing or modeling a trend and strong seasonality in a time series, we need to take care of the stationary remainder, which may be autocorrelated or conditionally heteroskedastic. Here we introduce a broad class of ARMA models that deal with autocorrelation in time series.\nRecall that a time series is (weakly) stationary if its mean and autocovariance do not depend on time, \\(t\\); autocovariance depends only on the lag \\(h\\). We will use these two conditions to show that the time series processes that we will model as AR, MA, or ARMA are stationary. Remember that the presence of autocorrelation does not mean that there is a trend.\nWe start by checking the two conditions for other typical univariate processes: white noise and random walk.\nCode# Set seed for reproducible random number generation\nset.seed(123) \nT &lt;- 300L\nLag &lt;- 0:15\nMost of the examples in this lecture are simulated to show how sample autocorrelations may differ from theoretical ones, even if the simulated time series are sampled from a process of a certain type. All simulated series are of length 300.\nSee Figure 4.1 with plots for \\(X_t \\sim \\mathrm{WN}(0, \\sigma^2)\\), where \\(\\sigma = 2\\). More specifically, \\[\nX_t \\sim \\text{i.i.d.}\\; N(0, \\sigma^2);\\; \\sigma = 2.\n\\tag{4.1}\\]\nCode# Theoretical ACF, PACF\nRHO &lt;- c(1, rep(0, max(Lag)))\n\n# Sample data\nX &lt;- ts(rnorm(T, sd = 2))\n\n# Plots\np1 &lt;- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\")\np2 &lt;- ggplot(data.frame(Lag, PACF = RHO), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\")\np3 &lt;- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\")\np4 &lt;- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\")\np5 &lt;- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\")\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 4.1: White noise as specified in Equation 4.1.\nRandom walk patterns are widely found in nature, for example, in the phenomenon of Brownian motion. Notice the slow linear decay in the ACF of nonstationary time series (Figure 4.2) \\[\nS_t = \\sum_{i = 1}^t X_i,\n\\tag{4.2}\\] where \\(X_t \\sim \\mathrm{i.i.d.}(0, \\sigma^2)\\) and \\(\\sigma = 2\\).\nCode# Sample data\nRW0 &lt;- ts(cumsum(X))\n\n# Plots\np3 &lt;- forecast::autoplot(RW0) + \n    ggtitle(\"Sample time series\")\np4 &lt;- forecast::ggAcf(RW0) + \n    ggtitle(\"Sample ACF\")\np5 &lt;- forecast::ggAcf(RW0, type = \"partial\") + \n    ggtitle(\"Sample PACF\")\np3 + p4 + p5 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 4.2: Zero-mean random walk as specified in Equation 4.2.\nRandom walk may look very different if the i.i.d. process has a non-zero mean. Here is a random walk with drift (Figure 4.3): \\[\nS_t = \\sum_{i = 1}^t X_i,\n\\tag{4.3}\\] where \\(X_t \\sim \\mathrm{i.i.d.}(a, \\sigma^2)\\), \\(a = 0.2\\), and \\(\\sigma = 2\\).\nCode# Sample data\nRW2 &lt;- ts(cumsum(X + 0.2))\n\n# Plots\np3 &lt;- forecast::autoplot(RW2) + \n    ggtitle(\"Sample time series\")\np4 &lt;- forecast::ggAcf(RW2) + \n    ggtitle(\"Sample ACF\")\np5 &lt;- forecast::ggAcf(RW2, type = \"partial\") + \n    ggtitle(\"Sample PACF\")\np3 + p4 + p5 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 4.3: Random walk with drift as specified in Equation 4.3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autoregressive Moving Average (ARMA) Models</span>"
    ]
  },
  {
    "objectID": "l04_arma.html#introduction",
    "href": "l04_arma.html#introduction",
    "title": "\n4  Autoregressive Moving Average (ARMA) Models\n",
    "section": "",
    "text": "NoteExample: Check the stationarity of white noise\n\n\n\nLet \\(X_t\\) be a white noise series, \\(X_t \\sim \\mathrm{WN}(0, \\sigma^2)\\). Show that \\(X_t\\) is weakly stationary.\n\\(\\mathrm{E}(X_t) = 0\\) for all \\(t\\) \\(\\Rightarrow\\) \\(\\mathrm{E}(X_t)\\) is independent of \\(t\\).\nNow check the autocovariance \\[\n\\begin{align}\n\\mathrm{cov}(X_t, X_{t+h}) &=\n\\begin{cases}\n\\mathrm{cov}(X_t, X_t) & \\text{if } h = 0\\\\\n0                      & \\text{otherwise}\n\\end{cases} \\\\\n& =\n\\begin{cases}\n\\mathrm{var}(X_t) & \\text{if } h = 0\\\\\n0                 & \\text{otherwise}\n\\end{cases} \\\\\n& =\n\\begin{cases}\n\\sigma^2 & \\text{if } h = 0\\\\\n0        & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\\]\n\\(\\Rightarrow\\) \\(\\mathrm{cov}(X_t, X_{t+h})\\) does not depend on \\(t\\), depends only on \\(h\\).\nBoth conditions of weak stationarity are satisfied, \\(X_t\\) is weakly stationary.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that ACF(0) = PACF(0) = 1 always. Some R plotting functions show results for the zero lag (e.g., acf()), some do not (e.g., pacf() and the function forecast::ggAcf() used here).\n\n\n\n\n\n\n\n\nNoteExample: Check the stationarity of random walk\n\n\n\nCheck the stationarity of the random walk \\(S_t\\). \\(S_t = \\sum_{i=1}^t{X_t}\\), where \\(X_t \\sim \\text{i.i.d.}(0,\\sigma^2)\\), and \\(S_0 = 0\\).\n\\(\\mathrm{E}(S_t) = \\mathrm{E}(\\sum_{i=1}^t{X_t}) = 0\\) \\(\\Rightarrow\\) \\(\\mathrm{E}(X_t)\\) does not depend on \\(t\\).\nNow check the autocovariance \\[\n\\begin{split}\n\\mathrm{cov}(S_t, S_{t+h}) &= \\mathrm{cov}(S_t, S_t + X_{t+1}+\\dots + X_{t+h})\\\\\n& = \\mathrm{cov}(S_t, S_t) + \\mathrm{cov}(S_t, X_{t+1}+\\dots + X_{t+h})\\\\\n& = \\mathrm{var}(S_t) + 0\\\\\n& = \\mathrm{var}\\left(\\sum_{i=1}^t{X_t}\\right) \\\\\n& = \\sum_{i=1}^t\\mathrm{var}(X_t)\\\\\n& = t\\sigma^2\n\\end{split}\n\\]\nAutocovariance of \\(S_t\\) depends on time, \\(S_t\\) is not stationary. Also, notice that \\(S_t = S_{t-1} + X_t\\), that is, \\(S_t\\) is a nonstationary AR(1) process with \\(\\phi_1 = 1\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autoregressive Moving Average (ARMA) Models</span>"
    ]
  },
  {
    "objectID": "l04_arma.html#autoregressive-ar-models",
    "href": "l04_arma.html#autoregressive-ar-models",
    "title": "\n4  Autoregressive Moving Average (ARMA) Models\n",
    "section": "\n4.2 Autoregressive (AR) models",
    "text": "4.2 Autoregressive (AR) models\nAR(1) model\nWe are already familiar with an AR(1) model \\[\nY_{t} = \\phi_{1}  Y_{t -1} + \\epsilon_{t},\n\\] where\n\n\n\\(\\epsilon_{t}\\) is white noise (\\(\\mathrm{E}(\\epsilon_{t}) = 0\\) and \\(\\mathrm{E}(\\epsilon_{t}^2) = \\sigma^{2}_{\\epsilon}\\)),\n\n\\(\\epsilon_{t}\\) is independent of \\(Y_{t - k}, \\dots, Y_{t - 1}\\),\n\n\\(\\phi_{1}\\) is the coefficient of autoregression.\n\nThe AR(1) model can be rewritten as \\[\n(1 - \\phi_{1} B) Y_{t} = \\epsilon_{t},\n\\] or in even more compact notation as \\[\n\\phi(B)Y_t = \\epsilon_t,\n\\] where \\(\\phi(\\lambda)\\) is the polynomial \\(\\phi (\\lambda) = 1 - \\phi_{1} \\lambda\\).\nLet us find the variance of \\(Y_{t}\\) \\[\n\\begin{split}\n\\mathrm{var} (Y_{t} ) = \\mathrm{E} ( \\phi_{1} Y_{t - 1} + \\epsilon_{t})^{2} & =  \\mathrm{E} ( \\phi^{2}_{1} Y^{2}_{t - 1} + 2 \\phi_{1} Y_{t-1}  \\epsilon_{t} + \\epsilon^{2}_{ t}) \\\\\n\\\\\n& = \\phi^{2}_{1} \\mathrm{E}(Y^{2}_{t - 1}) + 2\\phi_{1} \\mathrm{E} (Y_{t - 1} \\epsilon_{t}) + \\mathrm{E}(\\epsilon^{2}_{t}) \\\\\n\\\\\n& = \\phi^{2}_{1} \\mathrm{var} \\left( Y_{t -1} \\right) + 0 + \\sigma^{2}_{\\epsilon} \\\\\n\\\\\n& = \\phi^{2}_{1} \\mathrm{var} \\left( Y_{t -1} \\right) + \\sigma^{2}_{\\epsilon}.\n\\end{split}\n\\]\nHence, if \\(\\phi_{1} \\neq \\pm 1\\), then \\[\n\\sigma^{2}_{Y} = \\frac{\\sigma^{2}_{\\epsilon}} {1 - \\phi^{2}_{1}}.\n\\tag{4.4}\\]\n\n\n\n\n\n\nNote\n\n\n\nFor Equation 4.4 to make sense, we require \\(|\\phi_{1}| &lt; 1\\). This is the condition under which AR(1) process is (weakly) stationary. E.g., if \\(\\phi_{1} = \\pm 1\\) then the process is a random walk and is also not stationary. We shall see later that all AR processes require some condition of this nature.\n\n\n\n\n\n\n\n\nNoteExample: Check the stationarity of AR(1)\n\n\n\nCheck that the AR(1) process \\(Y_t\\) is stationary, given that \\(Y_t = \\phi Y_{t-1} + \\epsilon_t\\), \\(|\\phi| &lt; 1\\), and \\(\\epsilon_t \\sim \\mathrm{WN}(0, \\sigma^2)\\).\n\\[\n\\begin{split}\n\\mathrm{E}(Y_t) &= \\mathrm{E}(\\phi Y_{t-1} + \\epsilon_t)\\\\\n& = \\phi \\mathrm{E}(Y_{t-1}) + \\mathrm{E}(\\epsilon_t)\\\\\n& = \\phi^2 \\mathrm{E}(Y_{t-2}) + 0\\\\\n& = \\phi^3 \\mathrm{E}(Y_{t-3})\\\\\n& = \\dots\\\\\n& = \\phi^N \\mathrm{E}(Y_{t-N})\\\\\n\\lim_{N \\rightarrow \\infty} \\mathrm{E}(Y_t) &= \\lim_{N \\rightarrow \\infty} \\phi^N \\mathrm{E}(Y_{t-N}) = 0\n\\end{split}\n\\] Therefore, the mean of \\(Y_t\\) does not depend on time.\nNow check the autocovariance. For \\(h=0\\), \\(\\mathrm{cov}(Y_t, Y_{t}) = \\mathrm{var}(Y_t) = \\sigma^2_Y\\), which is derived in Equation 4.4 and does not depend on time.\nFor \\(h \\neq 0\\), \\[\n\\begin{split}\n\\mathrm{cov}(Y_t, Y_{t-h}) &= \\mathrm{cov}(\\phi Y_{t-1} + \\epsilon_t, Y_{t-h})\\\\\n& = \\mathrm{cov}(\\phi Y_{t-1}, Y_{t-h}) + \\mathrm{cov}(\\epsilon_t, Y_{t-h})\\\\\n& = \\phi \\mathrm{cov}(Y_{t-1}, Y_{t-h}) + 0\\\\\n& = \\phi \\mathrm{cov}(\\phi Y_{t-2} + \\epsilon_{t-1}, Y_{t-h})\\\\\n& = \\phi^2 \\gamma_Y(h-2)\\\\\n& = \\dots\\\\\n& = \\phi^h \\gamma_Y(0) \\\\\n& = \\phi^h \\sigma_Y^2,\n\\end{split}\n\\] thus the autocovariance does not depend on time, depends only on lag \\(h\\).\nBoth conditions for weak stationarity are satisfied, the \\(Y_t\\) specified above is weakly stationary.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdditionally, see that the ACF of the AR(1) process \\[\n\\rho_Y(h) = \\frac{\\gamma_Y(h)}{\\gamma_Y(0)} = \\frac{\\phi^h \\gamma_Y(0)}{\\gamma_Y(0)} = \\phi^h\n\\tag{4.5}\\] shows exponential decay. Remember this about AR processes.\n\n\nAR(\\(p\\)) model\nNow we can extend our AR(1) model to \\[\n\\begin{split}\nY_{t} &= \\phi_{1} Y_{t  - 1} + \\phi_{2} Y_{t - 2} + \\dots + \\phi_{p} Y_{t - p} + \\epsilon_{t} \\\\\n&=\\sum_{i=1}^p\\phi_iY_{t-i} + \\epsilon_t\n\\end{split}\n\\tag{4.6}\\] with \\(\\epsilon_{t}\\) independent of \\(Y_{t - k}, \\dots , Y_{t - 1}\\); \\(p \\in \\mathbb{N}^{+}\\). This model is called an autoregressive model of order \\(p\\), or simply an AR(\\(p\\)) model.\nSimilarly to AR(1), every AR(\\(p\\)) model may be rewritten as \\[\n\\left(1 -  \\sum^{p}_{i = 1} \\phi_{i} B^{i} \\right) Y_{t} = \\epsilon_{t},\n\\] or in more compact notation as \\[\n\\phi (B) Y_t = \\epsilon_t,\n\\] where \\(\\phi(\\lambda)\\) is the polynomial \\(\\phi(\\lambda) = 1 - \\phi_{1} \\lambda - \\phi_{2} \\lambda^{2} - \\dots - \\phi_{p} \\lambda^{p}\\).\nThe stationarity condition for AR(\\(p\\)) model is defined through the polynomial \\(\\phi(\\lambda)\\). Specifically, all the roots of the function \\(\\phi(\\lambda)\\) lie outside the unit circle in the complex plane.\nACF of AR processes\nTo compute ACVF and ACF for the AR(\\(p\\)) model, multiplying both sides of Equation 4.6 by \\(Y_{t - k}\\) for \\(k \\geqslant p\\) and taking expectations yields \\[\n\\begin{split}\n\\mathrm{E} \\left( Y_{t} Y_{t - k} \\right) & =  \\mathrm{E} \\left( \\phi_{1} Y_{t-1} Y_{t-k} + \\phi_{2} Y_{t-2} Y_{t-k} + \\dots + \\phi_{p} Y_{t -p} Y_{t - k} + \\epsilon_{t} Y_{t -k} \\right) \\\\\n& =  \\phi_{1}  \\mathrm{E} \\left( Y_{t  - 1} Y_{t - k} \\right) + \\phi_{2} \\mathrm{E} \\left( Y_{t - 2} Y_{t - k} \\right) + \\dots+ \\phi_{p} \\mathrm{E} \\left(Y_{t - p} Y_{t - k} \\right) + \\mathrm{E} \\left( \\epsilon_{t} Y_{t  - k} \\right).\n\\end{split}\n\\]\nThus, we get autocovariances \\[\n\\gamma(k) = \\phi_{1}\\gamma(k -1) + \\phi_{2}\\gamma(k - 2) + \\dots + \\phi_{p} \\gamma(k  - p),\n\\] which turns into a mixed exponential decay of order \\(p\\) for the ACF \\[\n\\rho(k) = \\phi_{1} \\rho(k - 1) + \\phi_{2} \\rho(k - 2) + \\dots + \\phi_{p} \\rho(k - p).\n\\]\nTo identify the decay, we need the \\(p\\) starting correlations \\(\\rho(1), \\rho(2), \\dots, \\rho(p)\\), which we can find from the parameters \\(\\phi_{1}, \\phi_{2}, \\dots, \\phi_{p}\\).\nPACF of AR processes\nAn AR(\\(p\\)) model implies \\(Y_{t+h} = \\sum_{i=1}^p\\phi_{i} Y_{t + h - i} + \\epsilon_{t+h}\\), where the roots of \\(\\phi(\\lambda)\\) are outside the unit circle. When \\(h &gt; p\\), the regression of \\(Y_{t+h}\\) on \\(\\{ Y_{t+1}, \\dots, Y_{t+h-1}\\}\\) is \\[\n\\hat{Y}_{t+h} = \\sum^p_{i=1} \\phi_i Y_{t+h-i}.\n\\] Thus, when \\(h &gt; p\\), \\[\n\\begin{align}\n\\rho_{hh} &= \\mathrm{cor}(Y_{t+h}-\\hat{Y}_{t+h}, Y_t - \\hat{Y}_t) \\\\\n& = \\mathrm{cor}(\\epsilon_{t+h}, Y_t - \\hat{Y}_t) \\\\\n& = 0,\n\\end{align}\n\\] because, by the causality of the process, the difference \\(Y_t - \\hat{Y}_t\\) depends only on \\(\\{ \\epsilon_{t+h-1}, \\epsilon_{t+h-2},\\dots \\}\\).\nWhen \\(h\\leqslant p\\), \\(\\rho_{hh}\\) is not zero, and \\(\\rho_{11}, \\dots, \\rho_{p-1,p-1}\\) are not necessarily zero.\n\n\n\n\n\n\nNote\n\n\n\nThe important conclusion is that the PACF of an AR(\\(p\\)) process necessarily cuts off (PACF = 0) after lag \\(p\\).\n\n\n\n\n\n\n\n\nNoteExample: PACF of AR(1) process\n\n\n\nFor an AR(1) process, the best linear predictor \\(P(X_{n + 1} | X_n) = \\phi X_n\\).\nFor \\(h = 0\\), \\(\\rho_{00} = 1\\).\nFor \\(h = 1\\), \\(\\rho_{11} = \\mathrm{cor}(X_t, X_{t+1}) = \\phi\\) (no intermediate lags, see Equation 4.5).\nFor \\(h = 2\\), note that \\(X_{t+2} = \\phi X_{t+1} + \\epsilon_{t+2}\\), then \\[\n\\begin{align}\n\\rho_{22} &= \\mathrm{cor}(X_t - P(X_t | X_{t+1}), X_{t+2} - P(X_{t+2} | X_{t+1})) \\\\\n&= \\mathrm{cor}(X_t - P(X_t | X_{t+1}), X_{t+2} - \\phi X_{t+1})\\\\\n&= \\mathrm{cor}(X_t - P(X_t | X_{t+1}), \\epsilon_{t+2})\\\\\n&= 0\n\\end{align}\n\\] because the term \\(X_t - P(X_t | X_{t+1})\\) is a function of \\(X_t\\) and \\(X_{t+1}\\) only, while \\(\\epsilon_{t+2}\\) is the future noise.\nThus, for \\(X_t \\sim\\) AR(1), the PACF \\[\n\\rho_{hh} =\n\\begin{cases}\n1 & \\text{if } h = 0\\\\\n\\phi & \\text{if } h = 1\\\\\n0 & \\text{if } h \\geqslant 2\n\\end{cases}\n\\]\n\n\nSee the plots for the following AR(1) process in Figure 4.4: \\[\nX_t = \\phi_1 X_{t-1} + \\epsilon_t,\n\\tag{4.7}\\] where \\(\\epsilon_t \\sim N(0,\\sigma^2)\\), \\(\\phi_1=0.6\\), and \\(\\sigma = 1\\).\n\nCode# Theoretical ACF, PACF\nphi &lt;- 0.6\nRHO &lt;- phi^Lag\nALPHA &lt;- c(1, phi, rep(0, max(Lag) - 1))\n\n# Sample data\nX &lt;- arima.sim(list(order = c(1, 0, 0), ar = phi), n = T)\n\n# Plots\np1 &lt;- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\")\np2 &lt;- ggplot(data.frame(Lag, PACF = ALPHA), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\")\np3 &lt;- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\")\np4 &lt;- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\")\np5 &lt;- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\")\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 4.4: AR(1) process as specified in Equation 4.7.\n\n\n\n\nSee the plots for the following AR(1) process in Figure 4.5 (only the coefficient \\(\\phi_1\\) changed from the previous specification): \\[\nX_t = \\phi_1 X_{t-1} + \\epsilon_t;\\; \\epsilon_t \\sim N(0,\\sigma^2);\\; \\phi_1=-0.6,\\; \\sigma = 1.\n\\tag{4.8}\\]\n\nCode# Theoretical ACF, PACF\nphi &lt;- -0.6\nRHO &lt;- phi^Lag\nALPHA &lt;- c(1, phi, rep(0, max(Lag) - 1))\n\n# Sample data\nX &lt;- arima.sim(list(order = c(1, 0, 0), ar = phi), n = T)\n\n# Plots\np1 &lt;- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\")\np2 &lt;- ggplot(data.frame(Lag, PACF = ALPHA), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\")\np3 &lt;- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\")\np4 &lt;- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\")\np5 &lt;- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\")\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 4.5: AR(1) process as specified in Equation 4.8.\n\n\n\n\nACF of the AR(1) model takes two graphical forms depending on whether the coefficient \\(\\phi_{1}\\) is positive or negative. In the first case, the decay occurs through the positive axis only (Figure 4.4). In the second case, the decay alternates between the negative and positive axes (Figure 4.5). This is so-called mixed exponential decay. Although both forms are examples of exponential decay, they appear different visually.\nAs the order \\(p\\) of an AR(\\(p\\)) model increases, the number of different visual presentations of the ACF increases as well. The ACF of an AR(\\(p\\)) model can take \\(2^{p}\\) different graphical forms, depending on the signs (positive or negative) of the parameters \\(\\phi_{1}, \\phi_{2}, \\dots, \\phi_{p}\\). Thus, the ACF of an AR(2) model can take \\(2^{2} = 4\\) different graphical forms.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autoregressive Moving Average (ARMA) Models</span>"
    ]
  },
  {
    "objectID": "l04_arma.html#the-yulewold-representation-and-linear-processes",
    "href": "l04_arma.html#the-yulewold-representation-and-linear-processes",
    "title": "\n4  Autoregressive Moving Average (ARMA) Models\n",
    "section": "\n4.3 The Yule–Wold representation and linear processes",
    "text": "4.3 The Yule–Wold representation and linear processes\nA process \\(X_{t}\\), (\\(t = 0, \\pm 1, \\pm 2, \\dots\\)) is said to be linear if it has a representation of the form \\[\nX_{t} = \\mu + \\sum^{\\infty}_{r = - \\infty} c_{r} \\epsilon_{t - r},\n\\] where \\(\\mu\\) is a common mean, \\(c_{r}\\) is a sequence of fixed constants, and \\(\\epsilon_{t}\\) are uncorrelated random variables with mean 0 and common variance.\nWe assume \\(\\sum c^{2}_{r} &lt; \\infty\\) to ensure that the variances of the individual \\(X_{t}\\) are finite (stationarity and existence condition). Then the process \\(X_{t}\\) is necessarily (weakly) stationary.\nIf we also require that \\(\\epsilon_{t}\\) are identically distributed, then \\(X_{t}\\) is strictly stationary. For example, see the case of normally distributed \\(\\epsilon_{t}\\).\nIf \\(c_r = 0\\) for all \\(r &lt; 0\\), \\(X_t\\) is called causal (i.e., the process at the time \\(t\\) does not depend on the future, yet unobserved, values of \\(\\epsilon_{t}\\)).\nThe representation of a causal stationary process (depending only on the past) \\(X_{t}\\) with zero mean in the form \\[\nX_{t} = \\sum^{\\infty}_{r = 0} c_{r} \\epsilon_{t - r},\n\\tag{4.9}\\] where \\(\\epsilon_{t} \\sim \\mathrm{WN}(0, \\sigma^{2}\\)) and \\(\\sum c^{2}_{r} &lt; \\infty\\), is sometimes called the Yule–Wald representation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autoregressive Moving Average (ARMA) Models</span>"
    ]
  },
  {
    "objectID": "l04_arma.html#moving-average-ma-models",
    "href": "l04_arma.html#moving-average-ma-models",
    "title": "\n4  Autoregressive Moving Average (ARMA) Models\n",
    "section": "\n4.4 Moving average (MA) models",
    "text": "4.4 Moving average (MA) models\nMA(1) model\nA zero mean stationary process \\(Y_{t}\\) is called a moving average process of order 1, or an MA(1) model, if \\(Y_{t}\\) satisfies \\[\nY_{t} = \\epsilon_{t} + \\theta_{1}  \\epsilon_{t - 1},\n\\] where \\(\\epsilon_{t}\\) is white noise.\nIn other words, an MA(1) process has \\(c_{r} = 0\\) for every \\(r &gt; 1\\) in its Yule–Wold representation Equation 4.9.\nThe MA(1) model may be rewritten as \\[\nY_{t} = (1 + \\theta_{1} B) \\epsilon_{t},\n\\] or in the compact form \\[\nY_t = \\theta (B) \\epsilon_t,\n\\] where \\(\\theta (\\lambda)\\) is the polynomial \\(\\theta(\\lambda) = 1 + \\theta_{1} \\lambda\\).\nMA(q) model\nWe can extend MA(1) model further and consider \\[\n\\begin{split}\nY_{t} &= \\epsilon_{t} + \\theta_{1} \\epsilon_{t -1} + \\theta_{2} \\epsilon_{t - 2} + \\dots + \\theta_{q} \\epsilon_{t - q} \\\\\n&= \\epsilon_{t} + \\sum_{i=1}^q \\theta_i \\epsilon_{t-i}.\n\\end{split}\n\\] This model is called a moving average model of order \\(q\\), or a MA(\\(q\\)).\nWe can write down MA(\\(q\\)) as \\[\nY_{t} = \\left(1 + \\theta_{1} B + \\theta_{2} B^{2} + \\dots + \\theta_{q} B^{q} \\right) \\epsilon_{t} ,\n\\] or in the compact form \\[\nY_t = \\theta (B) \\epsilon_t,\n\\] where \\(\\theta(\\lambda)\\) is the polynomial \\(\\theta(\\lambda) = 1 + \\theta_{1} \\lambda + \\theta_{2} \\lambda^{2} + \\dots + \\theta_{q} \\lambda^{q}\\).\nNow compute the ACVF and ACF of the MA(\\(q\\)) process \\(Y_{t}\\). Using the Yule–Wald representation of \\(Y_{t}\\), \\[\nY_{t} = \\sum^{q}_{r = 0} \\theta_{r} \\epsilon_{t - r},\n\\] the autocovariance \\[\n\\begin{split}\n\\gamma_Y(h) & = \\mathrm{cov}(Y_{t}, Y_{t+h}) = \\mathrm{cov} \\left( \\sum^{q}_{r = 0} \\theta_{r }\\epsilon_{t - r}, \\sum^{q}_{\\ell = 0} \\theta_{\\ell} \\epsilon_{t + h - \\ell} \\right) \\\\\n\\\\\n& = \\sum^{q}_{r = 0} \\sum^{q}_{\\ell = 0} \\mathrm{cov} \\left( \\theta_{r } \\epsilon_{t - r},  \\theta_{\\ell} \\epsilon_{t + h - \\ell} \\right) \\\\\n\\\\\n& = \\sum^{q}_{r = 0} \\sum^{q}_{\\ell = 0} \\theta_{r } \\theta_{\\ell} \\mathrm{cov}  \\left( \\epsilon_{t - r}, \\epsilon_{t - r}  \\right) ~~ \\text{since cov} \\left( \\epsilon_{u}, \\epsilon_{v}  \\right) = 0 ~ \\text{unless} ~ u = v \\\\\n\\\\\n& = \\sigma_{\\epsilon}^{2} \\sum^{q}_{r = 0} \\theta_{r} \\theta_{r+h}.\n\\end{split}\n\\] Note that the case of no time shift between the noise series, i.e., when \\(t-r = t+h-l\\), leads to \\(l=r+h\\), which is used in the last row.\nHence, \\[\n\\gamma_Y(h) = \\left\\{\n\\begin{array}{lcl}\n\\sigma_{\\epsilon}^{2} \\sum^{q  - h}_{r = 0} \\theta_{r} \\theta_{r+h} & \\text{for} &  h \\leqslant q;\\\\\n0 & \\text{for} & h &gt; q.\n\\end{array}\n\\right.\n\\tag{4.10}\\]\nThis leads us to the most important result about the correlation structure for MA(\\(q\\)) processes. For an MA(\\(q\\)) process, \\(\\gamma(h) = 0\\) for all \\(h &gt; q\\). Equivalently, the ACF has \\(q\\) ‘starting’ correlations or spikes, \\(\\rho(1), \\rho(2), \\dots, \\rho(q)\\), and then \\(\\rho(h) = 0\\) for \\(h &gt; q\\). We say that the ACF ‘cuts off’ after the lag \\(q\\).\nNow compute the actual values of the starting correlations \\(\\rho(1), \\dots, \\rho(q)\\) in terms of the parameters \\(\\theta_{0} = 1, \\theta_{1}, \\theta_{2}, \\dots, \\theta_{q}\\). Using Equation 4.10, we get\n\n\nFor the MA(1) model \\(q = 1\\). Hence \\[\n\\begin{split}\nh  =  0 &\\rightarrow \\gamma(0) = \\sigma^{2} \\sum^{1}_{ r=0}  \\theta^{2}_{r} = \\sigma^{2} \\left( \\theta^{2}_{0} + \\theta^{2}_{1} \\right) = \\sigma^{2} \\left(1 + \\theta^{2}_{1}\\right). \\\\\n\\\\\nh  =  1 &\\rightarrow \\gamma(1) = \\sigma^{2}  \\sum^{0}_{ r=0}  \\theta_{r} \\theta_{r + 1} = \\sigma^{2} \\theta_{0} \\theta_{1} = \\sigma^{2} \\theta_{1}.\n\\end{split}\n\\]\nTherefore \\(\\rho(1) = \\gamma(1) / \\gamma(0) = \\theta_{1} / (1 + \\theta^{2}_{1})\\).\n\nFor the MA(2) model \\(q = 2\\). Hence \\[\n\\begin{split}\nh =  0 &\\rightarrow \\gamma(0) = \\sigma^{2} \\sum^{2}_{r=0}  \\theta^{2}_{r} = \\sigma^{2} \\left(1 + \\theta^{2}_{1} + \\theta^{2}_{2}\\right). \\\\\n\\\\\nh  =  1 &\\rightarrow \\gamma(1) = \\sigma^{2} \\sum^{1}_{r=0}  \\theta_{r} \\theta_{r + 1} = \\sigma^{2} \\left(\\theta_{1} + \\theta_{1} \\theta_{2} \\right) = \\sigma^{2} \\theta_{1}  \\left(1 + \\theta_{2}\\right). \\\\\n\\\\\nh  =  2 &\\rightarrow \\gamma(2) = \\sigma^{2} \\sum^{0}_{r=0}  \\theta_{r} \\theta_{r + 2} = \\sigma^{2} \\theta_{2}.\n\\end{split}\n\\] Therefore \\(\\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = \\frac{\\theta_{1} \\left(1 + \\theta_{2} \\right)}{1+ \\theta^{2}_{1} + \\theta^{2}_{2}}\\) and \\(\\rho(2) = \\frac{\\gamma(2)}{\\gamma(0)} = \\frac{\\theta_{2}}{1+ \\theta^{2}_{1} + \\theta^{2}_{2}}\\).\nProceed analogously for the MA(\\(q\\)) model.\n\nAnother method of obtaining the correlation coefficients of the MA(\\(q\\)) is to derive the covariance \\(\\gamma(h) = \\mathrm{cov} (Y_{t}, Y_{t - h} )\\) directly by using the bilinear properties of covariance and determining the interaction between the white noise terms. The advantage of this method is that we do not have to memorize anything. Moreover, since white noise has the property that \\(\\mathrm{cov} (\\epsilon_{u}, \\epsilon_{v}) = 0\\) whenever \\(u \\neq v\\), interactions occur only when the time indices match up, i.e., only when \\(u = v\\).\n\n\n\n\n\n\nNoteExample: ACVF and ACF of MA(1)\n\n\n\nConsider an MA(1) model. Let \\(h \\geqslant 0\\). Applying the covariance properties we obtain the general expression: \\[\n\\begin{split}\n\\gamma(h) & = \\mathrm{cov}\\left( Y_{t}, Y_{t - h} \\right) \\\\\n& = \\mathrm{cov} \\left( \\epsilon_{t} + \\theta_{1} \\epsilon_{t - 1},\\; \\epsilon_{t - h} + \\theta_{1} \\epsilon_{t - h - 1}  \\right) \\\\\n& =  \\mathrm{cov} \\left( \\epsilon_{t},  \\epsilon_{t - h} \\right) + \\theta_{1} \\mathrm{cov} \\left(  \\epsilon_{t},  \\epsilon_{t - h - 1}  \\right) + \\theta_{1} \\mathrm{cov} \\left( \\epsilon_{t - 1}, \\epsilon_{t - h} \\right) + \\theta^{2}_{1} \\mathrm{cov} \\left(\\epsilon_{t - 1}, \\epsilon_{t - h - 1} \\right).\n\\end{split}\n\\]\nNote that the covariances in the last line of the above expression will be nonzero if the time indices match up. By plugging different values of \\(h\\) into the above equation, we obtain \\[\n\\begin{align}\nh  =  0 \\rightarrow \\gamma(0) &= \\mathrm{cov} \\left( \\epsilon_{t}, \\epsilon_{t} \\right) + 0 + 0 + \\theta^{2}_{1} \\mathrm{cov} \\left( \\epsilon_{t - 1}, \\epsilon_{t - 1} \\right) \\\\\n&= \\sigma^{2} + \\theta^{2}_{1} \\sigma^{2} \\\\\n&= \\sigma^{2} \\left( 1 + \\theta^{2}_{1} \\right). \\\\\n\\\\\nh = 1 \\rightarrow \\gamma(1) &= 0 + 0 + \\theta_{1} \\mathrm{cov} \\left( \\epsilon_{t - 1}, \\epsilon_{t - 1} \\right) + 0 \\\\\n&= \\sigma^{2} \\theta_{1}.\\\\\n\\\\\nh \\geqslant 2 \\rightarrow \\gamma(h) &= 0 + 0 + 0 + 0 \\\\\n&= 0.\n\\end{align}\n\\]\nThis method generalizes easily to computing autocovariances for MA(\\(q\\)) models of higher orders.\nRecall that ACF is the ACVF standardized by the variance (which is the ACVF at lag zero), hence the ACF \\[\n\\begin{align}\n\\rho(h) &= \\frac{\\gamma(h)}{\\gamma(0)} \\\\\n&=\n\\begin{cases}\n1 & \\text{if } h = 0 \\\\\n\\frac{\\theta_1}{1 + \\theta^2_1} & \\text{if } h = 1 \\\\\n0 & \\text{if } h &gt; 1\n\\end{cases}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have explicitly determined the structure of the ACF of an MA(\\(q\\)) process. Its most important feature is that \\(\\rho(h) = 0\\) for \\(h &gt; q\\). Remember this about MA processes.\n\n\nFiner features, such as expressing the \\(\\rho(h)\\) in terms of the \\(\\theta_{r}\\) coefficients, can be determined as above.\nPACF of MA processes\nFor an MA(\\(q\\)), we can write \\(Y_t = -\\sum_{j=1}^{\\infty} \\pi_j Y_{t-j} + \\epsilon_t\\). Moreover, no finite representation exists. From this result, it should be apparent that the PACF will never cut off, as in the case of an AR(\\(p\\)). For an MA(1), \\(Y_t = \\epsilon_t +\\theta \\epsilon_{t-1}\\), with \\(|\\theta| &lt; 1\\), it can be shown that (Shumway and Stoffer 2014): \\[\n\\rho_{hh} = - \\frac{(-\\theta)^h (1 - \\theta^2)}{1 - \\theta^{2(h+1)}}, \\; h \\geqslant 1.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nEvery MA(\\(q\\)) process has an infinite autoregressive expansion and its PACF never cuts off. In fact, it can be shown that the PACF of an MA(\\(q\\)) process damps out according to a mixed exponential decay of order \\(q\\).\n\n\nWe do not have to compute the PACF by performing numerous regressions first. The computations are done via a recursive formula.\nSimilar to AR(1) examples above, now consider two sets of plots for MA(1) processes with a positive and negative coefficient. For the MA(1) process \\[\nX_t = \\theta_1 \\epsilon_{t-1} + \\epsilon_t,\n\\tag{4.11}\\] where \\(\\epsilon_t \\sim N(0,\\sigma^2)\\), \\(\\theta_1=0.3\\), and \\(\\sigma = 1\\), see plots in Figure 4.6.\n\nCode# Theoretical ACF, PACF\ntheta &lt;- 0.3\nRHO &lt;- c(1, theta/(1 + theta^2), rep(0, max(Lag) - 1))\nALPHA &lt;- c(1, ARMAacf(ma = theta, lag.max = max(Lag), pacf = TRUE))\n\n# Sample data\nX &lt;- arima.sim(list(order = c(0, 0, 1), ma = theta), n = T)\n\n# Plots\np1 &lt;- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\")\np2 &lt;- ggplot(data.frame(Lag, PACF = ALPHA), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\")\np3 &lt;- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\")\np4 &lt;- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\")\np5 &lt;- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\")\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 4.6: MA(1) as specified in Equation 4.11.\n\n\n\n\nFor the MA(1) process with negative \\(\\theta_1\\), \\[\nX_t = \\theta_1 \\epsilon_{t-1} + \\epsilon_t,\n\\tag{4.12}\\] where \\(\\epsilon_t \\sim N(0,\\sigma^2)\\), \\(\\theta_1=-0.3\\), and \\(\\sigma = 1\\), see plots in Figure 4.7.\n\nCode# Theoretical ACF, PACF\ntheta &lt;- -0.3\nRHO &lt;- c(1, theta/(1 + theta^2), rep(0, max(Lag) - 1))\nALPHA &lt;- c(1, ARMAacf(ma = theta, lag.max = max(Lag), pacf = TRUE))\n\n# Sample data\nX &lt;- arima.sim(list(order = c(0, 0, 1), ma = theta), n = T)\n\n# Plots\np1 &lt;- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\")\np2 &lt;- ggplot(data.frame(Lag, PACF = ALPHA), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\")\np3 &lt;- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\")\np4 &lt;- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\")\np5 &lt;- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\")\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 4.7: MA(1) as specified in Equation 4.12.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autoregressive Moving Average (ARMA) Models</span>"
    ]
  },
  {
    "objectID": "l04_arma.html#arma",
    "href": "l04_arma.html#arma",
    "title": "\n4  Autoregressive Moving Average (ARMA) Models\n",
    "section": "\n4.5 ARMA",
    "text": "4.5 ARMA\nAn ARMA(\\(p, q\\)) is a mixture of \\(p\\) autoregressive components and \\(q\\) moving average components. It can be expressed as \\[\n\\phi^{p} (B)Y_{t} = \\theta^{q} (B) \\epsilon_{t},\n\\tag{4.13}\\] where \\(\\epsilon_t\\) is the zero-mean white noise with variance \\(\\sigma^2\\), \\(\\phi^{p}(\\lambda)\\) and \\(\\theta^{q}(\\lambda)\\) are the polynomials (without common roots) of degree \\(p\\) and \\(q\\), respectively.\nThe ARMA(\\(p, q\\)) Equation 4.13 can be expanded as follows: \\[\nX_t - \\phi_1 X_{t-1} - \\dots - \\phi_p X_{t-p} = \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\dots + \\theta_q \\epsilon_{t-q}.\n\\]\nARMA(1,2)\nShort notation \\(\\phi^{1} (B)Y_{t} = \\theta^{2} (B) \\epsilon_{t}\\) expands to \\[\n\\left(1 - \\phi_{1} B \\right) Y_{t} = \\left( 1 + \\theta_{1} B + \\theta_{2} B^{2} \\right) \\epsilon_{t}\n\\] and gives \\[\nY_{t} - \\phi_{1} Y_{t - 1} = \\epsilon_{t} + \\theta_{1} \\epsilon_{t - 1} + \\theta_{2}\\epsilon_{t - 2}.\n\\]\nIsolate \\(Y_{t}\\) on the left side of the equation to get the final form of the expansion \\[\nY_{t} = \\phi_{1} Y_{t -1} + \\epsilon_{t} + \\theta_{ 1} \\epsilon_{t - 1} + \\theta_{2} \\epsilon_{t-2}.\n\\]\nARMA(3,1)\nShort notation \\(\\phi^{3} (B)Y_{t} = \\theta^{1} (B) \\epsilon_{t}\\) expands to \\[\n\\left( 1 -  \\phi_{1} B - \\phi_{2} B^{2}  - \\phi_{3} B^{3} \\right) Y_{t} = \\left( 1 + \\theta_{1} B \\right) \\epsilon_{t}\n\\] or \\[\nY_{t} - \\phi_{1} Y_{t - 1} - \\phi_{2} Y_{t - 2} - \\phi_{3} Y_{t - 3} = \\epsilon_{t} + \\theta_{1} \\epsilon_{t-1}.\n\\]\nIsolating \\(Y_{t}\\) gives the final form \\[\nY_{t} = \\phi_{1} Y_{t - 1} + \\phi_{2} Y_{t - 2} + \\phi_{3} Y_{t - 3} + \\epsilon_{t} + \\theta_{1} \\epsilon_{t-1}.\n\\]\nIn a mixed ARMA(\\(p, q\\)) process (that is, \\(p &gt; 0\\), \\(q &gt; 0\\)), neither the ACF nor the PACF cuts off abruptly. Both the ACF and PACF exhibit mixed exponential decay. This occurs because the AR component introduces mixed exponential decay into the ACF, while the MA component introduces mixed exponential decay into the PACF.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autoregressive Moving Average (ARMA) Models</span>"
    ]
  },
  {
    "objectID": "l04_arma.html#summary-of-armaarma-models",
    "href": "l04_arma.html#summary-of-armaarma-models",
    "title": "\n4  Autoregressive Moving Average (ARMA) Models\n",
    "section": "\n4.6 Summary of AR/MA/ARMA models",
    "text": "4.6 Summary of AR/MA/ARMA models\nUse the following summary or Table 4.1 to decide on the appropriate model specifications.\nAR(\\(p\\)) models: \\(\\phi^{p} (B)Y_{t}  = \\epsilon_{t}\\)\n\nACF: \\(p\\) initial spikes, then damps out as a mixed exponential decay of order \\(p\\) (never 0)\nPACF: \\(p\\) initial spikes, then cuts off; PACF(\\(h\\)) = 0 for \\(h &gt; p\\)\n\n\nMA(\\(q\\)) models: \\(Y_{t} = \\theta^{q} (B) \\epsilon_{t}\\)\n\nACF: \\(q\\) initial spikes, then cuts off; ACF(\\(h\\)) = 0 for \\(h &gt; q\\)\n\nPACF: \\(q\\) initial spikes, then damps out as a mixed exponential decay of order \\(q\\) (never 0)\n\nARMA(\\(p,q\\)) models (\\(p&gt;0\\), \\(q&gt;0\\)): \\(\\phi^{p} (B)Y_{t} = \\theta^{q} (B) \\epsilon_{t}\\)\n\nACF: has max (\\(p, q\\)) initial spikes, then damps out as a mixed exponential decay driven by the AR(\\(p\\)) component (never 0)\nPACF: has max(\\(p, q\\)) initial spikes, then damps out as a mixed exponential decay driven by the MA(\\(q\\)) component (never 0)\n\n\n\nTable 4.1: Summary of ARMA(\\(p, q\\)) models\n\n\n\nModel\nACF\nPACF\n\n\n\nMA(\\(q\\))\nZero after lag \\(q\\)\n\nExponential decay\n\n\nAR(\\(p\\))\nExponential decay\nZero after lag \\(p\\)\n\n\n\nARMA(\\(p\\),\\(q\\))\nMixed exponential decay\nMixed exponential decay\n\n\n\n\n\n\nIn practice, the final selection of ARMA’s orders \\(p\\) and \\(q\\) (i.e., model specification) is often based on\n\ninformation criterion such as AIC or BIC or\nforecast accuracy (e.g., PRMSE and PMAE of point forecasts for a testing set)\n\nUse the examples in Section E.2 to practice the implementation of the rules in Table 4.1.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autoregressive Moving Average (ARMA) Models</span>"
    ]
  },
  {
    "objectID": "l04_arma.html#example-analysis",
    "href": "l04_arma.html#example-analysis",
    "title": "\n4  Autoregressive Moving Average (ARMA) Models\n",
    "section": "\n4.7 Example analysis",
    "text": "4.7 Example analysis\nConsider a univariate time series \\(Y_t\\) of 1918–2017 annual landings of golden tilefish in the U.S. North Atlantic region (Lyubchich and Nesslage 2020; Nesslage et al. 2021).\n\nD &lt;- read.csv(\"data/tilefish.csv\")\nY &lt;- ts(D$Landings, start = min(D$Year), frequency = 1)\n\nFirst, plot the data (Figure 4.8).\n\nCodeggplot2::autoplot(Y) + \n    xlab(\"Year\") +\n    ylab(\"Landings (tonne)\")\n\n\n\n\n\n\nFigure 4.8: Annual golden tilefish landings.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are some missing values in the time series, which present a problem for model fitting. Generally, ARMA models assume the input time series is ordered chronologically, with the same time differences between all consecutive observations (equally-spaced time series); these models cannot handle missing values. Omitting the missing values to bridge the gaps, such as using the function na.omit(), is not recommended since it breaks the time sequence (the time series becomes unequally-spaced) and may bias estimates of the time dependencies. Some functions may work with missing values, but their ways of handling the NAs may differ. For example, acf() with the argument na.action = na.pass computes the ACF using all available pairs of observations, and arima() generally allows missing values. However, one may prefer to impute missing values, with commonly used methods like linear interpolation of neighboring observations or an AR model.\n\n\nSince we are just learning about AR models, consider filling in the missing values using linear interpolation (Figure 4.9).\n\nY &lt;- forecast::na.interp(Y)\n\n\nCodeggplot2::autoplot(Y) + \n    geom_point(pch = 21, fill = ifelse(is.na(D$Landings), \"red\", \"white\")) +\n    xlab(\"Year\") +\n    ylab(\"Landings (tonne)\")\n\n\n\n\n\n\nFigure 4.9: Annual golden tilefish landings, with the imputed values shown in red.\n\n\n\n\nIf no obvious trend, plot sample ACF and PACF (Figure 4.10).\n\nCodep1 &lt;- forecast::ggAcf(Y) +\n    ggtitle(\"\")\np2 &lt;- forecast::ggAcf(Y, type = \"partial\") +\n    ggtitle(\"\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 4.10: Estimated ACF and PACF of the annual golden tilefish landings \\(Y_t\\).\n\n\n\n\nThe ACF decays faster than linearly (some indication of stationarity), even though the autocorrelations are statistically significant up to the lag of 6 years (Figure 4.10 A). The PACF is non-zero up to lag 2 (cuts off after lag 2; Figure 4.10 B). There is another significant spike at lag 7, but it is likely due to the sample variability and can be ignored. Knowing the expected behavior of ACF and PACF for ARMA models (see Table 4.1), AR(2) can be suggested in this case, which is equivalent to ARMA(2,0).\nGiven the potential sample variability affecting the estimates of ACF and PACF, there could be other model specifications, close to the selected one, that may perform similarly or even slightly better when compared to the one selected based on the visual analysis. For example, we can check other suitable combinations: ARMA(1,0) and ARMA(1,1).\nWe need some quantitative criteria to select the best model. The code below uses a ‘manual’ approach of calculating Akaike and Bayesian information criteria (AIC and BIC) to select the model from the specified set, but the next section lists R functions that do it more automatically.\n\n# Combinations of p and q\nresults &lt;- data.frame(p = c(2, 1, 1), \n                      q = c(0, 0, 1))\n\n# For each combination, fit the model and extract AIC and BIC values\nfor (i in 1:nrow(results)) {\n    mod &lt;- arima(Y, order = c(results$p[i], 0, results$q[i]))\n    results$aic[i] &lt;- mod$aic\n    results$bic[i] &lt;- BIC(mod)\n}\nresults\n\n#&gt;   p q  aic  bic\n#&gt; 1 2 0 1509 1520\n#&gt; 2 1 0 1512 1519\n#&gt; 3 1 1 1509 1519\n\n\n\n4.7.1 Automatic model selection\nThe function stats::ar() by default uses the Yule–Walker method to estimate the coefficients and AIC to select the best order for AR(\\(p\\)) model, where the maximum of \\(p\\) can be set by the user.\n\nar(Y, order.max = 11)\n\n#&gt; \n#&gt; Call:\n#&gt; ar(x = Y, order.max = 11)\n#&gt; \n#&gt; Coefficients:\n#&gt;      1       2  \n#&gt;  1.006  -0.209  \n#&gt; \n#&gt; Order selected 2  sigma^2 estimated as  201769\n\n\nThe function funtimes::ARest() is a wrapper for stats::ar(). It adds a difference-based estimator by Hall and Van Keilegom (2003), which is now used by default, and also switches between BIC or AIC as the model selection criterion. The selection is still done only for the order \\(p\\) in AR(\\(p\\)), without considering the possibility of MA terms.\n\nfuntimes::ARest(Y)\n\n#&gt; [1] 0.796\n\n\nTo check the correspondence with stats::ar(), change the method and fix the ar.order (turn off its automatic selection):\n\nfuntimes::ARest(Y, ar.method = \"yw\", ar.order = 2, ic = \"none\")\n\n#&gt; [1]  1.006 -0.209\n\n\n\n\n\n\n\n\nNote\n\n\n\nBIC selects more parsimonious models than AIC does. Such simpler models are usually preferred for forecasting tasks.\n\n\nThe function fable::ARIMA() searches over possible ARMA models within some order constraints (the default maximal orders and search strategy are different from the two functions above – see help files for each function in R). This function is a successor of the function forecast::auto.arima() and is probably the most comprehensive R function for estimating and selecting ARMA models. It allows to fit and select not only ARMA(\\(p,q\\)) models, but also more complex models that will be described later in the course.\n\nlibrary(fable)\nm &lt;- as_tsibble(Y) %&gt;% \n    model(ARIMA(Y ~ 1, ic = \"bic\"))\nreport(m)\n\n#&gt; Series: Y \n#&gt; Model: ARIMA(1,0,0) w/ mean \n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1  constant\n#&gt;       0.8321     150.9\n#&gt; s.e.  0.0537      42.7\n#&gt; \n#&gt; sigma^2 estimated as 203947:  log likelihood=-753\n#&gt; AIC=1512   AICc=1512   BIC=1519\n\n\nThe AIC in ar() selects AR(2), while BIC in funtimes::ARest() and fable::ARIMA() selects AR(1). Note, the ar() estimates suggest a model that might be close to being nonstationary, since the absolute value of the estimated coefficient is close to 1. Formally check the stationarity of the AR(2) model below.\n\n# Calculate the roots of characteristic polynomial\nroots &lt;- polyroot(c(1, -ar(Y, order.max = 11)$ar))\n\n# The AR model is causal (future-independent) \n# and stationary if |roots| &gt; 1\nall(Mod(roots) &gt; 1)\n\n#&gt; [1] TRUE\n\n\nFor now, continue with the model ARMA(1,0), which is equivalent to AR(1). Our next step is the residual diagnostics – check the residuals for homoskedasticity, uncorrelatedness, and normality (Figure 4.11).\n\nCodeggtime::gg_tsresiduals(m)\n\n\n\n\n\n\nFigure 4.11: Residual diagnostics for the ARMA(1,0) model estimated using the package fable.\n\n\n\n\nIf the residuals do not violate the assumptions, proceed with forecasting (Figure 4.12).\n\nCodem %&gt;%\n    fabletools::forecast(h = 10) %&gt;%\n    autoplot(Y)\n\n\n\n\n\n\nFigure 4.12: Predictions 10 steps ahead from the ARMA(1,0) model estimated using the package fable.\n\n\n\n\nHowever, the syntax and outputs of the package fable are quite different from the typical R alternatives. See Figure 4.13 with results still plotted using the package ggplot2, but obtained using the base-R functions. Also, see the base-R function tsdiag() for residual diagnostics.\n\nCodem &lt;- arima(Y, order = c(1, 0, 0))\ne &lt;- m$residuals\np1 &lt;- ggplot2::autoplot(e) + \n    ylab(\"Residuals\")\np2 &lt;- forecast::ggAcf(e) +\n    ggtitle(\"\")\np3 &lt;- ggpubr::ggqqplot(e) + \n    xlab(\"Standard normal quantiles\")\np4 &lt;- forecast::forecast(m, h = 10) %&gt;% \n    ggplot2::autoplot()\np1 / (p2 + p3) / p4 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 4.13: Residual diagnostics and predictions 10 steps ahead from the ARMA(1,0) model estimated using the base-R function stats::arima().\n\n\n\n\nUsing the estimated residuals, \\(\\hat{\\epsilon}_t = Y_t - \\hat{Y}_t\\), we can get the ‘fitted’ values, \\(\\hat{Y}_t\\) (technically, these are one-step-ahead predictions), see Figure 4.14.\n\nCodeFit &lt;- Y - e\nggplot2::autoplot(Y) + \n    autolayer(Fit, color = \"blue\") +\n    ylab(\"Landings (tonne)\")\n\n\n\n\n\n\nFigure 4.14: Oberved time series (black) and one-step-ahead predictions (red) from the ARMA(1,0) model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autoregressive Moving Average (ARMA) Models</span>"
    ]
  },
  {
    "objectID": "l04_arma.html#conclusion",
    "href": "l04_arma.html#conclusion",
    "title": "\n4  Autoregressive Moving Average (ARMA) Models\n",
    "section": "\n4.8 Conclusion",
    "text": "4.8 Conclusion\nWe defined AR, MA, and ARMA models and showed that time series processes corresponding to such models satisfy the conditions of weak stationarity.\nUsing ACF and PACF plots, we can select the orders \\(p\\) and \\(q\\) to specify an ARMA model, but we also can use a variety of functions running different evaluations and tests for competitive models.\nWe face the choice of different methods to estimate (i.e., estimators) and methods to compare models.\nThe most often used estimators for ARMA models are\n\nmaximum likelihood,\nYule–Walker, and\nBurg.\n\nThe OLS-based estimator is used less frequently.\nMethods to compare competing models may involve\n\nlog-likelihood, MAE, RMSE, and information criteria to assess in-sample (training) goodness-of-fit;\ndiagnostics checks of residuals (discard models if their residuals do not pass the tests);\ncomparisons of models on a testing set (PMAE, PRMSE, coverage, and width of prediction intervals).\n\nFor each of these options, there are multiple R functions available, or can be written from scratch.\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nHall P, Van Keilegom I (2003) Using difference-based methods for inference in nonparametric regression with time series errors. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 65:443–456. https://doi.org/10.1111/1467-9868.00395\n\n\nLyubchich V, Nesslage G (2020) Environmental drivers of golden tilefish fisheries v1.0. Version v1.0. Zenodo\n\n\nNesslage G, Lyubchich V, Nitschke P, et al (2021) Environmental drivers of golden tilefish (Lopholatilus chamaeleonticeps) commercial landings and catch-per-unit-effort. Fisheries Oceanography 30:608–622. https://doi.org/10.1111/fog.12540\n\n\nShumway RH, Stoffer DS (2014) Time series analysis and its applications with R examples, 3-EZ. Free Texts in Statistics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autoregressive Moving Average (ARMA) Models</span>"
    ]
  },
  {
    "objectID": "l05_arima.html",
    "href": "l05_arima.html",
    "title": "\n5  Autoregressive Integrated Moving Average (ARIMA) Models\n",
    "section": "",
    "text": "5.1 Introduction to ARIMA\nThe goal of this lecture is to implement Box–Jenkins methodology for nonstationary time series, by applying differences and fitting an ARMA model to the stationary differenced series. You should be able to recognize when the differencing is needed based on the time series and ACF plots.\nObjectives\nReading materials\nAudio overview\nWe have already discussed the class of ARMA models for representing stationary series. A generalization of this class, which incorporates a wide range of nonstationary series, is provided by the autoregressive integrated moving average (ARIMA) processes, i.e., processes which, after differencing finitely many times, reduce to ARMA processes.\nIf \\(d\\) is a nonnegative integer, then \\(X_t\\) is an ARIMA(\\(p, d, q\\)) process if \\(Y_t=(1 - B)^d X_t\\) is a causal ARMA(\\(p, q\\)) process. The process is stationary if \\(d = 0\\), in which case it reduces to an ARMA(\\(p, q\\)) process.\nFor example \\(X_t\\) is an ARIMA(1,1,0) process, then \\(Y_t\\) representing the series of its first-order differences (because \\(d = 1\\)) is an ARMA(1,0) process \\[\nY_t = (1 - B)X_t = \\phi_1 Y_{t-1} + Z_t,\n\\] where \\(|\\phi_1| &lt; 1\\) and \\(Z_t\\) is white noise.\nAn equation for ARIMA(\\(p, d, q\\)) is \\[\n(1 - B)^d (1 - \\phi_1B - \\dots - \\phi_pB^p)X_t =\n(1 + \\theta_1B + \\dots + \\theta_qB^q)Z_t,\n\\tag{5.1}\\] where \\(B\\) is the backshift operator, \\(d\\) is the order of differences, \\(\\phi_1, \\dots, \\phi_p\\) are autoregression coefficients, \\(p\\) is the autoregressive order, \\(\\theta_1, \\dots, \\theta_q\\) are moving average coefficients, \\(q\\) is the moving average order, and \\(Z_t\\sim \\mathrm{WN}(0,\\sigma^2)\\). The left part of Equation 5.1 consists of the differences and the AR part; the right part represents the MA part.\nWhy the process is called ‘integrated’?\nRecall the geometric interpretation of the integral of a curve \\(y = f(x)\\) defined on continuous \\(x\\). The integral of \\(y\\) corresponds to the area under the curve. For example, if \\(y = f(x)\\) is a function describing income \\(y\\) based on working time \\(x\\), the integral corresponds to the total income in a certain period.\nIn our lectures, we deal with time series defined using discrete times \\(t\\) (for example, years), so yearly income is \\(Y_t\\), and the total income over several years can be obtained by summing the individual annual incomes, \\(\\sum Y_t\\). Hence, here we integrate by summation.\nRecall the definition of random walk series, which is a cumulative sum of i.i.d. noise: \\[\nX_t = \\sum_{i=1}^t Y_i,\n\\] where \\(Y_t \\sim\\) i.i.d.(\\(0,\\sigma^2\\)). This \\(X_t\\) is the simplest example of integrated series. The notation \\(X_t \\sim\\) I(1) means \\(X_t\\) is a first-order integrated series. The differences of \\(X_t\\) \\[\n\\begin{align}\n(1 - B)X_t &= X_t - BX_t \\\\\n&= X_t - X_{t-1} \\\\\n&= Y_t\n\\end{align}\n\\] give us back the uncorrelated series \\(Y_t\\), hence the process \\(X_t\\) is an ARIMA(0,1,0) process.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Autoregressive Integrated Moving Average (ARIMA) Models</span>"
    ]
  },
  {
    "objectID": "l05_arima.html#introduction-to-arima",
    "href": "l05_arima.html#introduction-to-arima",
    "title": "\n5  Autoregressive Integrated Moving Average (ARIMA) Models\n",
    "section": "",
    "text": "Note\n\n\n\nModifications of Equation 5.1 exists, such as those having both the AR and MA parts on the right side. This affects the signs of estimated coefficients \\(\\phi_1, \\dots, \\phi_p\\) and possibly \\(\\theta_1, \\dots, \\theta_q\\). Check the help files of the software to know the exact form of the model it is estimating.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nARIMA(\\(p, d, q\\)) processes with \\(d \\geqslant 1\\) are also called difference-stationary processes or processes with a stochastic trend. I.e., difference-stationary means the process is not stationary but can be made stationary by proper differencing.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Autoregressive Integrated Moving Average (ARIMA) Models</span>"
    ]
  },
  {
    "objectID": "l05_arima.html#boxjenkins-methodology",
    "href": "l05_arima.html#boxjenkins-methodology",
    "title": "\n5  Autoregressive Integrated Moving Average (ARIMA) Models\n",
    "section": "\n5.2 Box–Jenkins methodology",
    "text": "5.2 Box–Jenkins methodology\nThe approach developed by Box and Jenkins (1976) applies the differencing to the original time series repeatedly, until a stationary time series is obtained. We will first learn how to identify the number of differences (i.e., the order of differences \\(d\\)) by analyzing plots of the time series and ACF at each iteration of the method. In the later lecture on detecting trends in time series, we will also introduce formal tests for the integration order.\nBelow is a general algorithm used to fit ARIMA(\\(p, d, q\\)) models\n\nStart assuming \\(d = 0\\).\nPlot the time series and ACF.\nIf the plots suggest nonstationarity, iterate differencing and plotting to update \\(d\\):\n\nApply differences of the current time series, write \\(d = d + 1\\)\n\nPlot the time series and ACF\nIf nonstationarity is still obvious, repeat the differencing and plotting\n\n\nIdentify the orders \\(p\\) and \\(q\\) from the plots of ACF and PACF of the latest (differenced) time series.\nEstimate the model.\nApply model diagnostics, particularly for homogeneity, uncorrelatedness, and normality of residuals. Address the violations by respecifying the model.\nForecast with the resultant model.\n\nUsing the Box–Jenkins methodology, the linear predictor approximately follows a normal distribution, i.e., \\[\n\\hat{X}_{n+h} \\sim N\\left( X_{n+h}, \\mathrm{var}(\\hat{X}_{n+h}) \\right).\n\\]\nTherefore, a \\((100 - \\alpha)\\)% prediction interval is \\[\n\\hat{X}_{n+h} \\pm z_{1-\\alpha/2} \\sqrt{\\mathrm{var}(\\hat{X}_{n+h})}.\n\\]\n\n\n\n\n\n\nNoteExample: ARIMA for Lake Baikal\n\n\n\nHere we find an ARIMA model for the Lake Baikal thaw (breakup) dates from the Global Lake and River Ice Phenology Database (Benson et al. 2020).\n\nCode# Calculate calendar day from ice break-up date\nB &lt;- read.csv(\"data/baikal.csv\", skip = 1) %&gt;%\n    mutate(Date_iceoff = as.Date(paste(iceoff_year, iceoff_month, iceoff_day,\n                                       sep = \"-\"))) %&gt;%\n    mutate(DoY_iceoff = as.numeric(format(Date_iceoff, \"%j\")))\n\n# Convert to ts format\niceoff &lt;- ts(B$DoY_iceoff, start = B$iceoff_year[1])\n\n\nFigure 5.1 shows that there is possibly a decreasing trend (ice melts earlier in the year), although the ACF declines fast.\n\nCodeX &lt;- iceoff\np1 &lt;- forecast::autoplot(X) +\n    xlab(\"Year\") +\n    ylab(\"Ice breakup day\")\np2 &lt;- forecast::ggAcf(X) +\n    ggtitle(\"\")\np3 &lt;- forecast::ggAcf(X, type = \"partial\") +\n    ggtitle(\"\")\np1 / (p2 + p3) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 5.1: Plots for the original time series of ice breakup days.\n\n\n\n\nTo remove this potential trend, we apply differencing once (more specifically, consecutive differencing, \\(X_t - X_{t-1}\\)) and produce the plots again. From Figure 5.2, there is no tendency in the differenced series, and the ACF declines fast, so we achieved stationarity and no need to difference the data more. Overall, we differenced the time series once to achieve stationarity, so the order of differences \\(d = 1\\).\n\nCodeX &lt;- diff(iceoff)\np1 &lt;- ggplot2::autoplot(X) +\n    xlab(\"Year\") +\n    ylab(\"diff(Ice breakup day)\")\np2 &lt;- forecast::ggAcf(X) +\n    ggtitle(\"\")\np3 &lt;- forecast::ggAcf(X, type = \"partial\") +\n    ggtitle(\"\")\np1 / (p2 + p3) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 5.2: Plots for the differenced time series of ice breakup days.\n\n\n\n\nContinue working with Figure 5.2 to identify the orders \\(p\\) and \\(q\\). If we treat the behavior of ACF as exhibiting a cut-off, and PACF having an exponential decay, an MA(\\(q\\)) model might be plausible (i.e., \\(p = 0\\)). Since the ACF cuts off after lag 1, \\(q = 1\\) in this case.\nHence, we specified the model for ice breakup dates to be ARIMA(0,1,1), which can be written as \\[\n(1 - B) Y_t = (1 + \\theta_1B)Z_t\n\\] or \\[\nY_t = Y_{t-1} + \\theta_1 Z_{t-1} + Z_t,\n\\] where \\(Y_t\\) represents the ice breakup dates in the year \\(t\\), \\(\\theta_1\\) is the moving average coefficient, and \\(Z_t\\sim \\mathrm{WN}(0,\\sigma^2)\\).\nWe can now estimate the model, for example, using stats::arima().\n\nmod_baikal &lt;- stats::arima(iceoff, order = c(0, 1, 1))\nmod_baikal\n\n#&gt; \n#&gt; Call:\n#&gt; stats::arima(x = iceoff, order = c(0, 1, 1))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ma1\n#&gt;       -0.843\n#&gt; s.e.   0.075\n#&gt; \n#&gt; sigma^2 estimated as 59:  log likelihood = -519,  aic = 1042\n\n\nWe can also check the orders selected automatically:\n\nforecast::auto.arima(iceoff)\n\n#&gt; Series: iceoff \n#&gt; ARIMA(0,1,1) \n#&gt; \n#&gt; Coefficients:\n#&gt;          ma1\n#&gt;       -0.843\n#&gt; s.e.   0.075\n#&gt; \n#&gt; sigma^2 = 59.4:  log likelihood = -519\n#&gt; AIC=1042   AICc=1043   BIC=1048\n\n\nIn the next step, we apply diagnostic checks for the residuals, for example, using plots (Figure 5.3). Remember that the residuals should resemble white noise.\n\nCodee &lt;- mod_baikal$residuals\np1 &lt;- ggplot2::autoplot(e) + \n    ylab(\"Residuals\")\np2 &lt;- forecast::ggAcf(e) +\n    ggtitle(\"\")\np3 &lt;- ggpubr::ggqqplot(e) + \n    xlab(\"Standard normal quantiles\")\np1 / (p2 + p3) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 5.3: Residual diagnostics for the ARIMA(0,1,1) model for ice breakup dates.\n\n\n\n\nGiven that the diagnostics plots show satisfactory behavior of the residuals (Figure 5.3), continue with forecasting using this model (Figure 5.4). Note that ARIMA(0,1,1) is mathematically equivalent to simple exponential smoothing, hence the forecast is a horizontal line.\n\nCodeggplot2::autoplot(forecast::forecast(mod_baikal, h = 10)) + \n    xlab(\"Year\") +\n    ylab(\"Ice breakup day\") +\n    ggtitle(\"\")\n\n\n\n\n\n\nFigure 5.4: ARIMA(0,1,1) model predictions of ice breakup dates 10 years ahead.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Autoregressive Integrated Moving Average (ARIMA) Models</span>"
    ]
  },
  {
    "objectID": "l05_arima.html#seasonal-arima-sarima",
    "href": "l05_arima.html#seasonal-arima-sarima",
    "title": "\n5  Autoregressive Integrated Moving Average (ARIMA) Models\n",
    "section": "\n5.3 Seasonal ARIMA (SARIMA)",
    "text": "5.3 Seasonal ARIMA (SARIMA)\nRecall that time series with seasonal variability or another strictly periodic component (e.g., daily cycles) can be deseasonalized by applying differencing that is not of consecutive values but with a lag equal to the period of the cyclical variability. We will use such differences to remove strong periodicity, as an additional step in the Box–Jenkins algorithm.\nSimilar to the regular differences, we will apply seasonal differences not to eliminate autocorrelations at the corresponding lags, but to achieve fast decay of ACF at seasonal lags. Even after the seasonal differences, there might be significant spikes at the seasonal lags in ACF and PACF, which can be addressed by selecting proper seasonal autoregressive and moving average orders. Therefore, we can define orders of integration, AR, and MA for the seasonal part of the time series in the same way we define these orders for the non-seasonal part.\nBased on the definitions in Brockwell and Davis (2002), \\(X_t\\) is a seasonal autoregressive integrated moving average, SARIMA(\\(p,d,q\\))(\\(P,D,Q\\))\\(_s\\) process if the differenced series \\(Y_t=(1 - B)^d (1 - B^s)^D X_t\\) is a causal ARMA process. Here \\(d\\) and \\(D\\) are nonnegative integers, and \\(s\\) is the period.\n\n\n\n\n\n\nNote\n\n\n\nIn practice, \\(D \\leqslant 1\\) and \\(P, Q \\leqslant 3\\).\n\n\nAn equation for SARIMA(\\(p, d, q\\))(\\(P, D, Q\\))\\(_s\\) is \\[\n\\begin{split}\n(1 - B)^d (1 - \\phi_1B - \\dots - \\phi_pB^p) (1 - B^s)^D (1 - \\Phi_1B^s - \\dots - \\Phi_PB^{sP}) X_t \\\\\n= (1 + \\theta_1B + \\dots + \\theta_qB^q) (1 + \\Theta_1B^s + \\dots + \\Theta_QB^{sQ})Z_t,\n\\end{split}\n\\tag{5.2}\\] where \\(D\\) is the order of seasonal differences, \\(\\Phi_1, \\dots, \\Phi_P\\) are the seasonal autoregression coefficients, \\(P\\) is the seasonal autoregressive order, \\(\\Theta_1, \\dots, \\Theta_q\\) are seasonal moving average coefficients, \\(Q\\) is the seasonal moving average order, and the rest of the terms are the same as in Equation 5.1.\n\n\n\n\n\n\nNoteExample: SARIMA for the number of airline passengers\n\n\n\nHere we revisit the time series on airline passengers from which we have removed the trend by taking regular non-seasonal differences once (\\(d = 1\\)). Notice from Figure 5.5 C and D how after taking the usual differences the upward trend disappeared, and ACF started to decay much faster. At the seasonal lags, however, the ACF decays still linearly (Figure 5.5 D), which suggested differencing at the seasonal lag to remove strong periodicity. After taking seasonal differences once (\\(D = 1\\)), the time series looks stationary (Figure 5.5 E), and the ACF decays fast at both seasonal and non-seasonal lags. This is enough differencing.\n\nCodeYt &lt;- log10(AirPassengers)\n\n# Apply first-order (non-seasonal) differences\nD1 &lt;- diff(Yt)\n\n# Additionally, apply first-order seasonal differences\nD1D12 &lt;- diff(D1, lag = 12)\n\np1 &lt;- ggplot2::autoplot(Yt) + \n    xlab(\"Year\") + \n    ylab(\"lg(Air passangers)\") + \n    ggtitle(\"Yt\")\np2 &lt;- forecast::ggAcf(Yt) + \n    ggtitle(\"Yt\") +\n    xlab(\"Lag (months)\")\np3 &lt;- ggplot2::autoplot(D1) + \n    xlab(\"Year\") + \n    ylab(\"lg(Air passangers)\") + \n    ggtitle(\"(1-B)Yt\")\np4 &lt;- forecast::ggAcf(D1) + \n    ggtitle(\"(1-B)Yt\") +\n    xlab(\"Lag (months)\")\np5 &lt;- ggplot2::autoplot(D1D12) + \n    xlab(\"Year\") + \n    ylab(\"lg(Air passangers)\") + \n    ggtitle(\"(1-B)(1-B12)Yt\")\np6 &lt;- forecast::ggAcf(D1D12) + \n    ggtitle(\"(1-B)(1-B12)Yt\") +\n    xlab(\"Lag (months)\")\n(p1 + p2) / (p3 + p4) / (p5 + p6) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 5.5: Time series plot of the airline passenger series with an estimated ACF and the detrended (differenced) series with their ACFs.\n\n\n\n\nFor the next step, see Figure 5.6 to identify the orders \\(p\\) and \\(q\\). For that, look only at the non-seasonal lags, 1–11. Both ACF and PACF have significant values at lags 1 and 3, which could correspond to AR(3), MA(3), ARMA(1,1), or ARMA with higher orders. From these options ARMA(1,1) has fewer parameters, hence we prefer this model, with \\(p = 1\\) and \\(q = 1\\), as the most parsimonious option. (However, given that the correlations at lag 2 are not statistically significant, information criteria may penalize adding extra arguments and might prefer a more compact specification, AR(1) or MA(1).)\nNext, use Figure 5.6 again to identify orders \\(P\\) and \\(Q\\). Now look only at the seasonal lags 12, 24, 36, etc. Both the ACF and PACF have significant values only on the first of those lags (lag 12), which could correspond to AR(1), MA(1), or ARMA(1,1) for the seasonal component. From these options, AR(1) and MA(1) are the most parsimonious so we should select one of them or use some numeric criterion to select the best model (e.g., information criterion like AIC or forecasting accuracy on a testing set).\n\nCodep6 &lt;- forecast::ggAcf(D1D12, lag.max = 36) + \n    ggtitle(\"(1-B)(1-B12)Yt\") +\n    xlab(\"Lag (months)\")\np7 &lt;- forecast::ggAcf(D1D12, lag.max = 36, type = \"partial\") + \n    ggtitle(\"(1-B)(1-B12)Yt\") +\n    xlab(\"Lag (months)\")\np6 + p7 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 5.6: ACF and PACF of the differenced airline passenger time series \\((1-B)(1-B^{12})Y_t\\).\n\n\n\n\nOverall, our analysis suggested that SARIMA(1,1,1)(1,1,0) or SARIMA(1,1,1)(0,1,1) is a plausible model for this time series. For example, estimate SARIMA(1,1,1)(0,1,1):\n\nmod_air &lt;- stats::arima(Yt, order = c(1, 1, 1),\n                        seasonal = list(order = c(0, 1, 1),\n                                        period = 12))\nmod_air\n\n#&gt; \n#&gt; Call:\n#&gt; stats::arima(x = Yt, order = c(1, 1, 1), seasonal = list(order = c(0, 1, 1), \n#&gt;     period = 12))\n#&gt; \n#&gt; Coefficients:\n#&gt;         ar1     ma1    sma1\n#&gt;       0.196  -0.578  -0.564\n#&gt; s.e.  0.247   0.213   0.075\n#&gt; \n#&gt; sigma^2 estimated as 0.000253:  log likelihood = 354,  aic = -700\n\n\nSARIMA(1,1,1)(0,1,1)\\(_{12}\\) model can be written down as \\[\n\\begin{split}\n(1 - B) (1 - \\phi_1B) (1 - B^{12}) X_t \\\\\n= (1 + \\theta_1B) (1 + \\Theta_1B^{12})Z_t.\n\\end{split}\n\\] We can apply the backshift operations are rewrite the model without the backshift operator as follows: \\[\n\\begin{split}\n(1 - B - \\phi_1B + \\phi_1B^2) (1 - B^{12}) X_t \\\\\n= (1 + \\theta_1B + \\Theta_1B^{12} + \\theta_1\\Theta_1B^{13})Z_t,\n\\end{split}\n\\] then \\[\n\\begin{split}\n(1 - B - \\phi_1B + \\phi_1B^2 - B^{12} + B^{13} + \\phi_1B^{13} - \\phi_1B^{14}) X_t \\\\\n= (1 + \\theta_1B + \\Theta_1B^{12} + \\theta_1\\Theta_1B^{13})Z_t,\n\\end{split}\n\\] then \\[\n\\begin{split}\nX_t - X_{t-1} - \\phi_1X_{t-1} + \\phi_1X_{t-2} - X_{t-12} + X_{t-13} + \\phi_1X_{t-13} - \\phi_1X_{t-14} \\\\\n= Z_t + \\theta_1Z_{t-1} + \\Theta_1Z_{t-12} + \\theta_1\\Theta_1Z_{t-13},\n\\end{split}\n\\] then \\[\n\\begin{split}\nX_t = X_{t-1} + \\phi_1X_{t-1} - \\phi_1X_{t-2} + X_{t-12} - X_{t-13} - \\phi_1X_{t-13} + \\phi_1X_{t-14} \\\\\n+ \\theta_1Z_{t-1} + \\Theta_1Z_{t-12} + \\theta_1\\Theta_1Z_{t-13} + Z_t,\n\\end{split}\n\\] where \\(X_t = \\lg(AirPassengers)\\) and \\(Z_t\\sim \\mathrm{WN}(0,\\sigma^2)\\).\nBelow are results based on the automatic selection of the orders:\n\nforecast::auto.arima(Yt)\n\n#&gt; Series: Yt \n#&gt; ARIMA(0,1,1)(0,1,1)[12] \n#&gt; \n#&gt; Coefficients:\n#&gt;          ma1    sma1\n#&gt;       -0.402  -0.557\n#&gt; s.e.   0.090   0.073\n#&gt; \n#&gt; sigma^2 = 0.000259:  log likelihood = 354\n#&gt; AIC=-702   AICc=-702   BIC=-693\n\n\nThe orders selected automatically based on AIC suggest that indeed the non-significance of correlations at lag 2 and relatively low correlations at lag 3 made it not worthy to estimate additional parameters in the non-seasonal part, for which MA(1) specification was selected, not the suggested ARMA(1,1), AR(3), or MA(3).\nIn the next step, we apply diagnostic checks for the residuals, for example, using plots (Figure 5.7).\n\nCodee &lt;- mod_air$residuals\np1 &lt;- ggplot2::autoplot(e) + \n    ylab(\"Residuals\")\np2 &lt;- forecast::ggAcf(e) +\n    ggtitle(\"\")\np3 &lt;- ggpubr::ggqqplot(e) + \n    xlab(\"Standard normal quantiles\")\np1 / (p2 + p3) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 5.7: Residual diagnostics for the SARIMA model for the airline passenger data.\n\n\n\n\nGiven that the diagnostics plots show satisfactory behavior of the residuals (Figure 5.7), continue with forecasting using this model (Figure 5.8).\n\nCodeggplot2::autoplot(forecast::forecast(mod_air, h = 24)) + \n    xlab(\"Year\") +\n    ylab(\"lg(Air passangers)\") +\n    ggtitle(\"\")\n\n\n\n\n\n\nFigure 5.8: SARIMA model predictions of airline passenger data 2 years ahead.\n\n\n\n\nCompare ACF in Figure 5.7 with ACF of regression residuals in Figure 3.15 and Figure 3.16. The residuals of those regression models are autocorrelated, while the models have more parameters than the specified SARIMA models. Hence, the SARIMA model is much better for this time series.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Autoregressive Integrated Moving Average (ARIMA) Models</span>"
    ]
  },
  {
    "objectID": "l05_arima.html#conclusion",
    "href": "l05_arima.html#conclusion",
    "title": "\n5  Autoregressive Integrated Moving Average (ARIMA) Models\n",
    "section": "\n5.4 Conclusion",
    "text": "5.4 Conclusion\nIn this lecture, we discovered ARIMA as an extension of ARMA modeling to nonstationary data and SARIMA as an extension of ARIMA models to time series with periodicity (seasonality).\nWe learned Box–Jenkins iterative procedure for identifying such models, including the orders of differences, \\(d\\) and \\(D\\), and \\(p\\), \\(q\\), \\(P\\), and \\(Q\\).\nPlease remember to specify the criterion for selecting a model beforehand and consider such options as cross-validation and using a testing set.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Autoregressive Integrated Moving Average (ARIMA) Models</span>"
    ]
  },
  {
    "objectID": "l05_arima.html#appendix",
    "href": "l05_arima.html#appendix",
    "title": "\n5  Autoregressive Integrated Moving Average (ARIMA) Models\n",
    "section": "\n5.5 Appendix",
    "text": "5.5 Appendix\nEquivalences\nMathematically, some models are equivalent one to another. Below are some examples.\n\nForecasts of ARIMA(0,1,1) are equivalent to simple exponential smoothing.\n\nAn ARIMA(0,1,1) model can be written as \\[\nY_t = Y_{t-1} + \\theta_1 Z_{t-1} + Z_t,\n\\] from which the one-step-ahead forecast is \\[\n\\hat{Y}_t = Y_{t-1} + \\theta_1 (Y_{t-1} - \\hat{Y}_{t-1}).\n\\] If we define \\(\\theta_1 = \\alpha - 1\\), then the above equation transforms to \\[\n\\begin{split}\n\\hat{Y}_t &= Y_{t-1} + (\\alpha - 1) (Y_{t-1} - \\hat{Y}_{t-1})\\\\\n&= Y_{t-1} + (\\alpha - 1) Y_{t-1} - (\\alpha - 1) \\hat{Y}_{t-1}\\\\\n&= \\alpha Y_{t-1} + (1 - \\alpha) \\hat{Y}_{t-1}\n\\end{split}\n\\]\n\nForecasts of ARIMA(0,2,2) are equivalent to Holt’s method.\nForecasts of SARIMA(0,1,\\(s+1\\))(0,1,0)\\(_s\\) are equivalent to Holt–Winters additive method.\n\n\n\n\n\nBenson B, Magnuson J, Sharma S (2020) Global lake and river ice phenology database. Version 1 (G01377). National Snow and Ice Data Center, Boulder, CO, USA\n\n\nBox GEP, Jenkins GM (1976) Time series analysis: Forecasting and control. Holden-Day, San Francisco, CA, USA\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Autoregressive Integrated Moving Average (ARIMA) Models</span>"
    ]
  },
  {
    "objectID": "l06_garch.html",
    "href": "l06_garch.html",
    "title": "6  Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models",
    "section": "",
    "text": "6.1 Introduction\nThe goal of this lecture is to introduce a class of models with conditional heteroskedasticity for stationary time series. You will be able to recognize the presence of such heteroskedasticity from graphs or statistical tests and model it.\nObjectives\nReading materials\nAudio overview\nIn contrast to the traditional time series analysis that focuses on modeling the conditional first moment, models of autoregressive conditional heteroskedasticity (ARCH) and generalized autoregressive conditional heteroskedasticity (GARCH) specifically take the dependency of the conditional second moment into modeling consideration and accommodate the increasingly important need to explain and model risk and uncertainty in, for example, financial time series.\nThe ARCH models were introduced in 1982 by Robert Engle to model varying (conditional) variance or volatility of time series. It is often found in economics that the larger values of time series also lead to larger instability (i.e., larger variances), which is termed (conditional) heteroskedasticity. Standard examples for demonstrating ARCH or GARCH effects are time series of stock prices, interest and foreign exchange rates, and even some environmental processes: high-frequency data on wind speed, energy production, air quality, etc. (see examples in Cripps and Dunsmuir 2003; Marinova and McAleer 2003; Taylor and Buizza 2004; Campbell and Diebold 2005). In 2003, Robert F. Engle was awarded 1/2 of the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel for his work on ARCH models (the other half was awarded to C. Granger, see Section 8.3). Although it is not one of the prizes that Alfred Nobel established in his will in 1895, the Sveriges Riksbank Prize is referred to along with the other Nobel Prizes by the Nobel Foundation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models</span>"
    ]
  },
  {
    "objectID": "l06_garch.html#features-of-arch",
    "href": "l06_garch.html#features-of-arch",
    "title": "6  Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models",
    "section": "\n6.2 Features of ARCH",
    "text": "6.2 Features of ARCH\nSince financial data typically have the autocorrelation coefficient close to 1 at lag 1 (e.g., the exchange rate between the US and Canadian dollar hardly changes from today to tomorrow), it is much more interesting and also more practically relevant to model the returns of a financial time series rather than the series itself. Let \\(Y_t\\) be a time series of stock prices. The returns \\(X_t\\) measure the relative changes in price and are typically defined as simple returns \\[\nX_t = \\frac{Y_t - Y_{t-1}}{ Y_{t-1} } = \\frac{Y_t}{ Y_{t-1} } - 1\n\\tag{6.1}\\] or logarithmic returns \\[\nX_t = \\ln Y_t - \\ln Y_{t-1}.\n\\tag{6.2}\\] The two forms are approximately the same, since \\[\n\\begin{split}\n\\ln Y_t - \\ln Y_{t-1} &= \\ln \\left(\\frac{Y_{t}}{Y_{t-1}} \\right) \\\\\n&= \\ln \\left(\\frac{Y_{t-1} + Y_{t} - Y_{t-1}}{Y_{t-1}} \\right) \\\\\n&= \\ln \\left(1 + \\frac{Y_{t} - Y_{t-1}}{Y_{t-1}} \\right) \\\\\n&\\approx \\frac{ Y_{t} - Y_{t-1}}{ Y_{t -1} }.\n\\end{split}\n\\tag{6.3}\\] The approximation \\(\\ln(1+x) \\approx x\\) works when \\(x\\) is close to zero, which is true for many real-world financial problems. However, logarithmic returns are often preferred because in many applications their distribution is closer to normal compared to one of simple returns. Also, log returns have infinite support (from \\(-\\infty\\) to \\(+\\infty\\)) compared to simple returns that have a lower bound of \\(-1\\).\nFor an example calculation of log returns, see Figure 6.1.\n\nCode# Load data and calculate log returns\nCAD &lt;- readr::read_csv(\"data/CAD.csv\",\n                       na = \"Bank holiday\",\n                       skip = 11) %&gt;% \n    filter(!is.na(USD)) %&gt;% \n    mutate(lnR = c(NA, diff(log(USD)))) %&gt;% \n    filter(!is.na(lnR))\n\np1 &lt;- ggplot(CAD, aes(x = Date, y = USD)) + \n    geom_line() +\n    ylab(\"CAD per USD\")\np2 &lt;- ggplot(CAD, aes(x = Date, y = lnR)) + \n    geom_line() +\n    ylab(\"Log return\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 6.1: CAD per USD daily noon exchange rates and log returns, from 2006-02-22 to 2016-02-22 (excluding bank holidays), obtained from Bank of Canada.\n\n\n\n\nRydberg (2000) summarizes some important stylized features of financial return series, which have been repeatedly observed in all kinds of assets including stock prices, interest rates, and foreign exchange rates:\n\n\nHeavy tails. The distribution of the returns \\(X_t\\) has tails heavier than the tails of a normal distribution.\n\nVolatility clustering. Large price changes occur in clusters. Indeed, often large price changes tend to be followed by large price changes, and periods of tranquility alternate with periods of high volatility.\n\nAsymmetry. There is evidence that the distribution of stock returns is slightly negatively skewed. One possible explanation could be that trades react more strongly to negative information than to positive information.\n\nAggregational Gaussianity. When the sampling frequency decreases, the central limit law sets in and the distribution of the returns over a long period tends toward a normal distribution.\n\nLong-range dependence. The returns themselves hardly show any autocorrelation, which, however, does not mean that they are independent. Both squared returns and absolute returns often exhibit persistent autocorrelations indicating possible long-memory dependence in those series.\n\nFigure 6.2 is the simplest check for the presence of ARCH effects: when the time series is not autocorrelated but is autocorrelated if squared.\n\nCodep1 &lt;- forecast::ggAcf(CAD$lnR) +\n    ggtitle(\"Log returns\")\np2 &lt;- forecast::ggAcf(CAD$lnR^2) +\n    ggtitle(bquote('(Log returns)'^2))\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 6.2: ACF of log returns and squared log returns for USD/CAD exchange.\n\n\n\n\nFor comparison, see Figure 6.3 with similar plots for simulated i.i.d. series.\n\nCodeset.seed(1)\niid &lt;- rnorm(nrow(CAD))\n\np1 &lt;- forecast::ggAcf(iid) +\n    ggtitle(\"iid\")\np2 &lt;- forecast::ggAcf(iid^2) +\n    ggtitle(bquote('iid'^2))\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 6.3: ACF of simulated i.i.d. \\(N(0,1)\\) series.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models</span>"
    ]
  },
  {
    "objectID": "l06_garch.html#models",
    "href": "l06_garch.html#models",
    "title": "6  Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models",
    "section": "\n6.3 Models",
    "text": "6.3 Models\nEngle (1982) defines an autoregressive conditional heteroskedastic (ARCH) model as \\[\n\\begin{split}\nX_{t} &=  \\sigma_{t} \\varepsilon_{t}, \\\\\n\\sigma^{2}_{t} &=  a_{0} + a_{1} X^{2}_{t-1} + \\dots + a_{p} X^{2}_{t-p},\n\\end{split}\n\\tag{6.4}\\] where \\(a_{0} &gt; 0\\), \\(a_{j} \\geqslant 0\\), \\(\\varepsilon_{t} \\sim \\mathrm{i.i.d.}(0,1)\\), and \\(\\varepsilon_{t}\\) is independent of \\(X_{t - j}\\), where \\(j \\geqslant 1\\). We write \\(X_{t} \\sim \\mathrm{ARCH} (p)\\).\nWe can see that \\[\n\\begin{split}\n\\mathrm{E} X_{t} & =  0, \\\\\n\\mathrm{var} \\left( X_{t} | X_{t - 1} , \\dots , X_{t - p} \\right) &= \\sigma^{2}_{t}, \\\\\n\\mathrm{cov} \\left( X_{t} , X_{k} \\right) &= 0 ~~\\mathrm{for~all}~~ t \\neq k.\n\\end{split}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nStationary ARCH is white noise.\n\n\nThus, in ARCH, the predictive distribution of \\(X_{t}\\) based on its past is a scale-transform of the distribution of \\(\\varepsilon_{t}\\) with the scaling constant \\(\\sigma_{t}\\) depending on the past of the process.\nBollerslev (1986) introduced a generalized autoregressive conditional heteroskedastic (GARCH) model by replacing the second formula in Equation 6.4 with \\[\n\\begin{split}\n\\sigma^{2}_{t} &= a_{0}  + a_{1} X^{2}_{ t  - 1} + \\dots + a_{p} X^{2}_{ t  - p} + b_{1} \\sigma^{2}_{t - 1} + \\dots + b_{q} \\sigma^{2}_{t  - q}\\\\\n&= a_0  + \\sum_{i=1}^p a_{i} X^{2}_{ t  - i} + \\sum_{j=1}^q b_{j} \\sigma^{2}_{ t  - j},\n\\end{split}\n\\tag{6.5}\\] where \\(a_{0} &gt; 0\\), \\(a_{i} \\geqslant 0\\), and \\(b_{j} \\geqslant 0\\). We write \\(X_{t}  \\sim  \\mathrm{GARCH}(p, q)\\).\nNotice the similarity between ARMA and GARCH models.\nThe parameters of ARCH/GARCH models are estimated using the method of conditional maximum likelihood. There exist several tests for ARCH/GARCH effects (e.g., analyzing time series and ACF plots, the Engle’s Lagrange multiplier test).\nThe approaches to selecting the orders \\(p\\) and \\(q\\) for GARCH include:\n\nVisual analysis of ACF and PACF of the squared time series and other residual diagnostics;\nVariations of information criteria such as AIC and BIC to account for the number of estimated parameters in the GARCH model (Brooks and Burke 2003);\nUsing GARCH(1,1) by following Hansen and Lunde (2005);\nUsing out-of-sample forecasts (comparing alternative model specifications on a testing set).\n\n\n6.3.1 Lagrange multiplier test\nThe Lagrange multiplier (LM) test is equivalent to an \\(F\\)-test for the significance of the least squares regression on squared values: \\[\nX^{2}_{t} = \\alpha_0  + \\alpha_1 X^{2}_{t-1} + \\dots + \\alpha_m X^{2}_{t-m} + e_t,\n\\tag{6.6}\\] where \\(e_t\\) denotes the error term, \\(m\\) is a positive integer, \\(t = m+1,\\dots,T\\), and \\(T\\) is the sample size (length of the time series).\nSpecifically, the null hypothesis is \\[\nH_0: \\alpha_1 = \\dots = \\alpha_m = 0.\n\\] Let the sum of squares total \\[\nSST = \\sum_{t=m+1}^T \\left( X_t^2 - \\overline{X_t^2} \\right) ^2,\n\\] where \\(\\overline{X_t^2}\\) is the sample mean of \\(X_t^2\\). The sum of squares of the errors \\[\nSSE = \\sum_{t=m+1}^T \\hat{e}_t^2,\n\\] where \\(\\hat{e}_t\\) is the least-squares residual of the linear regression (Equation 6.6).\nThen, the test statistic \\[\nF = \\frac{(SST - SSE)/m}{SSE/(T-2m-1)},\n\\tag{6.7}\\] which is asymptotically distributed under the null hypothesis as a \\(\\chi^2\\) distribution with \\(m\\) degrees of freedom.\n\n\n\n\n\n\nNoteExample: Testing ARCH effects in USD/CAD log returns\n\n\n\nWe have seen in Figure 6.2 the autocorrelation of squared log returns. Now apply the formal LM test.\n\nm &lt;- 12\nFinTS::ArchTest(CAD$lnR, lags = m)\n\n#&gt; \n#&gt;  ARCH LM-test; Null hypothesis: no ARCH effects\n#&gt; \n#&gt; data:  CAD$lnR\n#&gt; Chi-squared = 377, df = 12, p-value &lt;2e-16\n\n\nThe LM test implemented in the function FinTS::ArchTest() detects the presence of ARCH effects (rejects the null hypothesis) when considering 12 lags. In base R, the same results can be obtained by running the \\(F\\) test as follows:\n\nmat &lt;- embed(CAD$lnR^2, m + 1)\nmod &lt;- lm(mat[,1] ~ mat[,-1])\nanova(mod)\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: mat[, 1]\n#&gt;             Df   Sum Sq  Mean Sq F value Pr(&gt;F)    \n#&gt; mat[, -1]   12 5.15e-06 4.29e-07    36.8 &lt;2e-16 ***\n#&gt; Residuals 2479 2.89e-05 1.20e-08                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\nNoteExample: GARCH model for USD/CAD log returns\n\n\n\nLet’s estimate a GARCH(1,1) model for these data, using the conditional ML method. Note that omega in the results below is denoted \\(a_0\\) in our equations (the intercept in the variance model):\n\nlibrary(fGarch)\ngarch11 &lt;- fGarch::garchFit(lnR ~ garch(1, 1), \n                            data = CAD, trace = FALSE)\ngarch11@description &lt;- \"---\"\ngarch11\n\n#&gt; \n#&gt; Title:\n#&gt;  GARCH Modelling \n#&gt; \n#&gt; Call:\n#&gt;  fGarch::garchFit(formula = lnR ~ garch(1, 1), data = CAD, trace = FALSE) \n#&gt; \n#&gt; Mean and Variance Equation:\n#&gt;  lnR ~ garch(1, 1)\n#&gt;  [data = CAD]\n#&gt; \n#&gt; Conditional Distribution:\n#&gt;  norm \n#&gt; \n#&gt; Coefficient(s):\n#&gt;          mu        omega       alpha1        beta1  \n#&gt; -4.5264e-05   2.2304e-07   5.6511e-02   9.3805e-01  \n#&gt; \n#&gt; Std. Errors:\n#&gt;  based on Hessian \n#&gt; \n#&gt; Error Analysis:\n#&gt;          Estimate  Std. Error  t value Pr(&gt;|t|)    \n#&gt; mu     -4.526e-05   9.863e-05   -0.459  0.64630    \n#&gt; omega   2.230e-07   6.893e-08    3.236  0.00121 ** \n#&gt; alpha1  5.651e-02   6.699e-03    8.436  &lt; 2e-16 ***\n#&gt; beta1   9.381e-01   6.846e-03  137.013  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Log Likelihood:\n#&gt;  9445    normalized:  3.77 \n#&gt; \n#&gt; Description:\n#&gt;  ---\n\n\nNext, we run diagnostics of the fitted model, i.e., whether the residuals \\(\\varepsilon_{t}\\) are white noise and are normally distributed. The code plot(garch11) generates scatterplots, histograms, Q-Q plots, and ACF plots of the original data and the obtained residuals (Figure 6.4). Of course, the analysis can be performed also with separate commands. For example, see the ACFs of the residuals (Figure 6.5).\n\nCodepar(mfrow = c(1, 2))\nplot(garch11, which = c(3, 13))\n\n\n\n\n\n\nFigure 6.4: Selected diagnostics of the GARCH(1,1) model for the USD/CAD log returns.\n\n\n\n\n\nCodeet &lt;- residuals(garch11, standardize = TRUE)\np1 &lt;- forecast::ggAcf(et) +\n    ggtitle(\"Residuals\")\np2 &lt;- forecast::ggAcf(et^2) +\n    ggtitle(bquote('Residuals'^2))\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 6.5: ACF of residuals from the GARCH(1,1) model for the USD/CAD log returns.\n\n\n\n\nBased on the plots, autocorrelations were effectively removed, but the assumption of normality is violated – Figure 6.4 shows heavy and almost symmetric tails in the distribution of GARCH residuals.\nTo account for the heavy tails, we change the conditional distribution from normal to the standardized Student \\(t\\)-distribution (see ?fGarch::std).\n\ngarch11t &lt;- fGarch::garchFit(lnR ~ garch(1, 1),\n                             data = CAD, trace = FALSE, \n                             cond.dist = \"std\")\ngarch11t@description &lt;- \"---\"\ngarch11t\n\n#&gt; \n#&gt; Title:\n#&gt;  GARCH Modelling \n#&gt; \n#&gt; Call:\n#&gt;  fGarch::garchFit(formula = lnR ~ garch(1, 1), data = CAD, cond.dist = \"std\", \n#&gt;     trace = FALSE) \n#&gt; \n#&gt; Mean and Variance Equation:\n#&gt;  lnR ~ garch(1, 1)\n#&gt;  [data = CAD]\n#&gt; \n#&gt; Conditional Distribution:\n#&gt;  std \n#&gt; \n#&gt; Coefficient(s):\n#&gt;          mu        omega       alpha1        beta1        shape  \n#&gt; -3.1972e-05   1.8744e-07   5.5352e-02   9.4057e-01   8.6423e+00  \n#&gt; \n#&gt; Std. Errors:\n#&gt;  based on Hessian \n#&gt; \n#&gt; Error Analysis:\n#&gt;          Estimate  Std. Error  t value Pr(&gt;|t|)    \n#&gt; mu     -3.197e-05   9.433e-05   -0.339   0.7346    \n#&gt; omega   1.874e-07   8.038e-08    2.332   0.0197 *  \n#&gt; alpha1  5.535e-02   7.765e-03    7.128 1.02e-12 ***\n#&gt; beta1   9.406e-01   7.760e-03  121.209  &lt; 2e-16 ***\n#&gt; shape   8.642e+00   1.420e+00    6.084 1.17e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Log Likelihood:\n#&gt;  9473    normalized:  3.78 \n#&gt; \n#&gt; Description:\n#&gt;  ---\n\n\nVerify the diagnostics plots in Figure 6.6.\n\nCodepar(mfrow = c(2, 2))\nplot(garch11t, which = c(3, 13))\nplot(garch11t, which = c(9, 11))\n\n\n\n\n\n\nFigure 6.6: Selected diagnostics of the GARCH(1,1) model with the standardized Student-t distribution for the USD/CAD log returns.\n\n\n\n\nNow we can use the model for predictions (Figure 6.7). Few things to note here:\n\nthe model is ‘self-contained’ so we can just specify n.ahead for the number of steps to be predicted;\nGARCH models are typically applied to long time series, so the argument nx limits the length of the plotted observed time series (by default only the most recent 25% of observations are plotted);\nthe argument conf specifies the confidence level, then the conditional distribution from the model is used to compute critical values for the interval forecasts. Alternatively, one can specify the critical values manually with the argument crit_val.\n\nFor more details, see ?fGarch::predict.\n\nCodepredict(garch11t, n.ahead = 30, \n        conf = 0.95,\n        plot = TRUE)\n\n#&gt;    meanForecast meanError standardDeviation lowerInterval upperInterval\n#&gt; 1      -3.2e-05   0.00445           0.00445      -0.00892       0.00885\n#&gt; 2      -3.2e-05   0.00446           0.00446      -0.00894       0.00888\n#&gt; 3      -3.2e-05   0.00448           0.00448      -0.00896       0.00890\n#&gt; 4      -3.2e-05   0.00449           0.00449      -0.00899       0.00892\n#&gt; 5      -3.2e-05   0.00450           0.00450      -0.00901       0.00895\n#&gt; 6      -3.2e-05   0.00451           0.00451      -0.00903       0.00897\n#&gt; 7      -3.2e-05   0.00452           0.00452      -0.00906       0.00899\n#&gt; 8      -3.2e-05   0.00453           0.00453      -0.00908       0.00902\n#&gt; 9      -3.2e-05   0.00455           0.00455      -0.00910       0.00904\n#&gt; 10     -3.2e-05   0.00456           0.00456      -0.00913       0.00906\n#&gt; 11     -3.2e-05   0.00457           0.00457      -0.00915       0.00908\n#&gt; 12     -3.2e-05   0.00458           0.00458      -0.00917       0.00911\n#&gt; 13     -3.2e-05   0.00459           0.00459      -0.00919       0.00913\n#&gt; 14     -3.2e-05   0.00460           0.00460      -0.00921       0.00915\n#&gt; 15     -3.2e-05   0.00461           0.00461      -0.00924       0.00917\n#&gt; 16     -3.2e-05   0.00462           0.00462      -0.00926       0.00919\n#&gt; 17     -3.2e-05   0.00463           0.00463      -0.00928       0.00922\n#&gt; 18     -3.2e-05   0.00464           0.00464      -0.00930       0.00924\n#&gt; 19     -3.2e-05   0.00466           0.00466      -0.00932       0.00926\n#&gt; 20     -3.2e-05   0.00467           0.00467      -0.00934       0.00928\n#&gt; 21     -3.2e-05   0.00468           0.00468      -0.00937       0.00930\n#&gt; 22     -3.2e-05   0.00469           0.00469      -0.00939       0.00932\n#&gt; 23     -3.2e-05   0.00470           0.00470      -0.00941       0.00934\n#&gt; 24     -3.2e-05   0.00471           0.00471      -0.00943       0.00936\n#&gt; 25     -3.2e-05   0.00472           0.00472      -0.00945       0.00938\n#&gt; 26     -3.2e-05   0.00473           0.00473      -0.00947       0.00940\n#&gt; 27     -3.2e-05   0.00474           0.00474      -0.00949       0.00942\n#&gt; 28     -3.2e-05   0.00475           0.00475      -0.00951       0.00945\n#&gt; 29     -3.2e-05   0.00476           0.00476      -0.00953       0.00947\n#&gt; 30     -3.2e-05   0.00477           0.00477      -0.00955       0.00949\n\n\n\n\n\n\n\nFigure 6.7: Predictions using the GARCH(1,1) model for the USD/CAD log returns.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models</span>"
    ]
  },
  {
    "objectID": "l06_garch.html#extensions",
    "href": "l06_garch.html#extensions",
    "title": "6  Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models",
    "section": "\n6.4 Extensions",
    "text": "6.4 Extensions\nThere was a boom in creating new models by adding new features to GARCH:\n\nIGARCH – integrated GARCH\nEGARCH – exponential GARCH\nTGARCH – threshold GARCH\nQGARCH – quadratic GARCH\nGARCH-M – GARCH with heteroskedasticity in mean\nNGARCH – nonlinear GARCH\n…\nMARCH – modified GARCH\nSTARCH – structural ARCH\n…\n\nThus, these papers had to appear: Hansen and Lunde (2005) and Bollerslev (2009).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models</span>"
    ]
  },
  {
    "objectID": "l06_garch.html#model-building",
    "href": "l06_garch.html#model-building",
    "title": "6  Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models",
    "section": "\n6.5 Model building",
    "text": "6.5 Model building\nWe have considered the models for conditional heteroskedasticity, and in the example, we estimated the mean just as a constant (intercept mu), however, in a more general case one might need to model and remove trend and cyclical variability along with autocorrelations (recall the methods of smoothing, ARMA, and ARIMA modeling) before exploring the need to model ARCH.\nBelow are the steps for such a more general case of analysis, adapted from Chapter 3.3 in Tsay (2005):\n\nSpecify a mean equation by testing for trend and serial dependence in the data and, if necessary, build a time series model (e.g., an ARMA model) to remove any linear dependence.\nUse the residuals of the mean equation to test for ARCH effects.\nIf the ARCH effects are statistically significant, specify a volatility model and perform a joint estimation of the mean and volatility equations.\nCheck the fitted model carefully and refine it if necessary.\n\n\n\n\n\n\n\nNote\n\n\n\nThe joint estimation can be done in R using the function fGarch::garchFit() and specifying, e.g., formula = ~ arma(2, 1) + garch(1, 1).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models</span>"
    ]
  },
  {
    "objectID": "l06_garch.html#conclusion",
    "href": "l06_garch.html#conclusion",
    "title": "6  Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models",
    "section": "\n6.6 Conclusion",
    "text": "6.6 Conclusion\nWhereas originated in financial analysis, GARCH models are becoming popular in other domains, including environmental science. Note that close alternatives exist, for example, GAMLSS allows modeling different distributional parameters such as mean, scale, skewness, etc.\nGARCH effects are tested for and modeled after the mean (i.e., trend) and autocorrelations are removed. Standard model selection techniques can be adapted to specify GARCH models.\nR packages offering functions for GARCH modeling include (in alphabetic order): bayesforecast, betategarch, fGarch (used here), garchx, rmgarch, rugarch, tseries, and more (see, for example, CRAN Task Views on Empirical Finance).\n\n\n\n\nBollerslev T (1986) Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics 31:307–327. https://doi.org/10.1016/0304-4076(86)90063-1\n\n\nBollerslev T (2009) Glossary to ARCH (GARCH). In: Volatility and time series econometrics: Essays in honour of Robert F. Engle. SSRN\n\n\nBrooks C, Burke SP (2003) Information criteria for GARCH model selection. The European Journal of Finance 9:557–580. https://doi.org/10.1080/1351847021000029188\n\n\nCampbell SD, Diebold FX (2005) Weather forecasting for weather derivatives. Journal of the American Statistical Association 100:6–16. https://doi.org/10.1198/016214504000001051\n\n\nCripps E, Dunsmuir WTM (2003) Modeling the variability of Sydney Harbor wind measurements. Journal of Applied Meteorology 42:1131–1138. https://doi.org/10.1175/1520-0450(2003)042&lt;1131:MTVOSH&gt;2.0.CO;2\n\n\nEngle RF (1982) Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica 50:987–1007. https://doi.org/10.2307/1912773\n\n\nHansen PR, Lunde A (2005) A comparison of volatility models: Does anything beat a GARCH(1, 1)? Journal of Applied Econometrics 20:873–889. https://doi.org/10.1002/jae.800\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern time series analysis. Springer-Verlag, Berlin, Germany\n\n\nMarinova D, McAleer M (2003) Modelling trends and volatility in ecological patents in the USA. Environmental Modelling & Software 18:195–203. https://doi.org/10.1016/S1364-8152(02)00079-8\n\n\nRydberg TH (2000) Realistic statistical modelling of financial data. International Statistical Review 68:233–258. https://doi.org/10.2307/1403412\n\n\nTaylor JW, Buizza R (2004) A comparison of temperature density forecasts from GARCH and atmospheric models. Journal of Forecasting 23:337–355. https://doi.org/10.1002/for.917\n\n\nTsay RS (2005) Analysis of financial time series, 2nd edn. John Wiley & Sons, Hoboken, NJ, USA",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalized Autoregressive Conditional Heteroskedasticity (GARCH) Models</span>"
    ]
  },
  {
    "objectID": "l07_trendtest.html",
    "href": "l07_trendtest.html",
    "title": "7  Trend Detection",
    "section": "",
    "text": "7.1 Introduction\nThis lecture demonstrates the effects of autocorrelation on the results of statistical tests for trend detection. You will recall the assumptions of the classical \\(t\\)-test and Mann–Kendall tests and will be able to suggest bootstrapped modifications of these tests to overcome the problem of temporal dependence. Moreover, you will become familiar with tests for non-monotonic parametric trends and stochastic trends.\nObjectives\nReading materials\nAudio overview\nThe majority of studies focus on the detection of linear or monotonic trends, using classical \\(t\\)-test or rank-based Mann–Kendall test, typically under the assumption of uncorrelated data.\nThere exist two main problems:\nHence, our goal is to provide reliable inference even for dependent observations and to test different alternative trend shapes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Trend Detection</span>"
    ]
  },
  {
    "objectID": "l07_trendtest.html#introduction",
    "href": "l07_trendtest.html#introduction",
    "title": "7  Trend Detection",
    "section": "",
    "text": "dependence effect, i.e., the issue of inflating significance due to dependent observations – a possible remedy is to employ bootstrap (Noguchi et al. 2011; Cabilio et al. 2013);\nchangepoints or regime shifts that affect the linear or monotonic trend hypothesis (Seidel and Lanzante 2004; Powell and Xu 2011; Lyubchich 2016).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Trend Detection</span>"
    ]
  },
  {
    "objectID": "l07_trendtest.html#traditional-tests-assuming-independence",
    "href": "l07_trendtest.html#traditional-tests-assuming-independence",
    "title": "7  Trend Detection",
    "section": "\n7.2 ‘Traditional’ tests assuming independence",
    "text": "7.2 ‘Traditional’ tests assuming independence\n\n7.2.1 Student’s \\(t\\)-test for linear trend\nThe Student’s \\(t\\)-test for linear trend uses the regression model of linear trend \\[\nY_t = b_0 + b_1 t + e_t,\n\\] where \\(b_0\\) and \\(b_1\\) are the regression coefficients, \\(t\\) is time, and \\(e_t\\) are regression errors typically assumed to be homoskedastic, uncorrelated, and normally distributed.\nThe test hypotheses are\n\\(H_0\\): no trend (\\(b_1 = 0\\))\\(H_1\\): linear trend (\\(b_1 \\neq 0\\))\nFigure 7.1 shows a simulated stationary time series \\(Y_t \\sim \\mathrm{AR}(1)\\) of length 100 (notice the ‘burn-in’ period in simulations).\n\nCodeset.seed(123)\nY &lt;- arima.sim(list(order = c(1,0,0), ar = 0.5), n = 100, n.start = 100)\nforecast::autoplot(Y)\n\n\n\n\n\n\nFigure 7.1: A simulated stationary AR(1) series of length 100.\n\n\n\n\nApply the \\(t\\)-test to this time series \\(Y_t\\):\n\nt &lt;- 1:length(Y)\nmod &lt;- lm(Y ~ t)\nsummary(mod)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Y ~ t)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.5115 -0.6106  0.0166  0.6808  2.5868 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept) -0.57984    0.20182   -2.87    0.005 **\n#&gt; t            0.00746    0.00347    2.15    0.034 * \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.045,  Adjusted R-squared:  0.0352 \n#&gt; F-statistic: 4.62 on 1 and 98 DF,  p-value: 0.0341\n\n\nType I error (false positives) of this test is inflated due to the dependence effect (the assumption of uncorrelatedness is violated). Additionally, this test is limited only to detecting linear trends (see the alternative hypothesis \\(H_1\\)).\n\n7.2.2 Mann–Kendall test for monotonic trend\nThe Mann–Kendall test is based on the Kendall rank correlation and is used to determine if a non-seasonal time series has a monotonic trend over time.\n\\(H_0\\): no trend\\(H_1\\): monotonic trend\nTest statistic: \\[\nS=\\sum_{k=1}^{n-1}\\sum_{j=k+1}^n sgn(X_j-X_k),\n\\] where \\(sgn(x)\\) takes on the values of 1, 0, and \\(-1\\), for \\(x&gt;0\\), \\(x=0\\), and \\(x&lt;0\\), respectively.\nKendall (1975) showed that \\(S\\) is asymptotically normally distributed and, for the situations where there may be ties in the \\(X\\) values, \\[\n\\begin{split}\n\\mathrm{E}(S) &= 0, \\\\\n\\mathrm{var}(S) &= \\frac{1}{18} \\left[ n(n-1)(2n+5)-\\sum_{j=1}^p t_j(t_j-1)(2t_j+5) \\right],\n\\end{split}\n\\] where \\(p\\) is the number of tied groups in the time series, and \\(t_j\\) is the number of data points in the \\(j\\)th tied group.\nIts seasonal version is the sum of the statistics for individual seasons over all seasons (Hirsch et al. 1982): \\[\nS=\\sum_{j=1}^m S_j.\n\\]\nFor data sets as small as \\(n = 2\\) and \\(m = 12\\), the normal approximation of the test statistic is adequate and thus the test is easy to use. The method also accommodates both\n\na moderate number of missing observations and\nvalues below the detection limit, as the latter are treated as ties (see details in Esterby 1996).\n\nTo apply the test, use the package Kendall. Also, note the statement about bootstrap in the help file for the function Kendall::MannKendall().\n\nKendall::MannKendall(Y)\n\n#&gt; tau = 0.13, 2-sided pvalue =0.06\n\n\nThis test is still limited to only monotonic trends and independent observations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Trend Detection</span>"
    ]
  },
  {
    "objectID": "l07_trendtest.html#introduction-to-bootstrap",
    "href": "l07_trendtest.html#introduction-to-bootstrap",
    "title": "7  Trend Detection",
    "section": "\n7.3 Introduction to bootstrap",
    "text": "7.3 Introduction to bootstrap\n\n7.3.1 Bootstrap for independent data\nWhen some of the statistical assumptions are violated, we may switch to using other methods or try to accommodate these violations by modifying part of the existing method.\n\nOne of the assumptions of Pearson correlation analysis is the absence of outliers. When outliers are in the data, we can opt for using Spearman correlations based on ranks, not the actual values. Thus, here we switch the method.\nWe often use the normal distribution for inference about the population mean because we assume that averages of independent samples coming from the same population are distributed normally. The normal approximation is true for large samples or for small samples if the underlying population is close to being normally distributed. So what happens if we have a small sample from a non-normal population? We do not know the distributional law under which sample means are distributed in this case, but we can approximate the distribution using bootstrapped statistics.\n\nThe seminal paper by Efron (1979) describes bootstrap for i.i.d. data. In two words, the idea is the following: we can relax distributional assumptions and reconstruct the distribution of the sample statistic by resampling data with replacement and recalculating the statistic over and over. The resampling step will give us artificial ‘new’ samples, while the statistics calculated multiple times on those samples will let us approximate the distribution of the statistic of interest. Thus, by repeating the resampling and estimation steps many times, we will know how the statistic is distributed, even if the sample is small and not normally distributed.\nLet \\(x_i\\) (\\(i = 1, \\dots, n\\)) be sample values collected from a non-normally distributed population using simple random sampling. If the sample size \\(n\\) is small, we cannot be sure that the distribution of sample averages \\(\\bar{x}\\) is normal, so we will approximate it using the following bootstrapping steps:\n\nLet \\(x^*_i\\) (\\(i = 1, \\dots, n\\)) be a sample with a replacement from the original sample \\(x_i\\). This is a way for us, without knowing the underlying distribution of the population and without collecting new data, to get a new artificial sample from the population.\nCalculate the mean (or another statistic of interest) from the bootstrapped data. For example, \\(\\bar{x}^* = n^{-1} \\sum x^*_i\\) is the bootstrapped mean.\nRepeat the steps above a large number of times to obtain a distribution of the statistics \\(\\bar{x}^*_1, \\dots, \\bar{x}^*_B\\) (the bootstrapped distribution), where \\(B\\) is a large enough number of bootstrap replications. Typically, \\(B \\geqslant 1000\\).\nFinally, we use the bootstrapped distribution for the intended analysis. It often involves calculating confidence intervals. There are a few ways the intervals can be calculated (see Davison and Hinkley 1997), some of the methods provide symmetric intervals, while others do not. Probably the simplest is the percentile confidence interval, which computes the \\(\\alpha/2\\)th and \\((1-\\alpha/2)\\)th quantiles of the bootstrapped distribution, to get an interval for confidence \\(1 - \\alpha\\).\n\n\n\n\n\n\n\nNoteExample: Bootstrap confidence interval for the population mean\n\n\n\nConsider a small sample of mercury (Hg) concentrations in fish tissue (Lyubchich et al. 2016). These observations do not constitute a time series; the data were collected in such a way that we can treat the samples as independent. Figure 7.2 A shows that the underlying population of Hg concentrations is likely not normally distributed, so we will use bootstrapping to calculate a confidence interval for the mean.\n\nCode# Mercury concentrations\nHg &lt;- c(10.159162, 9.190562, 7.776279, 11.417387, 8.494544, \n        9.948271, 7.865391, 7.412350, 8.112304, 7.541787)\n\n# Set seed for reproducible bootstrapping\nset.seed(123)\n\n# Number of bootstrap replications\nB = 1000\n\n# Significance level\nalpha = 0.05\n\n# Compute bootstrapped means (option 1)\nxbar_star &lt;- numeric()\nfor (b in 1:B) {\n    # bootstrapped sample\n    Hgstar &lt;- sample(Hg, replace = TRUE)\n    # bootstrapped mean\n    xbar_star[b] &lt;- mean(Hgstar)\n}\n\n# Compute bootstrapped means (option 2)\nxbar_star &lt;- sapply(1:B, function(b) mean(sample(Hg, replace = TRUE)))\n\n# Bootstrap percentile interval for confidence 1 - alpha\ninterval &lt;- quantile(xbar_star, probs = c(alpha/2, 1 - alpha/2))\ninterval\n\n#&gt;  2.5% 97.5% \n#&gt;  8.04  9.64\n\n\n\nCodep1 &lt;- ggplot(data.frame(x = Hg), aes(x = x)) + \n    geom_histogram(aes(y = after_stat(density)), \n                   boundary = floor(min(Hg)), binwidth = 1, \n                   fill = \"grey50\") +\n    xlab(\"Hg (ng/g)\") + \n    ylab(\"Density\") +\n    ggtitle(\"Hg observations\")\np2 &lt;- ggplot(data.frame(x = xbar_star), aes(x = x)) + \n    geom_histogram(aes(y = after_stat(density)), bins = 15, fill = \"grey50\") +\n    geom_vline(xintercept = interval, lty = 2) +\n    xlab(\"Hg (ng/g)\") + \n    ylab(\"Density\") +\n    ggtitle(\"Bootstrapped means\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 7.2: Histograms of the original data and bootstrapped means. The dashed lines denote the bootstrap confidence interval for the mean.\n\n\n\n\nThe calculated confidence interval can be used in hypothesis testing. Note that the package boot provides functions with more options for the calculations.\n\n\nWe cannot apply the outlined bootstrapping directly to time series or spatial data because these data are not i.i.d. and resampling will break the order and dependence structure.\n\n7.3.2 Bootstrap for time series\nTo account for the dependence structure, some modifications to the bootstrap procedure were proposed, including block bootstrap and sieve bootstrap (see Bühlmann 2002 and references therein).\nWhile the bootstrap for independent data generates samples that mimic the underlying population distribution, the bootstrap for time series also aims to mimic or preserve the dependence structure of the time series. Hence, the first step of the outlined bootstrapping algorithm should be modified so that the generated bootstrapped samples preserve the dependence structure that we want to accommodate in our analysis. For example, if we want to accommodate serial dependence in the \\(t\\)-test, the bootstrapped time series should be autocorrelated similarly to the original series, so we can approximate the distribution of the test statistic when the assumption of independence is violated.\nBlock bootstrap works for general stationary time series or categorical series (e.g., genetic sequences). In the simplest version of the algorithm, the observed time series of length \\(n\\) is used to create overlapping blocks of fixed length \\(l\\), which are then resampled with replacement to create a bootstrapped time series. To match the original sample size, the last block entering the bootstrapped series can be truncated. By resampling blocks rather than individual observations, we preserve the original dependence structure within each block (Bühlmann 2002; Härdle et al. 2003).\nOther versions of this algorithm include non-overlapping blocks and blocks of random length \\(l\\) sampled from the geometric distribution (the latter version is also known as the stationary bootstrap), but these versions often show poorer performance (Bühlmann 2002; Härdle et al. 2003).\nThe block length \\(l\\) should be adjusted each time to the statistic of interest, data generating process, and purpose of the estimation, such as distribution, bias, or variance estimation (Bühlmann 2002). In other words, \\(l\\) is the algorithm’s major tuning parameter that is difficult to select automatically. Bühlmann (2002) also points out the lack of good interpretations and reliable diagnostic tools for the block length \\(l\\).\nSieve bootstrap generally works for time series that are realizations of linear AR(\\(\\infty\\)) processes (Bühlmann 2002), with more recent research extending the method to all stationary purely nondeterministic processes (Kreiss et al. 2011). In this algorithm, an autoregressive (AR) model acts as a sieve by approximating the dependence structure and letting through only the i.i.d. residuals \\(\\epsilon_t\\). We then center (subtract mean) and resample with replacement the residuals (there is no more structure we need to preserve after the sieve, so this bootstrap step is no different from resampling i.i.d. data). We introduce these bootstrapped residuals \\(\\epsilon^*_t\\) back into the AR(\\(\\hat{p}\\)) model to obtain a bootstrapped time series \\(X_t^*\\): \\[\nX_t^* = \\sum_{j=1}^{\\hat{p}} \\hat{\\phi}_j X^*_{t-j} + \\epsilon^*_t,\n\\] where \\(\\hat{\\phi}_j\\) are the AR coefficients, estimated on the original time series \\(X_t\\), and \\(\\hat{p}\\) is the selected AR order. The order \\(p\\) is the main tuning parameter in this algorithm, but as Bühlmann (2002) points out, it has several advantages compared with the block length in block bootstrap:\n\nThe order \\(\\hat{p}\\) can be selected using the Akaike information criterion (AIC) or Bayesian information criterion (BIC) based on the available data, which means the method adapts to the sample size and underlying dependence structure.\nThe AR order is more interpretable than the block length.\nThere exist diagnostic procedures to check how good is the selected \\(\\hat{p}\\), for example, graphs and tests for remaining autocorrelation in the AR(\\(\\hat{p}\\)) residuals \\(\\epsilon_t\\).\n\nOther versions of sieve bootstrap include options of obtaining \\(\\epsilon^*_t\\) from a parametric distribution (e.g., normal distribution with the mean of 0 and variance matching that of \\(\\epsilon_t\\)) or nonparametrically estimated probability density (e.g., using kernel smoothing) for incorporating additional variability in the data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Trend Detection</span>"
    ]
  },
  {
    "objectID": "l07_trendtest.html#bootstrapped-tests-for-trend-detection-in-time-series",
    "href": "l07_trendtest.html#bootstrapped-tests-for-trend-detection-in-time-series",
    "title": "7  Trend Detection",
    "section": "\n7.4 Bootstrapped tests for trend detection in time series",
    "text": "7.4 Bootstrapped tests for trend detection in time series\n\n7.4.1 Bootstrapped \\(t\\)-test and Mann–Kendall test\nNoguchi et al. (2011) enhanced the classical \\(t\\)-test and Mann–Kendall trend test with sieve bootstrap approaches that take into account the serial correlation of data to obtain more accurate and reliable estimates. While taking into account the dependence structure in the data, these tests are still limited to the linear or monotonic case:\n\\(H_0\\): no trend\\(H_1\\): linear trend (\\(t\\)-test) or monotonic trend (Mann–Kendall test)\nApply the sieve-bootstrapped tests to our time series data, using the package funtimes:\n\nfuntimes::notrend_test(Y, ar.method = \"yw\")\n\n#&gt; \n#&gt;  Sieve-bootstrap Student's t-test for a linear trend\n#&gt; \n#&gt; data:  Y\n#&gt; Student's t value = 2, p-value = 0.1\n#&gt; alternative hypothesis: linear trend.\n#&gt; sample estimates:\n#&gt; $AR_order\n#&gt; [1] 1\n#&gt; \n#&gt; $AR_coefficients\n#&gt; phi_1 \n#&gt; 0.344\n\nfuntimes::notrend_test(Y, test = \"MK\", ar.method = \"yw\")\n\n#&gt; \n#&gt;  Sieve-bootstrap Mann--Kendall's trend test\n#&gt; \n#&gt; data:  Y\n#&gt; Mann--Kendall's tau = 0.1, p-value = 0.2\n#&gt; alternative hypothesis: monotonic trend.\n#&gt; sample estimates:\n#&gt; $AR_order\n#&gt; [1] 1\n#&gt; \n#&gt; $AR_coefficients\n#&gt; phi_1 \n#&gt; 0.344\n\n\nNotice the different \\(p\\)-values from the first time we applied the tests without the bootstrap.\n\n7.4.2 Detecting non-monotonic trends\nConsider a time series \\[\nY_t = \\mu(t) + \\epsilon_t,\n\\tag{7.1}\\] where \\(t=1, \\dots, n\\), \\(\\mu(t)\\) is an unknown trend function, and \\(\\epsilon_t\\) is a weakly stationary time series.\nWe would like to test the hypotheses\n\\(H_0\\): \\(\\mu(t)=f(\\theta, t)\\)\\(H_1\\): \\(\\mu(t)\\neq f(\\theta,t)\\),\nwhere the function \\(f(\\cdot, t)\\) belongs to a known family of smooth parametric functions \\(S=\\bigl\\{f(\\theta, \\cdot), \\theta\\in \\Theta \\bigr\\}\\) and \\(\\Theta\\) is a set of possible parameter values and a subset of Euclidean space.\nSpecial cases include\n\nno trend (constant trend) \\(f(\\theta,t)\\equiv 0\\),\nlinear trend \\(f(\\theta,t)=\\theta_0+\\theta_1 t\\), and\n\nquadratic trend \\(f(\\theta,t)=\\theta_0+\\theta_1 t+\\theta_2t^2\\).\n\nThe following local regression or the local factor test statistic was developed by Wang et al. (2008) to be applied to pre-filtered observations replicating the residuals \\(\\epsilon_t\\) in Equation 7.1. The idea is to extract the hypothesized trend \\(f(\\theta,t)\\) and group residual consecutive in time into small groups. Then, apply the ANOVA \\(F\\)-test for these artificial groups: \\[\n\\begin{split}\n\\mathrm{WAVK}_n&= F_n=\\frac{\\mathrm{MST}}{\\mathrm{MSE}} \\\\\n&= \\frac{k_n}{n-1}\\sum_{i=1}^n{\\left( \\overline{V}_{i.}-\\overline{V}_{..}\\right)^2\\Big/ \\frac{1}{n(k_n-1)}\\sum_{i=1}^n\\sum_{j=1}^{k_n}{\\left(V_{ij}-\\overline{V}_{i.}\\right)^2}},\n\\end{split}\n\\] where MST is the treatment sum of squares, MSE is the error sum of squares, \\(\\{V_{i1}, \\dots, V_{ik_n}\\}\\) is \\(k_n\\) pre-filtered observations in the \\(i\\)th group, \\(\\overline{V}_{i.}\\) is the mean of the \\(i\\)th group, \\(\\overline{V}_{..}\\) is the grand mean.\nBoth \\(n\\to \\infty\\) and \\(k_n\\to \\infty\\); \\(\\rm{MSE}\\to\\) constant. Hence, we can consider \\(\\sqrt{n}(\\rm{MST}-\\rm{MSE})\\) instead of \\(\\sqrt{n}(F_n-1)\\).\nLyubchich et al. (2013) extended the WAVK approach:\n\nShowed that the structure of time series errors can be a linear process that is allowed not to degenerate to MA(\\(q\\)) or AR(\\(p\\)), or a conditionally heteroskedastic or GARCH process.\nDeveloped a data-driven bootstrap procedure to estimate the finite sample properties of the WAVK test under the unknown dependence structure.\nProposed to estimate the optimal size of local windows \\(k_n\\) by employing the nonparametric resampling \\(m\\)-out-of-\\(n\\) selection algorithm of Bickel et al. (1997).\n\nThe WAVK test is implemented in the package funtimes with the same sieve bootstrap as the \\(t\\)-test and Mann–Kendall test. It tests the null hypothesis of no trend vs. the alternative of (non)monotonic trend.\n\nfuntimes::notrend_test(Y, test = \"WAVK\", ar.method = \"yw\")\n\n#&gt; \n#&gt;  Sieve-bootstrap WAVK trend test\n#&gt; \n#&gt; data:  Y\n#&gt; WAVK test statistic = 4, moving window = 10, p-value = 0.4\n#&gt; alternative hypothesis: (non-)monotonic trend.\n#&gt; sample estimates:\n#&gt; $AR_order\n#&gt; [1] 1\n#&gt; \n#&gt; $AR_coefficients\n#&gt; phi_1 \n#&gt; 0.344\n\n\nAlso, the version of the test with the hybrid bootstrap by Lyubchich et al. (2013) is available. This version allows the user to specify different alternatives.\n\n# The null hypothesis is the same as above, no trend (constant trend)\nfuntimes::wavk_test(Y ~ 1, \n                    factor.length = \"adaptive.selection\", \n                    ar.method = \"yw\", \n                    out = TRUE)\n\n#&gt; \n#&gt;  Trend test by Wang, Akritas, and Van Keilegom (bootstrap p-values)\n#&gt; \n#&gt; data:  Y \n#&gt; WAVK test statistic = 0.1, adaptively selected window = 4, p-value =\n#&gt; 0.8\n#&gt; alternative hypothesis: trend is not of the form Y ~ 1.\n#&gt; sample estimates:\n#&gt; $trend_coefficients\n#&gt; (Intercept) \n#&gt;      -0.203 \n#&gt; \n#&gt; $AR_order\n#&gt; [1] 1\n#&gt; \n#&gt; $AR_coefficients\n#&gt; phi_1 \n#&gt; 0.344 \n#&gt; \n#&gt; $all_considered_windows\n#&gt;  Window WAVK-statistic p-value\n#&gt;       4         0.1403   0.760\n#&gt;       5        -0.0495   0.862\n#&gt;       7        -0.0955   0.826\n#&gt;      10        -0.2146   0.866\n\n# The null hypothesis is a quadratic trend\nfuntimes::wavk_test(Y ~ poly(t, 2), \n                    factor.length = \"adaptive.selection\", \n                    ar.method = \"yw\", \n                    out = TRUE)\n\n#&gt; \n#&gt;  Trend test by Wang, Akritas, and Van Keilegom (bootstrap p-values)\n#&gt; \n#&gt; data:  Y \n#&gt; WAVK test statistic = 4, adaptively selected window = 4, p-value = 0.01\n#&gt; alternative hypothesis: trend is not of the form Y ~ poly(t, 2).\n#&gt; sample estimates:\n#&gt; $trend_coefficients\n#&gt; (Intercept) poly(t, 2)1 poly(t, 2)2 \n#&gt;      -0.203       2.152      -1.795 \n#&gt; \n#&gt; $AR_order\n#&gt; [1] 0\n#&gt; \n#&gt; $AR_coefficients\n#&gt; numeric(0)\n#&gt; \n#&gt; $all_considered_windows\n#&gt;  Window WAVK-statistic p-value\n#&gt;       4           3.76   0.010\n#&gt;       5           2.97   0.028\n#&gt;       7           1.81   0.086\n#&gt;      10           0.93   0.234\n\n\nFor the application of this test to multiple time series, see Appendix C.\n\n\n\n\n\n\nNote\n\n\n\nStatistical test results depend on the alternative hypothesis. Sometimes the null hypothesis cannot be rejected in favor of the alternative hypothesis because the data do not match the specified alternative. For example, there are numerous examples when in Pearson correlation analysis the null hypothesis of independence cannot be rejected in favor of the alternative hypothesis of linear dependence because the underlying nonlinear dependence cannot be described as a linear relationship. See this vignette describing a few similar cases for the time series.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Trend Detection</span>"
    ]
  },
  {
    "objectID": "l07_trendtest.html#unit-roots",
    "href": "l07_trendtest.html#unit-roots",
    "title": "7  Trend Detection",
    "section": "\n7.5 Unit roots",
    "text": "7.5 Unit roots\nBy now, we have been identifying the order of integration (if a process \\(X_t \\sim\\) I(\\(d\\))) by looking at the time series plot of \\(X_t\\) and (largely) by looking at the plot of sample ACF. We differenced time series again and again until we saw a stable mean in the time series plot and a rapid (compared with linear), exponential-like decline in ACF. Here, we present a hypothesis testing approach originally suggested by Dickey and Fuller (1979) (Dickey–Fuller test).\nLet \\(X_1, \\dots, X_n\\) be observations from an AR(1) model: \\[\n\\begin{split}\nX_t-\\mu & =\\phi_1(X_{t-1}-\\mu) + Z_t, \\\\\nZ_t &\\sim \\mathrm{WN}(0,\\sigma^2),\n\\end{split}\n\\] where \\(|\\phi_1|&lt;1\\) and \\(\\mu=\\mathrm{E}X_t\\). For a large sample size \\(n\\), the maximum likelihood estimator \\(\\hat{\\phi_1}\\) of \\(\\phi_1\\) is approximately \\(N(\\phi_1, (1-\\phi^2_1)/n)\\). However, for the unit root case, this approximation is not valid! Thus, do not be tempted to use the normal approximation to construct a confidence interval for \\(\\phi_1\\) and check if it includes the value 1. Instead, consider a model that assumes a unit root (\\(H_0\\): unit root is present) and immediately removes it by differencing: \\[\n\\begin{split}\n\\Delta X_t = X_t - X_{t-1} &= \\phi^*_0 + \\phi^*_1X_{t-1}+Z_t,\\\\\nZ_t & \\sim {\\rm WN}(0,\\sigma^2),\n\\end{split}\n\\tag{7.2}\\] where \\(\\phi^*_0=\\mu(1-\\phi_1)\\) and \\(\\phi_1^*=\\phi_1 -1\\). Let \\(\\hat{\\phi}_1^*\\) be the OLS estimator of \\(\\phi_1^*\\), with its standard error estimated as \\[\n\\widehat{\\mathrm{SE}}\\left( \\hat{\\phi}_1^* \\right)\n= S\\left( \\sum_{t=2}^n \\left(X_{t-1}-\\bar{X} \\right)^2 \\right)^{-1/2},\n\\] where \\(S^2=\\sum_{t=2}^n\\left( \\Delta X_t - \\hat{\\phi}_0^* - \\hat{\\phi}_1^*X_{t-1}\\right)^2/(n-3)\\) and \\(\\bar{X}\\) is the sample mean. Dickey and Fuller (1979) derived the limit distribution of the test statistic \\[\n\\hat{\\tau}_{\\mu}=\\frac{\\hat{\\phi}_1^*}{\\widehat{\\mathrm{SE}}\\left( \\hat{\\phi}_1^* \\right)},\n\\] so we know the critical levels from this distribution (the 0.01, 0.05, and 0.10 quantiles are \\(-3.43\\), \\(-2.86\\), and \\(-2.57\\), respectively) and can test the null hypothesis of \\(\\phi_1^*=0\\) (notice the similarity with the usual \\(t\\)-test for significance of regression coefficients). An important thing to remember is that the \\(H_0\\) here assumes a unit root (nonstationarity).\nFor a more general AR(\\(p\\)) model, statistic \\(\\hat{\\tau}_{\\mu}\\) has a similar form (the \\(\\phi_1^*\\) is different: \\(\\phi_1^* = \\sum_{i=1}^p\\phi_i -1\\)), and the test is then called the Augmented Dickey–Fuller test (ADF test). The order \\(p\\) can be specified in advance or selected automatically using AIC or BIC.\nAnother popular test for unit roots, the Phillips–Perron test (PP test), is built on the ADF test and considers the same null hypothesis.\n\n\n\n\n\n\nNoteExample: ADF test applied to simulated data\n\n\n\nSimulate a time series \\(Y_t \\sim\\) I(2) and apply the rule of thumb approach of taking differences (Figure 7.3). The results in Figure 7.3 show that two differences are enough to remove the trend (\\(d = 2\\)).\n\nCodeset.seed(123)\nZ &lt;- rnorm(200)\nYt &lt;- ts(cumsum(cumsum(Z)))\n\n\n\nCode# Apply first-order (non-seasonal) differences\nD1 &lt;- diff(Yt)\n\n# Apply first-order (non-seasonal) differences again\nD2 &lt;- diff(D1)\n\np1 &lt;- forecast::autoplot(Yt) + \n    ylab(\"Original\") + \n    ggtitle(\"Yt\")\np2 &lt;- forecast::ggAcf(Yt) + \n    ggtitle(\"Yt\")\np3 &lt;- forecast::autoplot(D1) + \n    ylab(\"First differences\") + \n    ggtitle(\"(1-B)Yt\")\np4 &lt;- forecast::ggAcf(D1) + \n    ggtitle(\"(1-B)Yt\")\np5 &lt;- forecast::autoplot(D2) + \n    ylab(\"Second differences\") + \n    ggtitle(\"(1-B)2Yt\")\np6 &lt;- forecast::ggAcf(D2) + \n    ggtitle(\"(1-B)2Yt\")\n(p1 + p2) / (p3 + p4) / (p5 + p6) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 7.3: Time series and ACF plots for identifying the order of differences \\(d\\) of the simulated time series.\n\n\n\n\nNow apply the test, potentially several times, to identify the order \\(d\\).\n\ntseries::adf.test(Yt)\n\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  Yt\n#&gt; Dickey-Fuller = -2, Lag order = 5, p-value = 0.8\n#&gt; alternative hypothesis: stationary\n\n\nWith the high \\(p\\)-value, we cannot reject the \\(H_0\\) of a unit root. Apply the test again on the differenced series.\n\ntseries::adf.test(diff(Yt))\n\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  diff(Yt)\n#&gt; Dickey-Fuller = -2, Lag order = 5, p-value = 0.5\n#&gt; alternative hypothesis: stationary\n\n\nSame result. Difference once more and re-apply the test.\n\ntseries::adf.test(diff(Yt, differences = 2))\n\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  diff(Yt, differences = 2)\n#&gt; Dickey-Fuller = -6, Lag order = 5, p-value = 0.01\n#&gt; alternative hypothesis: stationary\n\n\nNow, when we are using the twice differenced series \\(\\Delta^2 Y_t=(1-B)^2Y_t\\), we can reject the \\(H_0\\) and accept the alternative hypothesis of stationarity. Since the time series has been differenced twice, we state that the integration order \\(d=2\\), or \\(Y_t \\sim\\) I(2).\nWhat are the potential problems? Multiple testing and alternative model specifications. By model, we mean the regression Equation 7.2 that includes the parameter \\(\\phi_1^*\\) that we are testing. Depending on what we know or assume about the process, we may add an intercept or even a parametric trend. In R, it can be done manually or with functions from the package urca:\n\nlibrary(urca)\nADF &lt;- ur.df(Yt, type = \"drift\", selectlags = \"AIC\")\nsummary(ADF)\n\n#&gt; \n#&gt; ############################################### \n#&gt; # Augmented Dickey-Fuller Test Unit Root Test # \n#&gt; ############################################### \n#&gt; \n#&gt; Test regression drift \n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.1896 -0.5737 -0.0834  0.5603  2.9911 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.301046   0.136667    2.20    0.029 *  \n#&gt; z.lag.1     -0.000895   0.000490   -1.83    0.069 .  \n#&gt; z.diff.lag   0.933704   0.024665   37.86   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.93 on 195 degrees of freedom\n#&gt; Multiple R-squared:  0.886,  Adjusted R-squared:  0.884 \n#&gt; F-statistic:  755 on 2 and 195 DF,  p-value: &lt;2e-16\n#&gt; \n#&gt; \n#&gt; Value of test-statistic is: -1.83 2.43 \n#&gt; \n#&gt; Critical values for test statistics: \n#&gt;       1pct  5pct 10pct\n#&gt; tau2 -3.46 -2.88 -2.57\n#&gt; phi1  6.52  4.63  3.81\n\nADF &lt;- ur.df(diff(Yt), type = \"drift\", selectlags = \"AIC\")\nsummary(ADF)\n\n#&gt; \n#&gt; ############################################### \n#&gt; # Augmented Dickey-Fuller Test Unit Root Test # \n#&gt; ############################################### \n#&gt; \n#&gt; Test regression drift \n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.1574 -0.5527 -0.0072  0.5857  2.9193 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept)   0.0812     0.0794    1.02    0.308  \n#&gt; z.lag.1      -0.0536     0.0248   -2.16    0.032 *\n#&gt; z.diff.lag   -0.0362     0.0717   -0.51    0.614  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.935 on 194 degrees of freedom\n#&gt; Multiple R-squared:  0.0274, Adjusted R-squared:  0.0174 \n#&gt; F-statistic: 2.73 on 2 and 194 DF,  p-value: 0.0676\n#&gt; \n#&gt; \n#&gt; Value of test-statistic is: -2.16 2.35 \n#&gt; \n#&gt; Critical values for test statistics: \n#&gt;       1pct  5pct 10pct\n#&gt; tau2 -3.46 -2.88 -2.57\n#&gt; phi1  6.52  4.63  3.81\n\nADF &lt;- ur.df(diff(Yt, differences = 2), type = \"drift\", selectlags = \"AIC\")\nsummary(ADF)\n\n#&gt; \n#&gt; ############################################### \n#&gt; # Augmented Dickey-Fuller Test Unit Root Test # \n#&gt; ############################################### \n#&gt; \n#&gt; Test regression drift \n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.1597 -0.6052 -0.0711  0.6161  3.0784 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  -0.0131     0.0675   -0.19     0.85    \n#&gt; z.lag.1      -1.1534     0.1049  -10.99   &lt;2e-16 ***\n#&gt; z.diff.lag    0.0833     0.0716    1.16     0.25    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.945 on 193 degrees of freedom\n#&gt; Multiple R-squared:  0.534,  Adjusted R-squared:  0.529 \n#&gt; F-statistic:  110 on 2 and 193 DF,  p-value: &lt;2e-16\n#&gt; \n#&gt; \n#&gt; Value of test-statistic is: -11 60.4 \n#&gt; \n#&gt; Critical values for test statistics: \n#&gt;       1pct  5pct 10pct\n#&gt; tau2 -3.46 -2.88 -2.57\n#&gt; phi1  6.52  4.63  3.81\n\n\nFrom the results above, the inclusion of the intercept (\"drift\") in the test model did not affect the conclusion. Now try adding a trend (type = \"trend\" adds the intercept automatically).\n\nADF &lt;- ur.df(Yt, type = \"trend\", selectlags = \"AIC\")\nsummary(ADF)\n\n#&gt; \n#&gt; ############################################### \n#&gt; # Augmented Dickey-Fuller Test Unit Root Test # \n#&gt; ############################################### \n#&gt; \n#&gt; Test regression trend \n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.2098 -0.5435 -0.0667  0.5545  2.9750 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.25599    0.15611    1.64     0.10    \n#&gt; z.lag.1     -0.00162    0.00131   -1.24     0.22    \n#&gt; tt           0.00192    0.00320    0.60     0.55    \n#&gt; z.diff.lag   0.93768    0.02558   36.66   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.932 on 194 degrees of freedom\n#&gt; Multiple R-squared:  0.886,  Adjusted R-squared:  0.884 \n#&gt; F-statistic:  502 on 3 and 194 DF,  p-value: &lt;2e-16\n#&gt; \n#&gt; \n#&gt; Value of test-statistic is: -1.24 1.73 1.85 \n#&gt; \n#&gt; Critical values for test statistics: \n#&gt;       1pct  5pct 10pct\n#&gt; tau3 -3.99 -3.43 -3.13\n#&gt; phi2  6.22  4.75  4.07\n#&gt; phi3  8.43  6.49  5.47\n\nADF &lt;- ur.df(diff(Yt), type = \"trend\", selectlags = \"AIC\")\nsummary(ADF)\n\n#&gt; \n#&gt; ############################################### \n#&gt; # Augmented Dickey-Fuller Test Unit Root Test # \n#&gt; ############################################### \n#&gt; \n#&gt; Test regression trend \n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.1777 -0.5443 -0.0691  0.5697  2.9673 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept)  0.24888    0.15538    1.60    0.111  \n#&gt; z.lag.1     -0.06228    0.02571   -2.42    0.016 *\n#&gt; tt          -0.00153    0.00122   -1.26    0.211  \n#&gt; z.diff.lag  -0.03604    0.07155   -0.50    0.615  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.933 on 193 degrees of freedom\n#&gt; Multiple R-squared:  0.0353, Adjusted R-squared:  0.0203 \n#&gt; F-statistic: 2.35 on 3 and 193 DF,  p-value: 0.0735\n#&gt; \n#&gt; \n#&gt; Value of test-statistic is: -2.42 2.1 3.13 \n#&gt; \n#&gt; Critical values for test statistics: \n#&gt;       1pct  5pct 10pct\n#&gt; tau3 -3.99 -3.43 -3.13\n#&gt; phi2  6.22  4.75  4.07\n#&gt; phi3  8.43  6.49  5.47\n\nADF &lt;- ur.df(diff(Yt, differences = 2), type = \"trend\", selectlags = \"AIC\")\nsummary(ADF)\n\n#&gt; \n#&gt; ############################################### \n#&gt; # Augmented Dickey-Fuller Test Unit Root Test # \n#&gt; ############################################### \n#&gt; \n#&gt; Test regression trend \n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.1801 -0.6461 -0.0636  0.6026  3.1220 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.064035   0.136866    0.47     0.64    \n#&gt; z.lag.1     -1.156959   0.105235  -10.99   &lt;2e-16 ***\n#&gt; tt          -0.000775   0.001196   -0.65     0.52    \n#&gt; z.diff.lag   0.085265   0.071776    1.19     0.24    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.946 on 192 degrees of freedom\n#&gt; Multiple R-squared:  0.535,  Adjusted R-squared:  0.527 \n#&gt; F-statistic: 73.5 on 3 and 192 DF,  p-value: &lt;2e-16\n#&gt; \n#&gt; \n#&gt; Value of test-statistic is: -11 40.3 60.4 \n#&gt; \n#&gt; Critical values for test statistics: \n#&gt;       1pct  5pct 10pct\n#&gt; tau3 -3.99 -3.43 -3.13\n#&gt; phi2  6.22  4.75  4.07\n#&gt; phi3  8.43  6.49  5.47\n\n\nIn this simulated example, misspecification of the testing model did not change our conclusion that \\(X_t \\sim\\) I(2), due to the automatic adjusting of the critical values tau to tau2 (model with intercept) and tau3 (model with trend).\n\n\n\n7.5.1 ADF and PP test problems\nThe ADF and PP tests are asymptotically equivalent but may differ substantially in finite samples due to the different ways in which they correct for serial correlation in the test regression.\nIn general, the ADF and PP tests have very low power against I(0) alternatives that are close to being I(1). That is, unit root tests cannot distinguish highly persistent stationary processes from nonstationary processes very well. Also, the power of unit root tests diminishes as deterministic terms are added to the test regressions. Tests that include a constant and trend in the test regression have less power than tests that only include a constant in the test regression.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Trend Detection</span>"
    ]
  },
  {
    "objectID": "l07_trendtest.html#conclusion",
    "href": "l07_trendtest.html#conclusion",
    "title": "7  Trend Detection",
    "section": "\n7.6 Conclusion",
    "text": "7.6 Conclusion\nTemporal dependence of time series is the most violated assumption of classical tests often employed for detecting trends, including the nonparametric Mann–Kendall test. However, multiple workarounds exist that allow the analyst to relax (avoid) the assumption of independence and provide more reliable inference. We have implemented sieve bootstrapping for time series as one such workaround.\nTest results depend on the specified alternative hypothesis. Remember that non-rejection of the null hypothesis doesn’t make it automatically true.\nWhen testing for unit roots (integration) in time series, the null hypothesis of ADF and PP tests is the unit root, while the alternative is the stationarity of the tested series. Iterative testing and differencing can be used to identify the order of integration \\(d\\).\n\n\n\n\nBickel PJ, Götze F, Zwet WR van (1997) Resampling fewer than \\(n\\) observations: Gains, losses, and remedies for losses. Statistica Sinica 7:1–31\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nBühlmann P (2002) Bootstraps for time series. Statistical Science 17:52–72. https://doi.org/10.1214/ss/1023798998\n\n\nCabilio P, Zhang Y, Chen X (2013) Bootstrap rank tests for trend in time series. Environmetrics 24:537–549. https://doi.org/10.1002/env.2250\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn. John Wiley & Sons, Hoboken, NJ, USA\n\n\nChatterjee S, Simonoff JS (2013) Handbook of regression analysis. John Wiley & Sons, Hoboken, NJ, USA\n\n\nDavison AC, Hinkley DV (1997) Bootstrap methods and their application. Cambridge University Press, Cambridge, UK\n\n\nDickey DA, Fuller WA (1979) Distribution of the estimators for autoregressive time series with a unit root. Journal of the American Statistical Association 74:427–431. https://doi.org/10.2307/2286348\n\n\nEfron B (1979) Bootstrap methods: Another look at the jackknife. The Annals of Statistics 7:1–26. https://doi.org/10.1214/aos/1176344552\n\n\nEsterby SR (1996) Review of methods for the detection and estimation of trends with emphasis on water quality applications. Hydrological Processes 10:127–149. https://doi.org/10.1002/(SICI)1099-1085(199602)10:2&lt;127::AID-HYP354&gt;3.0.CO;2-8\n\n\nHärdle W, Horowitz J, Kreiss J-P (2003) Bootstrap methods for time series. International Statistical Review 71:435–459. https://doi.org/10.1111/j.1751-5823.2003.tb00485.x\n\n\nHirsch RM, Slack JR, Smith RA (1982) Techniques of trend analysis for monthly water quality data. Water Resources Research 18:107–121. https://doi.org/10.1029/WR018i001p00107\n\n\nKendall MG (1975) Rank correlation methods, 4th edn. Charles Griffin, London, UK\n\n\nKreiss J-P, Paparoditis E, Politis DN (2011) On the range of validity of the autoregressive sieve bootstrap. Annals of Statistics 39:2103–2130. https://doi.org/10.1214/11-AOS900\n\n\nLyubchich V (2016) Detecting time series trends and their synchronization in climate data. Intelligence Innovations Investments 12:132–137. https://www.researchgate.net/publication/318283780_Detecting_time_series_trends_and_their_synchronization_in_climate_data\n\n\nLyubchich V, Gel YR, El‐Shaarawi A (2013) On detecting non‐monotonic trends in environmental time series: A fusion of local regression and bootstrap. Environmetrics 24:209–226. https://doi.org/10.1002/env.2212\n\n\nLyubchich V, Wang X, Heyes A, Gel YR (2016) A distribution-free \\(m\\)-out-of-\\(n\\) bootstrap approach to testing symmetry about an unknown median. Computational Statistics & Data Analysis 104:1–9. https://doi.org/10.1016/j.csda.2016.05.004\n\n\nNoguchi K, Gel YR, Duguay CR (2011) Bootstrap-based tests for trends in hydrological time series, with application to ice phenology data. Journal of Hydrology 410:150–161. https://doi.org/10.1016/j.jhydrol.2011.09.008\n\n\nPowell AM, Xu J (2011) Abrupt climate regime shifts, their potential forcing and fisheries impacts. Atmospheric and Climate Sciences 1:33. https://doi.org/10.4236/acs.2011.12004\n\n\nSeidel DJ, Lanzante JR (2004) An assessment of three alternatives to linear trends for characterizing global atmospheric temperature changes. Journal of Geophysical Research: Atmospheres 109: https://doi.org/10.1029/2003JD004414\n\n\nWang L, Akritas MG, Van Keilegom I (2008) An ANOVA-type nonparametric diagnostic test for heteroscedastic regression models. Journal of Nonparametric Statistics 20:365–382. https://doi.org/10.1080/10485250802066112",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Trend Detection</span>"
    ]
  },
  {
    "objectID": "l08_tsreg1.html",
    "href": "l08_tsreg1.html",
    "title": "\n8  Time Series Regression with Trends\n",
    "section": "",
    "text": "8.1 Spurious correlation\nThe goal of this lecture is to introduce methods of handling nonstationary time series in regression models. You will become familiar with the problem of spurious correlation (regression) and approaches helping to avoid it.\nObjectives\nReading materials\nAudio overview\nResults of statistical analysis (correlation, regression, etc.) are called spurious when they are likely driven not by the underlying mechanisms such as the physical relationships between variables, but by matching patterns (often, temporal patterns) of the variables leading to the statistical significance of tested relationships. Such matching patterns include trends, periodic fluctuations (seasonality), and more random patterns like spikes matching in several time series, for example, detected by searching over a large database of time series (a.k.a. cherry picking).\nCodeset.seed(123)\nT &lt;- 300\nXt &lt;- ts(rnorm(T))\nYt &lt;- ts(rnorm(T))\np1 &lt;- forecast::autoplot(Xt)\np2 &lt;- forecast::autoplot(Yt)\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.1: Original stationary and independent time series.\nFigure 8.1 shows two series of length \\(T = 300\\) simulated from the standard normal distribution, \\(N(0,1)\\). These are independent and identically distributed (i.i.d.) random variables: each of the \\(T\\) values in \\(X_t\\) and \\(Y_t\\) was drawn independently from other values from the same distribution. Two random variables are independent if the realization of one does not affect the probability distribution of the other. Independence is a strong condition, it also implies (includes) that the values are not correlated. This is true both for the values within the series \\(X_t\\) and \\(Y_t\\), and across \\(X_t\\) and \\(Y_t\\) (i.e., \\(X_t\\) and \\(Y_t\\) are not autocorrelated, nor correlated with each other). This is the asymptotic property of such a time series (as the sample size increases infinitely).\nIn finite samples, we may observe that a point estimate of the correlation coefficient even for such an ideal series is not exactly zero, but it will be usually not statistically significant. See the results (confidence interval and \\(p\\)-value) from the correlation \\(t\\)-test below:\ncor.test(Xt, Yt)\n\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  Xt and Yt\n#&gt; t = -1, df = 298, p-value = 0.3\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.1727  0.0529\n#&gt; sample estimates:\n#&gt;     cor \n#&gt; -0.0607\nNot many time series behave like that in real life – often we observe some trends. Let’s add linear trends to our simulated data, for example, trends going in the same direction but with slopes of different magnitudes. Here we use linear increasing trends, i.e., with positive slopes Figure 8.2.\nCodeXt &lt;- Xt + c(1:T)/95\nYt &lt;- Yt + c(1:T)/50\np1 &lt;- forecast::autoplot(Xt) +\n    ylim(-2, 8)\np2 &lt;- forecast::autoplot(Yt) +\n    ylim(-2, 8)\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.2: Time series with trends.\nAfter adding trends to each of the time series, the correlation of \\(X_t\\) and \\(Y_t\\) is strong and statistically significant, but this is not necessarily because these series are so strongly related to each other.\ncor.test(Xt, Yt)\n\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  Xt and Yt\n#&gt; t = 13, df = 298, p-value &lt;2e-16\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.519 0.665\n#&gt; sample estimates:\n#&gt;   cor \n#&gt; 0.597\nSome other factors may be driving the dynamics (trends) of \\(X_t\\) and \\(Y_t\\) in the same direction. Also, recall the general formula for computing correlation (or autocorrelation) of time series and that we need to subtract time-dependent means (not just a mean calculated over the whole period assuming the mean is not changing or time-independent), for example, as in Equation 2.3.\nFor example, a city’s growing population may result in more police officers and heavier pollution at the same time, partially because more people will drive their vehicles in the city. An attempt to correlate (regress) pollution with the number of police officers will produce a so-called spurious correlation (regression). The results of statistical tests will be likely statistically significant. However, is pollution directly related to the number of police officers? Will the dismissal of a police officer help to make the air cleaner?\nNot only common increasing/decreasing trends but also other systematic changes (such as seasonality) may be responsible for spurious correlation effects. For example, both high ice cream sales and harmful algal blooms typically occur in warm weather conditions and may be ‘significantly’ correlated, suggesting banning ice cream for the sake of a safer environment. See more interesting examples of spurious correlation at http://www.tylervigen.com/spurious-correlations.\nSometimes, some simple tricks may help to avoid spurious results. For example, analyze not the raw numbers, but the rates that remove the effect of population growth in a city: crime rate per 100,000 inhabitants, number of people older than 70 per 1,000 inhabitants, etc. For more general approaches, see the next section.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Time Series Regression with Trends</span>"
    ]
  },
  {
    "objectID": "l08_tsreg1.html#spurious-correlation",
    "href": "l08_tsreg1.html#spurious-correlation",
    "title": "\n8  Time Series Regression with Trends\n",
    "section": "",
    "text": "Note\n\n\n\nIt is probably a good idea to refrain from writing ‘positive trends’ (or ‘negative trends’) because they can be confused with ‘good’ or ‘beneficial’ trends. For example, a decrease in the unemployment rate is a positive (good) trend for a country, but it is a trend with a negative slope. Contrary, a linear trend in pollutant concentrations with a positive slope (going upward) shows a negative (worsening) tendency for the ecosystem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Time Series Regression with Trends</span>"
    ]
  },
  {
    "objectID": "l08_tsreg1.html#common-approaches-to-regressing-time-series-with-trends",
    "href": "l08_tsreg1.html#common-approaches-to-regressing-time-series-with-trends",
    "title": "\n8  Time Series Regression with Trends\n",
    "section": "\n8.2 Common approaches to regressing time series with trends",
    "text": "8.2 Common approaches to regressing time series with trends\nConsider a situation when we are given time series with trends (like in Figure 8.2), we do not know the data generating process (DGP; i.e., the true dependence structure), and we want to use these series in regression analysis.\nIn general, there are three alternative ways of dealing with trends in regression:\n\nIncorporate time effect into the model;\nUse deviations from trends (i.e., model and remove trends), or\nUse differenced series (i.e., remove trends by differencing).\n\nAfter these three approaches, here we consider a special case of cointegration (Section 8.3).\n\n8.2.1 Incorporate time effects\nBased on the time series plots (Figure 8.2), a linear time trend would fit these data, since we see a linear increase of values with time (so we add a linear time effect \\(t\\) in our model). Alternatively, e.g., if we observed parabolic structure, we could include \\(t+t^2\\) or another form of trend.\n\nt &lt;- c(1:T)\nmod_time &lt;- lm(Yt ~ Xt + t)\nsummary(mod_time)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Yt ~ Xt + t)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.7564 -0.6069  0.0546  0.6404  2.5802 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.000813   0.114665    0.01     0.99    \n#&gt; Xt          -0.063741   0.060617   -1.05     0.29    \n#&gt; t            0.020741   0.000939   22.09   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.99 on 297 degrees of freedom\n#&gt; Multiple R-squared:  0.756,  Adjusted R-squared:  0.755 \n#&gt; F-statistic:  461 on 2 and 297 DF,  p-value: &lt;2e-16\n\n\nThis model looks like this: \\[\nY_t = \\beta_0 + \\beta_1X_t + \\beta_2t + \\epsilon_t,\n\\] estimated as: \\[\\begin{align}\n\\widehat{Y}_t &= \\hat{\\beta}_0 + \\hat{\\beta}_1X_t + \\hat{\\beta}_2t,\\\\\n\\widehat{Y}_t &=  0.0008 - 0.0637X_t + 0.0207t.\n\\end{align}\\]\nIn the above model, the (highly statistically significant) time term took over the trend influence, thus, the coefficient for \\(X\\) shows the ‘real’ relationship between \\(Y\\) and \\(X\\). Notice, the coefficient \\(\\beta_1\\) is not statistically significant, what we expected.\n\n8.2.2 Use deviations from trends\nHere we fit a separate time trend (may be of a different form for each time series: linear, quadratic, log, etc.) for each variable and find deviations from these trends. Based on Figure 8.2, linear trends are appropriate here: \\[\nY_t = a_0 + a_1t + e_{(Y)t}; \\quad X_t = b_0 + b_1t + e_{(X)t},\n\\] where \\(e_{(Y)t}\\) and \\(e_{(X)t}\\) are the trend residuals for the series \\(Y_t\\) and \\(X_t\\), respectively.\nAfter the trend coefficients \\(a_0\\), \\(a_1\\), \\(b_0\\), and \\(b_1\\) are estimated,\n\nMY &lt;- lm(Yt ~ t)\nMX &lt;- lm(Xt ~ t)\n\nthe smoothed series are \\[\n\\widetilde{Y}_t = \\hat{a}_0 + \\hat{a}_1t; \\quad \\widetilde{X}_t = \\hat{b}_0 + \\hat{b}_1t\n\\] and the estimated trend residuals are (Figure 8.3) \\[\n\\hat{e}_{(Y)t} = Y_t - \\widetilde{Y}_t\\quad\\text{and}\\quad \\hat{e}_{(X)t} = X_t - \\widetilde{X}_t.\n\\]\n\nCodep1 &lt;- ggplot2::autoplot(as.ts(MY$resid)) +\n    xlab(\"t\") +\n    ylab(\"eYt\")\np2 &lt;- ggplot2::autoplot(as.ts(MX$resid)) +\n    xlab(\"t\") +\n    ylab(\"eXt\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.3: Residuals from individually estimated linear trends.\n\n\n\n\nUse the residuals in our regression model in place of the original variables: \\[\n\\hat{e}_{(Y)t} = \\beta_0 + \\beta_1\\hat{e}_{(X)t} + \\epsilon_t.\n\\]\n\nmod_devTrend &lt;- lm(MY$residuals ~ MX$residuals)\nsummary(mod_devTrend)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = MY$residuals ~ MX$residuals)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.7564 -0.6069  0.0546  0.6404  2.5802 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)   2.05e-17   5.71e-02    0.00     1.00\n#&gt; MX$residuals -6.37e-02   6.05e-02   -1.05     0.29\n#&gt; \n#&gt; Residual standard error: 0.989 on 298 degrees of freedom\n#&gt; Multiple R-squared:  0.00371,    Adjusted R-squared:  0.000366 \n#&gt; F-statistic: 1.11 on 1 and 298 DF,  p-value: 0.293\n\n\nWe got the result (the relationship between the variables is not statistically significant) similar to running the model incorporating time trends in Section 8.2.1. Again, it is a reasonable result based on how the time series were simulated (independent, although with trends in the same direction).\n\n8.2.3 Use differenced series\nInstead of assuming a deterministic trend as in the previous subsections, we can try to eliminate a stochastic trend by differencing the time series. We define the lag-1 difference operator \\(\\Delta\\) by \\[\n\\Delta X_t = X_t - X_{t-1} = (1-B)X_t,\n\\] where \\(B\\) is the backward shift operator, \\(BX_t = X_{t-1}\\).\nThere are tests developed in econometrics to find the appropriate order of differences (unit-root tests). Here, however, we will use the rule of thumb: for time trends looking linear (our case, see Figure 8.2) use the first-order differences, for parabolic shapes – the second-order differences. After differencing, the series should look stationary.\nThe first-order differences for our series (Figure 8.4) can be calculated as follows:\n\nD1X &lt;- diff(Xt)\nD1Y &lt;- diff(Yt)\n\n\nCodep1 &lt;- ggplot2::autoplot(D1Y) +\n    xlab(\"t\")\np2 &lt;- ggplot2::autoplot(D1X) +\n    xlab(\"t\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.4: First-order differences of the time series.\n\n\n\n\nThe series of first-order differences look stationary (Figure 8.4). Use these differenced series instead of the original time series in a regression model: \\[\n\\Delta Y_t = \\beta_0 + \\beta_1 \\Delta X_t + \\epsilon_t.\n\\]\n\nmod_diff &lt;- lm(D1Y ~ D1X)\nsummary(mod_diff)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = D1Y ~ D1X)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -4.056 -1.050  0.178  1.005  5.028 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)   0.0284     0.0840    0.34     0.74\n#&gt; D1X           0.0114     0.0620    0.18     0.85\n#&gt; \n#&gt; Residual standard error: 1.45 on 297 degrees of freedom\n#&gt; Multiple R-squared:  0.000114,   Adjusted R-squared:  -0.00325 \n#&gt; F-statistic: 0.0338 on 1 and 297 DF,  p-value: 0.854\n\n\nAs expected (since the original series \\(X_t\\) and \\(Y_t\\), before adding the trend, were uncorrelated), the coefficient \\(\\beta_1\\) and the whole regression model are not statistically significant.\n\n8.2.4 Wrong approach (do not repeat at home) leading to spurious regression\nWhat if we forget about the three approaches above and just use the time series with trends in a regression model? This could be such a model: \\[\nY_t = \\beta_0 + \\beta_1X_t + \\epsilon_t.\n\\]\n\nbadModel &lt;- lm(Yt ~ Xt)\nsummary(badModel)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Yt ~ Xt)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -4.022 -1.020  0.098  1.219  4.091 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.5812     0.1454    10.9   &lt;2e-16 ***\n#&gt; Xt            0.8883     0.0692    12.8   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.61 on 298 degrees of freedom\n#&gt; Multiple R-squared:  0.356,  Adjusted R-squared:  0.354 \n#&gt; F-statistic:  165 on 1 and 298 DF,  p-value: &lt;2e-16\n\n\nThe bad model shows spurious statistically significant effects, which are not true.\nBeware of trends!\n\n\n\n\n\n\nNoteExample: Predicting sales of home appliances\n\n\n\nRecall the dishwasher example from the first lecture. Let’s use the above methods for regressing the time series of dishwasher shipments (\\(DISH_t\\)) and residential investments (\\(RES_t\\)).\nFirst, look at the time series plots of the raw data (Figure 8.5).\n\nCodeD &lt;- read.delim(\"data/dish.txt\") %&gt;% \n    rename(Year = YEAR)\np1 &lt;- ggplot(D, aes(x = Year, y = DISH)) + \n    geom_line()\np2 &lt;- ggplot(D, aes(x = Year, y = RES)) + \n    geom_line()\np1 + p2 + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.5: Time series plots of the dishwasher shipments (DISH) and residential investments (RES).\n\n\n\n\nIncorporate time effects\nIncorporate linear trend using the model \\[\nDISH_t = \\beta_0 + \\beta_1 RES_t + \\beta_2 Year + \\epsilon_t;\n\\tag{8.1}\\] see the residual diagnostics below and in Figure 8.6.\n\nM_time &lt;- lm(DISH ~ RES + Year, data = D)\nsummary(M_time)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = DISH ~ RES + Year, data = D)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -508.6 -237.7  -52.1  236.1  859.9 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -1.32e+05   2.32e+04   -5.71  8.1e-06 ***\n#&gt; RES          5.89e+01   9.35e+00    6.29  2.0e-06 ***\n#&gt; Year         6.69e+01   1.19e+01    5.63  1.0e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 353 on 23 degrees of freedom\n#&gt; Multiple R-squared:  0.893,  Adjusted R-squared:  0.884 \n#&gt; F-statistic: 96.3 on 2 and 23 DF,  p-value: 6.65e-12\n\n\n\nshapiro.test(M_time$residuals)\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  M_time$residuals\n#&gt; W = 1, p-value = 0.4\n\nlawstat::runs.test(M_time$residuals, plot.it = FALSE)\n\n#&gt; \n#&gt;  Runs Test - Two sided\n#&gt; \n#&gt; data:  M_time$residuals\n#&gt; Standardized Runs Statistic = -4, p-value = 3e-04\n\n\n\nCodep1 &lt;- ggplot(D, aes(x = Year, y = M_time$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ylab(\"Residuals\")\np2 &lt;- forecast::ggAcf(M_time$residuals) +\n    ggtitle(\"\") +\n    xlab(\"Lag (years)\")\np3 &lt;- ggpubr::ggqqplot(M_time$residuals) + \n    xlab(\"Standard normal quantiles\")\np1 + p2 + p3 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.6: Diagnostics plots for residuals from Equation 8.1.\n\n\n\n\nUse differenced time series\nDifference the time series (Figure 8.7) to use in the model \\[\n\\Delta DISH_t = \\beta_0 + \\beta_1 \\Delta RES_t + \\epsilon_t\n\\tag{8.2}\\] see the residual diagnostics below and in Figure 8.8.\n\nD_DISH &lt;- diff(D$DISH)\nD_RES &lt;- diff(D$RES)\nM_diff &lt;- lm(D_DISH ~ D_RES)\nsummary(M_diff)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = D_DISH ~ D_RES)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -424.4 -121.3    4.4   72.3  498.0 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)    71.22      43.16    1.65     0.11    \n#&gt; D_RES          44.14       6.07    7.27  2.1e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 213 on 23 degrees of freedom\n#&gt; Multiple R-squared:  0.697,  Adjusted R-squared:  0.684 \n#&gt; F-statistic: 52.8 on 1 and 23 DF,  p-value: 2.13e-07\n\n\n\nCodep1 &lt;- ggplot(D[-1,], aes(x = Year, y = D_DISH)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4)\np2 &lt;- ggplot(D[-1,], aes(x = Year, y = D_RES)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4)\np3 &lt;- ggplot(data.frame(D_DISH, D_RES), aes(y = D_DISH, x = D_RES)) +\n    geom_point()\np1 + p2 + p3 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.7: Time differences of dishwasher shipments and residential investments, and a scatterplot for assessing pairwise relationships.\n\n\n\n\n\nshapiro.test(M_diff$residuals)\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  M_diff$residuals\n#&gt; W = 1, p-value = 0.8\n\nlawstat::runs.test(M_diff$residuals, plot.it = FALSE)\n\n#&gt; \n#&gt;  Runs Test - Two sided\n#&gt; \n#&gt; data:  M_diff$residuals\n#&gt; Standardized Runs Statistic = -2, p-value = 0.07\n\n\n\nCodep1 &lt;- ggplot(D[-1,], aes(x = Year, y = M_diff$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ylab(\"Residuals\")\np2 &lt;- forecast::ggAcf(M_diff$residuals) +\n    ggtitle(\"\") +\n    xlab(\"Lag (years)\")\np3 &lt;- ggpubr::ggqqplot(M_diff$residuals) + \n    xlab(\"Standard normal quantiles\")\np1 + p2 + p3 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.8: Diagnostics plots for residuals from Equation 8.2.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Time Series Regression with Trends</span>"
    ]
  },
  {
    "objectID": "l08_tsreg1.html#sec-cointegration",
    "href": "l08_tsreg1.html#sec-cointegration",
    "title": "\n8  Time Series Regression with Trends\n",
    "section": "\n8.3 Cointegration",
    "text": "8.3 Cointegration\nGenerally, cointegration might be characterized by two or more I(1) variables indicating a common long-run development, i.e., the variables do not drift away from each other except for transitory fluctuations. This defines a statistical equilibrium that, in empirical analysis, can often be interpreted as a long-run [economic] relation (Engle and Granger 1987).\nIn other words, two I(1) series \\(X_t\\) and \\(Y_t\\) are cointegrated if their linear combination \\(u_t\\) is I(0): \\[\nY_t - \\beta X_t = u_t.\n\\tag{8.3}\\]\nCointegration means a common stochastic trend (see Appendix C on testing for a common parametric trend). The vector \\((1, -\\beta)^{\\top}\\) is called the cointegration vector.\nFor the development of methods of analyzing time series cointegration, in 2003, Clive W. J. Granger was awarded 1/2 of the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel (the other half was awarded to R. Engle, see Chapter 6).\n\n8.3.1 Two-step Engle–Granger method\n\nEstimate long-run relationship, i.e., regression in levels as in Equation 8.3, and test residuals for I(0).\nIf the residual series \\(u_t\\) is I(0), use it in error correction model (ECM) regression \\[\n\\begin{split}\n\\Delta Y_t &= a_0 -\\gamma_Y(Y_{t-1}-\\beta X_{t-1})+\\sum_{j=1}^{n_X}a_{Xj}\\Delta X_{t-j}+\\sum_{j=1}^{n_Y}a_{Yj}\\Delta Y_{t-j} + u_{Y,t},\\\\\n\\Delta X_t &= b_0 +\\gamma_X(Y_{t-1}-\\beta X_{t-1})+\\sum_{j=1}^{k_X}b_{Xj}\\Delta X_{t-j}+\\sum_{j=1}^{k_Y}b_{Yj}\\Delta Y_{t-j} + u_{X,t},\n\\end{split}\n\\tag{8.4}\\] where \\(u_X\\) and \\(u_Y\\) are pure random processes. If \\(X_t\\) and \\(Y_t\\) are cointegrated, at least one \\(\\gamma_i\\), \\(i = X, Y\\), has to be different from zero.\n\nOLS estimator is super consistent, convergence \\(T\\). However, OLS can be biased in small samples.\nThe representation in Equation 8.4 has the advantage that it only contains stationary variables, although the underlying relation is between nonstationary (I(1)) variables. Thus, if the variables are cointegrated and the cointegration vector is known, the traditional statistical procedures can be applied for estimation and testing.\n\n\n\n\n\n\nNoteExample: Error correction model for simulated data\n\n\n\nDemonstrate the analysis using simulated time series (Figure 8.9).\n\nCodep1 &lt;- ggplot2::autoplot(Xt)\np2 &lt;- ggplot2::autoplot(Yt)\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 8.9: Simulated I(1) time series.\n\n\n\n\nApply unit-root test to check the integration order of each series, using the R package tseries:\n\ntseries::adf.test(Xt)\n\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  Xt\n#&gt; Dickey-Fuller = -2, Lag order = 6, p-value = 0.5\n#&gt; alternative hypothesis: stationary\n\ntseries::adf.test(diff(Xt))\n\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  diff(Xt)\n#&gt; Dickey-Fuller = -7, Lag order = 6, p-value = 0.01\n#&gt; alternative hypothesis: stationary\n\ntseries::adf.test(Yt)\n\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  Yt\n#&gt; Dickey-Fuller = -2, Lag order = 6, p-value = 0.5\n#&gt; alternative hypothesis: stationary\n\ntseries::adf.test(diff(Yt))\n\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  diff(Yt)\n#&gt; Dickey-Fuller = -8, Lag order = 6, p-value = 0.01\n#&gt; alternative hypothesis: stationary\n\n\nWith the confidence of 95%, the ADF test results show that each of the time series, \\(X_t\\) and \\(Y_t\\), are I(1). (However, we have used the test 4 times, without controlling the overall Type I error.)\nFit the linear regression \\[\nY_t = a + bX_t + u_t.\n\\tag{8.5}\\] The vector \\([1, -b]\\) is the cointegration vector.\n\nUt &lt;- lm(Yt ~ Xt)$residuals\ntseries::adf.test(Ut)\n\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  Ut\n#&gt; Dickey-Fuller = -5, Lag order = 6, p-value = 0.01\n#&gt; alternative hypothesis: stationary\n\n\nWhile each of the time series \\(X_t\\) and \\(Y_t\\) is I(1), the resulting residual series \\(u_t \\sim \\mathrm{I}(0)\\), thus we conclude, \\(X_t\\) and \\(Y_t\\) are cointegrated.\nApply a simple error correction model (with \\(n_X = n_Y = 1\\)), using the R package dynlm or just specify lags using the package dplyr:\n\n# Error correction term\nect &lt;- Ut[-length(Ut)]\n\n# Differenced series\ndy &lt;- diff(Yt)\ndx &lt;- diff(Xt)\n\nModel using dynlm::dynlm():\n\nlibrary(dynlm)\necmdat1 &lt;- cbind(dy, dx, ect)\necm1 &lt;- dynlm(dy ~ L(ect, 1) + L(dy, 1) + L(dx, 1), data = ecmdat1)\nsummary(ecm1)\n\n#&gt; \n#&gt; Time series regression with \"ts\" data:\n#&gt; Start = 3, End = 250\n#&gt; \n#&gt; Call:\n#&gt; dynlm(formula = dy ~ L(ect, 1) + L(dy, 1) + L(dx, 1), data = ecmdat1)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.8348 -0.3860 -0.0224  0.3696  1.5586 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.00919    0.03588    0.26   0.7980    \n#&gt; L(ect, 1)   -0.67417    0.07482   -9.01   &lt;2e-16 ***\n#&gt; L(dy, 1)    -0.49772    0.07341   -6.78    9e-11 ***\n#&gt; L(dx, 1)     0.22842    0.07906    2.89   0.0042 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.565 on 244 degrees of freedom\n#&gt; Multiple R-squared:  0.265,  Adjusted R-squared:  0.256 \n#&gt; F-statistic: 29.3 on 3 and 244 DF,  p-value: 3.2e-16\n\n\nModel using lm() and dplyr::lag():\n\necm2 &lt;- lm(dy ~ dplyr::lag(ect, 1) + \n               dplyr::lag(as.vector(dy), 1) + \n               dplyr::lag(as.vector(dx), 1))\nsummary(ecm2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = dy ~ dplyr::lag(ect, 1) + dplyr::lag(as.vector(dy), \n#&gt;     1) + dplyr::lag(as.vector(dx), 1))\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.8348 -0.3860 -0.0224  0.3696  1.5586 \n#&gt; \n#&gt; Coefficients:\n#&gt;                              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                   0.00919    0.03588    0.26   0.7980    \n#&gt; dplyr::lag(ect, 1)           -0.67417    0.07482   -9.01   &lt;2e-16 ***\n#&gt; dplyr::lag(as.vector(dy), 1) -0.49772    0.07341   -6.78    9e-11 ***\n#&gt; dplyr::lag(as.vector(dx), 1)  0.22842    0.07906    2.89   0.0042 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.565 on 244 degrees of freedom\n#&gt;   (1 observation deleted due to missingness)\n#&gt; Multiple R-squared:  0.265,  Adjusted R-squared:  0.256 \n#&gt; F-statistic: 29.3 on 3 and 244 DF,  p-value: 3.2e-16\n\n\nThere is also the R package ecm, but it uses a modified formulation of the model, see details for the function ecm::ecm().\n\n\nIn the example above, the time series were simulated as cointegrated. Below is an example of another I(1) process \\(W_t\\) but with a stochastic trend different from that of \\(X_t\\). In this case, the linear combination of individually integrated \\(W_t\\) and \\(X_t\\) does not produce a stationary time series, thus, \\(W_t\\) and \\(X_t\\) are not cointegrated.\n\nU2 &lt;- lm(Wt ~ Xt)$residuals\ntseries::adf.test(U2)\n\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  U2\n#&gt; Dickey-Fuller = -2, Lag order = 6, p-value = 0.7\n#&gt; alternative hypothesis: stationary\n\n\n\n8.3.2 Johansen test\nThe Johansen test allows for more than one cointegrating relationship. The null hypothesis for the trace test is that the number of cointegration vectors is \\(r&lt;k\\), vs. the alternative that \\(r=k\\). The testing proceeds sequentially for \\(k=1,2,\\dots\\); and the first non-rejection of the null hypothesis is taken as an estimate of \\(r\\).\nUsing the R package urca:\n\nlibrary(urca)\nvecm &lt;- ca.jo(cbind(Yt, Xt, Wt))\nsummary(vecm)\n\n#&gt; \n#&gt; ###################### \n#&gt; # Johansen-Procedure # \n#&gt; ###################### \n#&gt; \n#&gt; Test type: maximal eigenvalue statistic (lambda max) , with linear trend \n#&gt; \n#&gt; Eigenvalues (lambda):\n#&gt; [1] 0.270915 0.025774 0.000838\n#&gt; \n#&gt; Values of teststatistic and critical values of test:\n#&gt; \n#&gt;           test 10pct  5pct 1pct\n#&gt; r &lt;= 2 |  0.21   6.5  8.18 11.6\n#&gt; r &lt;= 1 |  6.48  12.9 14.90 19.2\n#&gt; r = 0  | 78.36  18.9 21.07 25.8\n#&gt; \n#&gt; Eigenvectors, normalised to first column:\n#&gt; (These are the cointegration relations)\n#&gt; \n#&gt;          Yt.l2  Xt.l2 Wt.l2\n#&gt; Yt.l2  1.00000  1.000   1.0\n#&gt; Xt.l2 -0.49209 -5.234 -26.2\n#&gt; Wt.l2 -0.00497  0.383 -19.3\n#&gt; \n#&gt; Weights W:\n#&gt; (This is the loading matrix)\n#&gt; \n#&gt;       Yt.l2   Xt.l2     Wt.l2\n#&gt; Yt.d -0.686 0.00228  2.64e-06\n#&gt; Xt.d -0.179 0.00796 -1.54e-05\n#&gt; Wt.d  0.180 0.00253  1.59e-04\n\n\nIf two time series are cointegrated, then the usual regression in Equation 8.5 is the so-called long-run equilibrium relation or attractor, i.e., the relationship between \\(X_t\\) and \\(Y_t\\) can be explained by Equation 8.5 in a long run. Equation 8.5 is applied for estimation, not for testing (see Figure 6.1 in Kirchgässner and Wolters 2007 on highly dispersed \\(t\\)-statistic). The error correction model in Equation 8.4 should be estimated for testing (\\(p\\)-values from the ECM can be used for testing, also see Chapter 6 in Kirchgässner and Wolters 2007).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Time Series Regression with Trends</span>"
    ]
  },
  {
    "objectID": "l08_tsreg1.html#conclusion",
    "href": "l08_tsreg1.html#conclusion",
    "title": "\n8  Time Series Regression with Trends\n",
    "section": "\n8.4 Conclusion",
    "text": "8.4 Conclusion\nNow we can incorporate trend effects into our models, using the three considered approaches or by testing for cointegration and applying an error correction model. The next step would be to incorporate autocorrelation structure in the residuals (the simulated example considered here used independent normally distributed noise, so it was an artificial ideal case of no autocorrelation, whereas we usually encounter autocorrelations, e.g., see residual diagnostics in the examples in Section 8.2.1).\n\n\n\n\nEngle RF, Granger CWJ (1987) Co-integration and error correction: Representation, estimation, and testing. Econometrica 55:251–276. https://doi.org/10.2307/1913236\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern time series analysis. Springer-Verlag, Berlin, Germany\n\n\nWooldridge JM (2013) Introductory econometrics: A modern approach, 5th edn. Cengage Learning, Mason, OH, USA",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Time Series Regression with Trends</span>"
    ]
  },
  {
    "objectID": "l09_tsreg2.html",
    "href": "l09_tsreg2.html",
    "title": "9  Time Series Regression with Correlated Errors",
    "section": "",
    "text": "9.1 Introduction\nThe goal of this lecture is to bring together knowledge about regression, time series trends, and autocorrelation with the goal of obtaining correct inference in regression problems involving time series. You should become capable of suggesting a model that would meet the goal of data analysis (testing certain relationships) while satisfying assumptions (e.g., uncorrelatedness of residuals) and minimizing the risk of spurious results.\nObjectives\nReading materials\nAudio overview\nHere we explore a regression model with autocorrelated residuals. We already know how to\nHere we bring these two skills together.\nIf the residuals do not satisfy the independence assumption, we can sometimes describe them in terms of a specific correlation model involving one or more new parameters and some ‘new residuals.’ These new residuals essentially satisfy the OLS assumptions and can replace the original correlated residuals (which we will sometimes call the ‘old’ residuals) in the model.\nWhen modeling time series, we often want to find so-called leading indicators, i.e., exogenous variables whose lagged values can be used for predicting the response, so we do not need to predict the \\(X\\)-variables. Hence, our models may include lagged versions of both the response and predictor variables (a general form of such a model was given in Equation 2.2).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time Series Regression with Correlated Errors</span>"
    ]
  },
  {
    "objectID": "l09_tsreg2.html#introduction",
    "href": "l09_tsreg2.html#introduction",
    "title": "9  Time Series Regression with Correlated Errors",
    "section": "",
    "text": "forecast, construct prediction intervals, and test hypotheses about regression coefficients when the residuals satisfy the ordinary least squares (OLS) assumptions, i.e., the residuals are white noise and have a joint normal distribution \\(N(0, \\sigma^{2})\\);\nmodel serial dependence (i.e., autocorrelation) in stationary univariate time series, such as using ARMA models.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time Series Regression with Correlated Errors</span>"
    ]
  },
  {
    "objectID": "l09_tsreg2.html#sec-ccf",
    "href": "l09_tsreg2.html#sec-ccf",
    "title": "9  Time Series Regression with Correlated Errors",
    "section": "\n9.2 Cross-correlation function",
    "text": "9.2 Cross-correlation function\nCross-correlation is a correlation between different time series, typically with a time offset \\(k\\) (a.k.a. lag): \\[\n\\mathrm{cor}(X_{t+k},Y_t).\n\\tag{9.1}\\] (Recall that autocorrelation is also a correlation with a lag present, but for the same time series.) By calculating cross-correlations at each lag \\(k = 0, \\pm 1, \\pm 2, \\dots\\), we obtain the cross-correlation function (CCF). CCF shows how the correlation coefficient varies with the lag \\(k\\) and helps us to identify important lags and find leading indicators for predictive models. For example, the relationship between nutrient input and algal blooms in a marine environment is not immediate since the algae need time to reproduce. The correlation between a time series of births and demand for baby formula would be distributed over smaller lags \\(l\\) \\[\n\\mathrm{cor}(\\mathrm{Births}_{t - l}, \\mathrm{FormulaDemand}_t)\n\\] than the correlation lags \\(L\\) between births and school enrollments \\[\n\\mathrm{cor}(\\mathrm{Births}_{t - L}, \\mathrm{SchoolEnrollment}_t),\n\\] where \\(l,L &gt; 0\\) and \\(l &lt; L\\).\nThe functions ccf() and forecast::ggCcf() estimate the cross-correlation function and plot it as a base-R or a ggplot2 graph, respectively. The functions use the notations like in Equation 9.1, hence when using ccf(x, y) we are typically interested in identifying negative \\(k\\)s that correspond to correlation between past values of the predictor \\(X_t\\) and current values of the response \\(Y_t\\).\nFor example, Figure 9.1 shows the CCF for two time series of sales that have been detrended by taking differences. From this plot, lagged values of the first series, \\(\\Delta \\mathrm{BJsales{.}lead}_{t - 2}\\) and \\(\\Delta \\mathrm{BJsales{.}lead}_{t - 3}\\) are significantly correlated with current values of the second series, \\(\\Delta \\mathrm{BJsales}_{t}\\). Hence, we can call BJsales{.}lead a leading indicator for BJsales.\n\nCodeforecast::ggCcf(diff(BJsales.lead), diff(BJsales))\n\n\n\n\n\n\nFigure 9.1: Cross-correlation function of the detrended time series.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function ccf() assumes that the two time series are evenly spaced and have no missing data. We can use the argument na.action = na.pass to ignore any missing values.\n\n\nNote that the 95% confidence band shown in Figure 9.1 is based on how i.i.d. series of length \\(n\\) would correlate (the same way the band is produced for an autocorrelation function with acf()). However, when \\(n\\) is small, there are not so many pairs \\((X_{t+k}, Y_t)\\) to calculate the correlations at large lags, hence the confidence bands should be adjusted by using the smaller \\(n'(k) = n - |k|\\) to account for this lack of confidence.\nAnother concern is the autocorrelation of individual time series that may lead to spurious cross-correlation (Dean and Dunsmuir 2016). Intuitively, say if \\(X_t\\) and \\(Y_t\\) are independent from each other but both \\(X_t\\) and \\(Y_t\\) are positively autocorrelated – high (or low) values in \\(X_t\\) follow each other, and high (or low) values in \\(Y_t\\) follow each other – there could be a time alignment among our considered lags \\(k = 0, \\pm 1, \\pm 2, \\dots\\) when the ups (or downs) in \\(X_{t+k}\\) match those in \\(Y_t\\). This may result in spurious cross-correlation. To address the concern of autocorrelation, we can apply bootstrapping to approximate the distribution of cross-correlation coefficients corresponding not to the i.i.d. series but to stationary time series with the same autocorrelation structure as the input data. The function funtimes::ccf_boot() implements the sieve bootstrap and returns results for both Pearson and Spearman correlations (Figure 9.2).\n\nCodeset.seed(123)\nres &lt;- funtimes::ccf_boot(diff(BJsales.lead), diff(BJsales), \n                          B = 10000, plot = \"none\")\np1 &lt;- res %&gt;% \n    ggplot(aes(x = Lag, y = r_P, xend = Lag, yend = 0)) + \n    geom_ribbon(aes(ymin = lower_P, ymax = upper_P), fill = 4) +\n    geom_point() + \n    geom_segment() + \n    geom_hline(yintercept = 0) + \n    ylab(\"Pearson correlation\")\np2 &lt;- res %&gt;% \n    ggplot(aes(x = Lag, y = r_S, xend = Lag, yend = 0)) + \n    geom_ribbon(aes(ymin = lower_S, ymax = upper_S), fill = 4) +\n    geom_point() + \n    geom_segment() + \n    geom_hline(yintercept = 0) + \n    ylab(\"Spearman correlation\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 9.2: Pearson and Spearman cross-correlation functions of the detrended time series with sieve bootstrap confidence intervals. Note that the function funtimes::ccf_boot() automatically produces a base-R plot that was suppressed here to save the results as the object res and use the package ggplot2 for the plots.\n\n\n\n\nFinally, remember that the reported Pearson correlations in Figure 9.1 and Figure 9.2 A measure the direction and strength of linear relationships, while Spearman correlations in Figure 9.2 B correspond to monotonic relationships. To further investigate whether the lagged relationships are linear, monotonic, or of some other form, we need to plot the lagged scatterplots (Figure 9.3). Also see other functions in the package astsa, such as the function astsa::lag1.plot() for exploring nonlinear autocorrelations.\n\nCodeastsa::lag2.plot(diff(BJsales.lead), diff(BJsales), max.lag = 5)\n\n\n\n\n\n\nFigure 9.3: Lagged scatterplots of the detrended time series with lowess smooths and (linear) Pearson correlations reported in the right corners.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time Series Regression with Correlated Errors</span>"
    ]
  },
  {
    "objectID": "l09_tsreg2.html#linear-regression-with-arma-errors",
    "href": "l09_tsreg2.html#linear-regression-with-arma-errors",
    "title": "9  Time Series Regression with Correlated Errors",
    "section": "\n9.3 Linear regression with ARMA errors",
    "text": "9.3 Linear regression with ARMA errors\nWe begin with the simplest case, a constant mean model, where the residuals are serially correlated and follow an AR(1) model, that is \\[\nY_{t} = \\beta_{0} + \\epsilon_{t},\n\\tag{9.2}\\] where \\(\\epsilon_{t} \\sim\\) AR(1), i.e., \\[\n\\epsilon_{t} = \\phi \\epsilon_{t - 1} + a_{t}.\n\\tag{9.3}\\]\nHere \\(\\phi\\) is a real number satisfying \\(0 &lt; |\\phi| &lt; 1\\); \\(a_{t}\\) is white noise with zero mean and the variance \\(\\nu^{2}\\), i.e., \\(a_{t} \\sim \\mathrm{WN}(0,\\nu^2)\\). We also assume that \\(\\mathrm{cov}(a_{t}, \\epsilon_{s}) = 0\\) for all \\(s &lt; t\\) (i.e., that the residuals \\(\\epsilon_s\\) are not correlated with the future white noise \\(a_t\\)).\nEquation 9.3 allows us to express Equation 9.2 as \\[\nY_{t} = \\beta_{0} + \\phi \\epsilon_{t - 1} + a_{t}.\n\\tag{9.4}\\]\nThe advantage of this representation is that the new residuals \\(a_{t}\\) satisfy the OLS assumptions. In particular, since \\(a_{t}\\) is white noise, \\(a_{t}\\) is homoskedastic and uncorrelated.\nWe shall also assume that \\(a_{t}\\) are normally distributed (for justification of the construction of confidence intervals and prediction intervals). However, even if \\(a_{t}\\) are not normal, \\(a_{t}\\) are uncorrelated, which is a big improvement over the serially correlated \\(\\epsilon_{t}\\).\nOur goal is to remove the \\(\\epsilon_{t}\\) entirely from the constant mean model and replace them with \\(a_{t}\\) acting as new residuals. This can be done as follows. First, write Equation 9.2 for \\(t-1\\) and multiply both sides by \\(\\phi\\), \\[\n\\phi Y_{t -1} = \\phi \\beta_{0} + \\phi \\epsilon_{t - 1}.\n\\tag{9.5}\\]\nTaking the difference (Equation 9.4 minus Equation 9.5) eliminates \\(\\epsilon_{t}\\) from the model: \\[\n\\begin{split}\nY_{t} - \\phi Y_{t - 1} &= \\left( \\beta_{0} + \\phi \\epsilon_{t - 1} + a_{t} \\right) - \\left( \\phi \\beta_{0} + \\phi \\epsilon_{t - 1}  \\right) \\\\\n&= (1 - \\phi) \\beta_{0} + a_{t}.\n\\end{split}\n\\]\nTherefore we can rewrite the constant mean model in Equation 9.2 as \\[\nY_{t} = (1 - \\phi) \\beta_{0} + \\phi Y_{t - 1} + a_{t}.\n\\tag{9.6}\\]\nIn general, for any multiple linear regression \\[\nY_{t} = \\beta_{0} + \\sum^{k}_{j =1} \\beta_{j} X_{t,j} + \\epsilon_{t}, ~~  \\text{where} ~~ \\epsilon_{t} \\sim \\mbox{AR(1)},\n\\tag{9.7}\\] we can perform a similar procedure of eliminating \\(\\epsilon_{t}\\).\nThis elimination procedure leads to the alternate expression \\[\nY_{t} = (1 - \\phi) \\beta_{0} + \\phi Y_{t -1} + \\sum^{k}_{j = 1} \\beta_{j} (X_{t,j} - \\phi X_{t-1, j}) + a_{t},\n\\tag{9.8}\\] where \\(a_{t}\\) is white noise, i.e., homoskedastic with constant (zero) mean and uncorrelated. See Appendix B describing the method of generalized least squares and an example of \\(k=1\\).\nNote that rewriting the model in this way pulls the autocorrelation parameter for the old residuals, \\(\\phi\\), into the regression part of the model. Thus there are now \\(k + 2\\) unknown regression parameters (\\(\\beta_{0}, \\beta_{1}, \\dots, \\beta_{k}\\), and \\(\\phi\\)). The introduction of an additional parameter into the regression part of the model can be regarded as the price to be paid for extracting new residuals \\(a_{t}\\) that satisfy the OLS assumptions.\nNote that the new residuals \\(a_{t}\\) have smaller variance than the \\(\\epsilon_{t}\\). In fact, \\[\n\\begin{split}\n\\sigma^{2} & = \\mbox{var} (\\epsilon_{t} ) = \\mbox{var} (\\phi \\epsilon_{t - 1} + a_{t}) \\\\\n\\\\\n& =  \\phi^{2} \\mbox{var}(\\epsilon_{t - 1}) + \\mbox{var} (a_{t} ) ~~~ \\mbox{since} ~~ \\mbox{cov}(a_{t}, \\epsilon_{t - 1}) = 0\\\\\n\\\\\n& = \\phi^{2}\\sigma^{2}  + \\nu^{2},\n\\end{split}\n\\] leading to the relation \\[\n\\nu^{2} = \\sigma^{2} (1 - \\phi^{2} ).\n\\tag{9.9}\\]\nHowever, comparing Equation 9.6 with Equation 9.6, and Equation 9.8 with Equation 9.7, we see that the rewritten form of the model is not linear in terms of the parameters \\(\\beta_{0}, \\beta_{1}, \\dots, \\beta_{k}\\) and \\(\\phi\\). For example, the intercept term \\((1 - \\phi) \\beta_{0}\\) involves a product of two of the parameters. This nonlinearity makes the OLS, implemented in the R functions lm() and lsfit(), a poor method for obtaining parameter estimates. Instead, we will use the method of maximum likelihood (ML) carried out through such R functions as arima().\nThe function arima() allows us to input the model in its original form, as in Equation 9.7. It then internally rewrites the model to put it in the form of Equation 9.8. (So we do not have to rewrite the model!) It then makes the assumption that the \\(a_t\\) are normal and constructs the multivariate normal likelihood function \\[\nL (Y_{1} , \\dots , Y_{n}; Q ),\n\\] where \\(n\\) is the sample size and \\(Q\\) is the vector of all unknown parameters. In general, for an AR(1) model with \\(k\\) original predictors, we have \\(Q = (\\phi, \\beta_{0}, \\beta_{1}, \\dots, \\beta_{k}, \\nu^{2})\\). Recall that \\(\\nu^{2} = \\mathrm{var}(a_{t}) = \\mathrm{cov}(a_{t} , a_{t})\\).\nThe function arima() then uses the historical data \\(Y_{1}, \\dots, Y_{n}\\) to find the parameter estimates \\[\n\\hat{Q} = \\left( \\hat{\\phi}, \\hat{\\beta}_{0} , \\hat{\\beta}_{1} , \\dots, \\hat{\\beta}_{k} , \\hat{\\nu}^2 \\right),\n\\] which maximize the likelihood \\(L\\). These estimates (and other things, such as the standard errors of the estimates) can be saved to an output object in R. We will use an example to illustrate the use and interpretation of the function arima().\nMoreover, we can extend the regression model in Equation 9.7 with AR(1) errors to a model with a more general form of errors, ARMA, by assuming \\(\\epsilon_t \\sim \\text{ARMA}(p,q)\\) (see Chapter 6.6 in Brockwell and Davis 2002 and https://robjhyndman.com/hyndsight/arimax/): \\[\n\\begin{split}\nY_t &= \\sum^{k}_{j =1} \\beta_{j} X_{t,j} + \\epsilon_{t},\\\\\n\\epsilon_{t} &= \\phi_1 \\epsilon_{t-1} + \\dots + \\phi_p \\epsilon_{t-p} + \\theta_1 a_{t-1} + \\dots + \\theta_q a_{t-q} + a_t.\n\\end{split}\n\\tag{9.10}\\] This model in Equation 9.10 can be specified in R functions arima(), fable::ARIMA(), forecast::Arima(), and forecast::auto.arima().\n\n\n\n\n\n\nNote\n\n\n\nThe R package forecast has been superseded by the new package fable that uses an alternate parameterization of constants, see ?fable::ARIMA.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that the variables \\(Y_t\\) and \\(X_{t,j}\\) (\\(j = 1,\\dots,k\\)) should be detrended prior to the analysis (to avoid spurious regression results, see the previous lecture on dealing with trends in regression). If differencing is chosen as the method of detrending, the orders of differences \\(d\\) and \\(D\\) can be specified directly within the mentioned ARIMA functions (so R will do the differencing for us).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time Series Regression with Correlated Errors</span>"
    ]
  },
  {
    "objectID": "l09_tsreg2.html#arimax",
    "href": "l09_tsreg2.html#arimax",
    "title": "9  Time Series Regression with Correlated Errors",
    "section": "\n9.4 ARIMAX",
    "text": "9.4 ARIMAX\nARIMAX (‘X’ stands for ‘external regressor’) models are closely related to Equation 9.10, but there is an important difference. For simplicity of notation, we can present an ARMAX(\\(p,q\\)) model that is a regular ARMA(\\(p,q\\)) model for \\(Y_t\\) plus the external regressors: \\[\n\\begin{split}\nY_t &= \\phi_1 Y_{t-1} + \\dots + \\phi_p Y_{t-p} + \\theta_1 a_{t-1} + \\dots + \\theta_q a_{t-q} + a_t\\\\\n&+\\sum^{k}_{j =1} \\beta_{j} X_{t,j},\n\\end{split}\n\\tag{9.11}\\] where \\(a_t\\) is still a zero-mean white noise process. Interestingly, at this time there is no convenient way to estimate this model in R. One could manually write lagged values of \\(Y_t\\) as external regressors (i.e., create new variables in R for \\(Y_{t-1},\\dots, Y_{t-p}\\)), use these variables in the R functions mentioned above, but force \\(p=0\\) (e.g., Soliman et al. 2019 used the functions from the R package forecast).\nThe difference between Equation 9.10 and Equation 9.11 is the presence of lagged values of the response variable, \\(Y_t\\), in Equation 9.11. As Hyndman points out, regression coefficients \\(\\beta_j\\) in Equation 9.11 lose their interpretability compared with usual regression and do not show the effect on \\(Y_t\\) when \\(X_t\\) increased by one. Instead, \\(\\beta\\)’s in ARMAX Equation 9.11 are interpreted conditional on the value of previous values of the response variable (https://robjhyndman.com/hyndsight/arimax/). Therefore, model formulation as in Equation 9.10 may be preferred.\n\n\n\n\n\n\nNote\n\n\n\nOther applicable models include models with mixed effects such as for repeated measures ANOVA (e.g., implemented in R using nlme::gls() and nlme::lme(), see different correlation structures; without a grouping factor, the results of nlme::lme(..., correlation = corARMA(...)) should be similar to estimating model in Equation 9.10). Other regression functions often borrow the functionality (and syntax) of the package nlme for estimating random effects, so autocorrelated residuals can be incorporated into a generalized additive model, GAM, mgcv::gamm(); generalized additive model for location scale and shape, GAMLSS, gamlss::gamlss(), which also can be used just as GAM, if scale and shape parameters are not modeled. A slightly different solution is possible using a generalized autoregressive moving average model (GARMA) demonstrated in Appendix D, see gamlss.util::garmaFit().\n\n\n\n\n\n\n\n\nNoteExample: Golden tilefish and AMO\n\n\n\nHere we revisit time series of 1918–2017 annual landings of golden tilefish in the U.S. North Atlantic region and the Atlantic Multi-decadal Oscillation (AMO) index characterizing climatic conditions (Figure 9.4). These time series were described in Nesslage et al. (2021) with R code by Lyubchich and Nesslage (2020). The goal is to develop a regression model to explore the relationship between the landings and AMO.\n\nCodeD &lt;- read.csv(\"data/tilefish.csv\")\nsummary(D)\n\n#&gt;       Year         Landings         AMO          \n#&gt;  Min.   :1918   Min.   :   5   Min.   :-0.43383  \n#&gt;  1st Qu.:1943   1st Qu.: 454   1st Qu.:-0.14821  \n#&gt;  Median :1968   Median : 749   Median : 0.01392  \n#&gt;  Mean   :1968   Mean   : 952   Mean   : 0.00147  \n#&gt;  3rd Qu.:1992   3rd Qu.:1204   3rd Qu.: 0.14944  \n#&gt;  Max.   :2017   Max.   :3968   Max.   : 0.35817  \n#&gt;                 NA's   :3\n\n\n\nCodep1 &lt;- ggplot(D, aes(x = Year, y = Landings)) +\n    geom_line() +\n    xlab(\"Year\") +\n    ylab(\"Landings (tonne)\")\np2 &lt;- ggplot(D, aes(x = Year, y = AMO)) +\n    geom_line() +\n    xlab(\"Year\") +\n    ylab(\"AMO\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 9.4: Time series plots of the golden tilefish landings and AMO.\n\n\n\n\nInterpolation of missing data (a.k.a. imputation) is a separate and very expansive topic. For a univariate time series, linear interpolation can be implemented using forecast::na.interp(). Also, the landings time series required a power transformation, so the square root transformation was applied (Figure 9.5).\n\nCodeD &lt;- D %&gt;% \n    mutate(Landings_noNA = as.numeric(forecast::na.interp(D$Landings))) %&gt;% \n    mutate(Landings_noNA_sqrt = sqrt(Landings_noNA))\n\nD %&gt;% \n    dplyr::select(AMO, Landings_noNA_sqrt) %&gt;% \n    GGally::ggpairs()\n\n\n\n\n\n\nFigure 9.5: Scatterplot matrix of landings and AMO.\n\n\n\n\n\nCodeforecast::ggCcf(D$AMO, D$Landings_noNA_sqrt) + \n    ggtitle(\"\")\n\n\n\n\n\n\nFigure 9.6: Estimated cross-correlation function (CCF) of the AMO and landings time series.\n\n\n\n\nBased on the strongest lagged correlations (Figure 9.6), implement a model \\[\n\\sqrt{Landings_t} = \\beta_0 + \\beta_{1} AMO_{t-7} + \\epsilon_{t},\n\\] where \\(\\epsilon_{t} \\sim\\) ARMA(\\(p,q\\)), and the orders \\(p\\) and \\(q\\) are selected automatically based on Akaike information criterion.\n\nCodelibrary(fable)\nm1 &lt;- D %&gt;% \n    select(Year, Landings_noNA_sqrt, AMO) %&gt;% \n    as_tsibble(index = Year) %&gt;% \n    model(ARIMA(Landings_noNA_sqrt ~ dplyr::lag(AMO, 7)))\n\nreport(m1)\n\n#&gt; Series: Landings_noNA_sqrt \n#&gt; Model: LM w/ ARIMA(2,0,0) errors \n#&gt; \n#&gt; Coefficients:\n#&gt;         ar1     ar2  dplyr::lag(AMO, 7)  intercept\n#&gt;       0.991  -0.184               -6.30       28.3\n#&gt; s.e.  0.104   0.102                5.02        3.4\n#&gt; \n#&gt; sigma^2 estimated as 41.61:  log likelihood=-307\n#&gt; AIC=625   AICc=626   BIC=638\n\n\nWe forgot to check the stationarity of the time series! It seems that the landings can be considered as a unit-root process, so differencing is needed, hence a modified model is\n\\[\n\\Delta \\sqrt{Landings_t} = \\beta_0 + \\beta_{1} AMO_{t-7} + \\epsilon_{t}.\n\\]\n\nCodem2 &lt;- forecast::auto.arima(diff(D$Landings_noNA_sqrt),\n                 xreg = dplyr::lag(D$AMO, 7)[-1],\n                 allowmean = TRUE)\nm2\n\n#&gt; Series: diff(D$Landings_noNA_sqrt) \n#&gt; Regression with ARIMA(0,0,0) errors \n#&gt; \n#&gt; Coefficients:\n#&gt;        xreg\n#&gt;       -2.58\n#&gt; s.e.   3.82\n#&gt; \n#&gt; sigma^2 = 46.3:  log likelihood = -313\n#&gt; AIC=629   AICc=630   BIC=634\n\n\nNote it is different from the implementation with d = 1, which differences all the series.\n\\[\n\\Delta \\sqrt{Landings_t} = \\beta_0 + \\beta_{1} \\Delta AMO_{t-7} + \\epsilon_{t}\n\\]\n\nCodeforecast::auto.arima(D$Landings_noNA_sqrt,\n                     xreg = dplyr::lag(D$AMO, 7), \n                     d = 1)\n\n#&gt; Series: D$Landings_noNA_sqrt \n#&gt; Regression with ARIMA(0,1,0) errors \n#&gt; \n#&gt; Coefficients:\n#&gt;        xreg\n#&gt;       -5.37\n#&gt; s.e.   5.14\n#&gt; \n#&gt; sigma^2 = 45.7:  log likelihood = -309\n#&gt; AIC=623   AICc=623   BIC=628\n\n\nSimilarly, using the newer package fable.\n\nCodem3 &lt;- D %&gt;% \n    select(Year, Landings_noNA_sqrt, AMO) %&gt;% \n    as_tsibble(index = Year) %&gt;% \n    model(ARIMA(Landings_noNA_sqrt ~ pdq(d = 1) + dplyr::lag(AMO, 7)))\n\nreport(m3)\n\n#&gt; Series: Landings_noNA_sqrt \n#&gt; Model: LM w/ ARIMA(0,1,0) errors \n#&gt; \n#&gt; Coefficients:\n#&gt;       dplyr::lag(AMO, 7)\n#&gt;                    -5.37\n#&gt; s.e.                5.14\n#&gt; \n#&gt; sigma^2 estimated as 45.73:  log likelihood=-309\n#&gt; AIC=623   AICc=623   BIC=628",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time Series Regression with Correlated Errors</span>"
    ]
  },
  {
    "objectID": "l09_tsreg2.html#sec-GAMLSS",
    "href": "l09_tsreg2.html#sec-GAMLSS",
    "title": "9  Time Series Regression with Correlated Errors",
    "section": "\n9.5 GAMLSS",
    "text": "9.5 GAMLSS\nStasinopoulos and Rigby (2007) provide an extension of a generalized additive model to \\(k=1,2,3,4\\) parameters \\(\\boldsymbol{\\theta}_k\\) of a distribution in a so-called generalized additive model for location scale and shape (GAMLSS). The \\(k\\) parameters represent the location parameter \\(\\mu\\) (it is what we typically model with regression models), scale \\(\\sigma\\), and two shape parameters: skewness and kurtosis. The model can be further generalized for \\(k&gt;4\\) if needed. It allows us to fit \\(k\\) individual models to study relationships between regressors \\(\\boldsymbol{x}\\) and parameters of the response distribution: \\[\ng_k(\\boldsymbol{\\theta}_k) = h_k\\left(\\boldsymbol{X}_k,\\boldsymbol{\\beta}_k\\right) + \\sum_{j=1}^{J_k}h_{jk}(\\boldsymbol{x}_{jk}),\n\\tag{9.12}\\] where \\(k=1\\) produces model for the mean; \\(h_k(\\cdot)\\) and \\(h_{jk}(\\cdot)\\) are nonlinear functions; \\(\\boldsymbol{\\beta}_k\\) is a parameter vector of length \\(J_k\\); \\(\\boldsymbol{X}_k\\) is an \\(n\\times J_k\\) design matrix; \\(\\boldsymbol{x}_{jk}\\) are vectors of length \\(n\\). The terms in the GAMLSS provide a flexible framework to specify nonlinearities, random effects, and correlation structure as in the mixed effects models (Zuur et al. 2009); see Table 3 by Stasinopoulos and Rigby (2007) for the possible specifications of the additive terms. Hence, a model as in Equation 9.12 may accommodate non-normal distributions, possibly nonlinear relationships, and spatiotemporal dependencies in the data.\n\n\n\n\n\n\nNoteExample: Insurance claims GAMLSS\n\n\n\nConsider the weekly number of home insurance claims related to water and weather damage in one Canadian city. The number is standardized by the daily number of insured properties in that city. The date of the claim corresponds to reported date of water weather damage, not the date when the claim was filed. This allows us to explore the relationship between the number of claims and weekly total precipitation (mm).\n\nCodeInsurance &lt;- read.csv(\"data/insurance_weekly.csv\") %&gt;%\n    dplyr::select(Claims, Precipitation)\nsummary(Insurance)\n\n#&gt;      Claims       Precipitation   \n#&gt;  Min.   :  0.00   Min.   : 0.000  \n#&gt;  1st Qu.:  1.00   1st Qu.: 0.775  \n#&gt;  Median :  3.00   Median : 3.800  \n#&gt;  Mean   :  3.61   Mean   : 7.713  \n#&gt;  3rd Qu.:  4.00   3rd Qu.:10.000  \n#&gt;  Max.   :170.00   Max.   :77.300\n\n\n\nCodeInsurance %&gt;%\n    GGally::ggpairs()\n\n\n\n\n\n\nFigure 9.7: Scatterplot matrix of weekly weather-related home insurance outcomes and precipitation.\n\n\n\n\nBased on the distribution plots in Figure 9.7, the data are highly right-skewed (the distributions have heavy right tails). The number of claims is also a discrete variable. Therefore, we deal with non-normal distributions and need to use generalized-type models, like the generalized linear or additive models (GLMs or GAMs). Since the claim counts contain many zeros and exhibit overdispersion, we might need to use the zero-adjusted Poisson distribution or negative binomial distribution for this response variable (Gupta et al. 1996; Stasinopoulos and Rigby 2007).\nIn our case, there is just a slight chance that past-week precipitation affects the current-week insurance claims. Hence, we will keep the current precipitation and additionally explore the lagged effects, using the cross-correlation function (Figure 9.8).\n\nCodelogconstant &lt;- 1\nforecast::ggCcf(Insurance$Precipitation, \n                log(Insurance$Claims + logconstant),\n                lag.max = 3) + \n    ggtitle(\"\")\n\n\n\n\n\n\nFigure 9.8: Estimated cross-correlation function (CCF) of precipitation and number of home insurance claims.\n\n\n\n\nBased on the estimated CCFs (Figure 9.8), past-week precipitation is significantly correlated with the current-week number of claims, so we can add the lagged predictor into our models.\n\nInsurance &lt;- Insurance %&gt;%\n    mutate(Precipitation_lag1 = dplyr::lag(Precipitation, 1),\n           Week = 1:nrow(Insurance),\n           Year = rep(2002:2011, each = 52),\n           Claims_ln = log(Claims + logconstant)) %&gt;% \n    mutate(Claims_ln_lag1 = dplyr::lag(Claims_ln, 1))\n\n# The model function doesn't accept NAs, so remove them\nInsurance_noNA &lt;- na.omit(Insurance)\n\nBased on Figure 9.9, there might be an increasing trend in the number of claims that we can approximate with a linear function.\n\nCodep1 &lt;- ggplot2::autoplot(as.ts(log(Insurance$Claims + logconstant))) +\n    xlab(\"Week\") +\n    ylab(\"ln(Number of claims)\")\np2 &lt;- forecast::ggAcf(as.ts(log(Insurance$Claims + logconstant)),\n                      lag.max = 110) +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 9.9: Time series plot and sample ACF of the weekly number of claims in a city, after the ln transformation.\n\n\n\n\nPlot the data once again after the transformations (Figure 9.10).\n\nCodeInsurance %&gt;%\n    dplyr::select(-Claims, -Year) %&gt;%\n    GGally::ggpairs()\n\n\n\n\n\n\nFigure 9.10: Scatterplot matrix of the weekly number of weather-related home insurance claims and precipitation.\n\n\n\n\nNegative binomial distribution has two parameters (see ?NBI) – location and scale – so the GAMLSS using this distribution family can include up to two equations (\\(k = 1, 2\\) in Equation 9.12). See ?gamlss.family and gamlss::chooseDist() for other distribution families, continuous and discrete; somewhat out-of-date tables with many of these distributions listed are available from Stasinopoulos and Rigby (2007).\nWe will model a linear relationship between claims and the week number (parametric linear trend; note that the variable Week here is the time index, not the week of the year) but will use nonparametric additive smooths for the relationships between number of claims and precipitation amounts.\nWe will model the scale (variability) of the number of claims using a smooth term of the week number.\nWe start by fitting a model as follows: \\[\n\\begin{split}\nClaims_t &= NegBin(\\mu_t, \\sigma_t) \\\\\n\\ln(\\mu_t) &= a_0 + a_1 Week_t + f_1(Precipitation_t) + f_2(Precipitation_{t-1}) + \\epsilon_t \\\\\n\\ln(\\sigma_t) &= b_0 + f_3(Week) \\\\\n\\end{split}\n\\tag{9.13}\\] where \\(\\epsilon_t \\sim \\mathrm{WN}(0,\\sigma^2)\\); \\(a_0\\), \\(a_1\\), and \\(b_0\\) are parametric coefficients; \\(f_1\\), \\(f_2\\), and \\(f_3\\) are nonparametric smooths.\nIn its expandable collection of smoothers, the R package gamlss features penalized B-splines. See the help files ?gamlss and ?pb for more details and the review of spline functions in R by Perperoglou et al. (2019). However, here we used the original P-splines ps() because they provided more visually smooth results than pb() under the default settings. The control arguments change the default settings to speed up the algorithm convergence.\n\nlibrary(gamlss)\nset.seed(12345)\nm00_gamlss &lt;- gamlss(Claims ~ ps(Precipitation) + Week + ps(Precipitation_lag1)\n                     ,sigma.formula = ~ps(Week)\n                     ,family = NBI\n                     ,control = gamlss.control(c.crit = 0.01, trace = FALSE)\n                     ,data = Insurance_noNA)\n\nBased on the observed ACF and PACF patterns in Figure 9.11, an appropriate model for the temporal dependence could be ARMA(1,0), ARMA(0,3), or ARMA(1,1).  Since the package gamlss does not have built-in functionality to specify correlation structures in the residuals, we will include the lagged claims as a predictor in the model.\n\nCodeplot(m00_gamlss, ts = TRUE)\n\n#&gt; ******************************************************************\n#&gt;   Summary of the Randomised Quantile Residuals\n#&gt;                            mean   =  0.0101 \n#&gt;                        variance   =  0.919 \n#&gt;                coef. of skewness  =  0.215 \n#&gt;                coef. of kurtosis  =  4.16 \n#&gt; Filliben correlation coefficient  =  0.994 \n#&gt; ******************************************************************\n\n\n\n\n\n\n\nFigure 9.11: Residual diagnostics of the initial GAMLSS for the number of home insurance claims.\n\n\n\n\n\nset.seed(12345)\nm10_gamlss &lt;- gamlss(Claims ~ ps(Precipitation) + Week + ps(Precipitation_lag1) + Claims_ln_lag1\n                     ,sigma.formula = ~ps(Week)\n                     ,family = NBI\n                     ,control = gamlss.control(c.crit = 0.01, trace = FALSE)\n                     ,data = Insurance_noNA)\n\nResidual diagnostics of the resulting GAMLSS look satisfactory (Figure 9.12), with most of the correlations disappeared, and the residuals being approximately normally distributed. See Dunn and Smyth (1996) for the details on randomized quantile residuals.\n\nCodeplot(m10_gamlss, ts = TRUE)\n\n#&gt; ******************************************************************\n#&gt;   Summary of the Randomised Quantile Residuals\n#&gt;                            mean   =  0.0167 \n#&gt;                        variance   =  0.932 \n#&gt;                coef. of skewness  =  0.129 \n#&gt;                coef. of kurtosis  =  3.83 \n#&gt; Filliben correlation coefficient  =  0.996 \n#&gt; ******************************************************************\n\n\n\n\n\n\n\nFigure 9.12: Residual diagnostics of the GAMLSS for the number of home insurance claims.\n\n\n\n\nSee the model summary below. Note it has a table for each parameter of the distribution.\n\nsummary(m10_gamlss)\n\n#&gt; ******************************************************************\n#&gt; Family:  c(\"NBI\", \"Negative Binomial type I\") \n#&gt; \n#&gt; Call:  gamlss(formula = Claims ~ ps(Precipitation) + Week +  \n#&gt;     ps(Precipitation_lag1) + Claims_ln_lag1, sigma.formula = ~ps(Week),  \n#&gt;     family = NBI, data = Insurance_noNA, control = gamlss.control(c.crit = 0.01,  \n#&gt;         trace = FALSE)) \n#&gt; \n#&gt; Fitting method: RS() \n#&gt; \n#&gt; ------------------------------------------------------------------\n#&gt; Mu link function:  log\n#&gt; Mu Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            0.341242   0.099084    3.44  0.00062 ***\n#&gt; ps(Precipitation)      0.013650   0.002602    5.25  2.3e-07 ***\n#&gt; Week                   0.001096   0.000274    3.99  7.4e-05 ***\n#&gt; ps(Precipitation_lag1) 0.007413   0.002953    2.51  0.01239 *  \n#&gt; Claims_ln_lag1         0.280665   0.056151    5.00  8.0e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; ------------------------------------------------------------------\n#&gt; Sigma link function:  log\n#&gt; Sigma Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -0.307970   0.227831   -1.35   0.1771    \n#&gt; ps(Week)    -0.003157   0.000806   -3.92   0.0001 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; ------------------------------------------------------------------\n#&gt; NOTE: Additive smoothing terms exist in the formulas: \n#&gt;  i) Std. Error for smoothers are for the linear effect only. \n#&gt; ii) Std. Error for the linear terms maybe are not accurate. \n#&gt; ------------------------------------------------------------------\n#&gt; No. of observations in the fit:  519 \n#&gt; Degrees of Freedom for the fit:  16\n#&gt;       Residual Deg. of Freedom:  503 \n#&gt;                       at cycle:  6 \n#&gt;  \n#&gt; Global Deviance:     2194 \n#&gt;             AIC:     2226 \n#&gt;             SBC:     2294 \n#&gt; ******************************************************************\n\n\nSince the model passes residual diagnostics and all coefficients are significant, we can stop refining the model. This is the selected model in mathematical notations: \\[\n\\begin{split}\nClaims_t &= NegBin(\\mu_t, \\sigma_t) \\\\\n\\ln(\\mu_t) &= a_0 + a_1 Week_t + a_2 ln(Claims_{t-1}) + f_1(Precipitation_t) + f_2(Precipitation_{t-1}) + \\epsilon_t \\\\\n\\ln(\\sigma_t) &= b_0 + f_3(Week) \\\\\n\\end{split}\n\\tag{9.14}\\] where \\(\\epsilon_t \\sim \\mathrm{WN}(0,\\sigma^2)\\); \\(a_0\\), \\(a_1\\), \\(a_2\\), and \\(b_0\\) are parametric coefficients; \\(f_1\\), \\(f_2\\), and \\(f_3\\) are nonparametric smooths.\nThe term plots visualize the relationships represented by the model (Figure 9.13 and Figure 9.14).\n\nCodegamlss.ggplots::fitted_terms(m10_gamlss, rug = TRUE, nrow = 2, what = \"mu\")\n\n\n\n\n\n\nFigure 9.13: GAMLSS terms showing the estimated relationships between the regressors and the location parameter for the distribution of the number of home insurance claims.\n\n\n\n\n\nCodegamlss.ggplots::fitted_terms(m10_gamlss, rug = TRUE, nrow = 1, what = \"sigma\")\n\n\n\n\n\n\nFigure 9.14: GAMLSS terms showing the estimated relationships between the regressors and the scale parameter for the distribution of the number of home insurance claims.\n\n\n\n\nA similar result can be achieved by using a generalized additive mixed model (GAMM) with ARMA errors, using the package mgcv.\n\nlibrary(mgcv)\n\n# Fit the model using gam() to estimate theta\ngam_fit &lt;- mgcv::gam(Claims ~ s(Precipitation) + Week + s(Precipitation_lag1)\n                     ,family = nb() # nb() automatically estimates theta\n                     ,method = \"REML\"\n                     ,data = Insurance_noNA)\n\n# Extract the estimated theta\nestimated_theta &lt;- exp(gam_fit$family$getTheta(TRUE))\n\n# Fit the GAMM with the estimated theta and AR(1) correlation structure\nm10_gamm &lt;- mgcv::gamm(Claims ~ s(Precipitation) + Week + s(Precipitation_lag1)\n                       ,family = negbin(theta = estimated_theta)\n                       ,method = \"REML\"\n                       ,correlation = corARMA(p = 1, q = 0)\n                       ,data = Insurance_noNA)\n\n# Summary of the GAM part\nsummary(m10_gamm$gam)\n\n# Summary of the mixed part (GAM smooths are treated as linear predictors)\nsummary(m10_gamm$lme)\n\nOverall, the package gamlss allowed us to model different parameters (location, scale, etc.) of the distribution of claims, using additive models and a wide range of distributions; we specified autocorrelations manually by incorporating lagged versions of the variables. In contrast, mgcv::gamm() allowed us to specify the correlation structure directly, but we were limited in the selection of distributions and in modeling only the location parameter of the distribution.\nAlso, see the generalized autoregressive moving average (GARMA) model applied in Appendix D.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time Series Regression with Correlated Errors</span>"
    ]
  },
  {
    "objectID": "l09_tsreg2.html#sec-Granger",
    "href": "l09_tsreg2.html#sec-Granger",
    "title": "9  Time Series Regression with Correlated Errors",
    "section": "\n9.6 Granger causality",
    "text": "9.6 Granger causality\nThe Granger causality (Granger 1969; Kirchgässner and Wolters 2007) concept is based on predictability, which is why we should consider it in the time series course. Pearl causality is based on the analysis of interventions (Rebane and Pearl 1987; Pearl 2009).\nLet \\(I_t\\) be the total information set available at the time \\(t\\). This information set includes the two time series \\(X\\) and \\(Y\\). Let \\(\\bar{X}_t\\) be the set of all current and past values of \\(X\\), i.e., \\(\\bar{X}_t = \\{X_{t}, X_{t-1}, \\dots, X_{t-k}, \\dots \\}\\) and analogously of \\(Y\\). Let \\(\\sigma^2(\\cdot)\\) be the variance of the corresponding forecast error.\n\n\n\n\n\n\nNote\n\n\n\nDifference of two sets, \\(A\\) and \\(B\\), is denoted by \\(A \\setminus B\\); but sometimes the minus sign is used, \\(A - B\\).\n\n\nGranger causality\n\\(X\\) is (simply) Granger causal to \\(Y\\) if future values of \\(Y\\) can be predicted better, i.e., with a smaller forecast error variance, if current and past values of \\(X\\) are used: \\[\n\\sigma^2(Y_{t+1}|I_t) &lt; \\sigma^2(Y_{t+1}|I_t \\setminus \\bar{X}_t).\n\\tag{9.15}\\]\nInstantaneous Granger causality\n\\(X\\) is instantaneously Granger causal to \\(Y\\) if the future value of \\(Y\\), \\(Y_{t+1}\\), can be predicted better, i.e., with a smaller forecast error variance, if the future value of \\(X\\), \\(X_{t+1}\\), is used in addition to the current and past values of \\(X\\): \\[\n\\sigma^2(Y_{t+1}|\\{I_t, X_{t+1}\\}) &lt; \\sigma^2(Y_{t+1}|I_t ).\n\\tag{9.16}\\]\nFeedback\nThere is feedback between \\(X\\) and \\(Y\\) if \\(X\\) is causal to \\(Y\\) and \\(Y\\) is causal to \\(X\\). Feedback is only defined for the case of simple causal relations.\nThe test for Equation 9.15 and Equation 9.16 is, essentially, an \\(F\\)-test comparing two nested models: with additional predictors \\(X\\) and without. In other words, consider the model: \\[\nY_t = \\beta_0 + \\sum_{k=1}^{k_1}\\beta_k Y_{t-k} + \\sum_{k=k_0}^{k_2}\\alpha_k X_{t-k} + U_t\n\\tag{9.17}\\] with \\(k_0 = 1\\). An \\(F\\)-test is applied to test the null hypothesis, H\\(_0\\): \\(\\alpha_1 = \\alpha_2 = \\dots = \\alpha_{k_2} = 0\\). By switching \\(X\\) and \\(Y\\) in Equation 9.17, it can be tested whether a simple causal relation from \\(Y\\) to \\(X\\) exists. There is a feedback relation if the null hypothesis is rejected in both directions (\\(X\\rightarrow Y\\) and \\(Y\\rightarrow X\\)). To test whether there is an instantaneous causality, we finally set \\(k_0 = 0\\) and perform a \\(t\\) or \\(F\\)-test for the null hypothesis H\\(_0\\): \\(\\alpha_0 = 0\\).\nThe problem with this test is that the results are strongly dependent on the number of lags of the explanatory variable, \\(k_2\\). There is a trade-off: the more lagged values we include, the better the influence of this variable can be captured. This argues for a high maximal lag. On the other hand, the power of this test is lower the more lagged values are included (Chapter 3 of Kirchgässner and Wolters 2007). Two general procedures can be used to select the lags: inspecting the sensitivity of results to different \\(k_2\\) (sensitivity analysis) or one of the different information criteria guiding model selection.\n\n\n\n\n\n\nNoteExample: Insurance claims and precipitation Granger causality test\n\n\n\nFor example, use the insurance data to test the relationships between the log-transformed home insurance number of claims and precipitation. A quick test has been performed to check that the time series are stationary and do not have a strong seasonality, so the chance of detecting spurious relationships is minimized. We can use the series in Equation 9.17.\n\nlmtest::grangertest(Claims_ln ~ Precipitation \n                    ,order = 1\n                    ,data = Insurance_noNA)\n\n#&gt; Granger causality test\n#&gt; \n#&gt; Model 1: Claims_ln ~ Lags(Claims_ln, 1:1) + Lags(Precipitation, 1:1)\n#&gt; Model 2: Claims_ln ~ Lags(Claims_ln, 1:1)\n#&gt;   Res.Df Df    F Pr(&gt;F)   \n#&gt; 1    515                  \n#&gt; 2    516 -1 10.5 0.0013 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe low \\(p\\)-value shows that precipitation is a Granger cause of the number of home insurance claims. Now test the reverse relationship.\n\nlmtest::grangertest(Precipitation ~ Claims_ln  \n                    ,order = 1\n                    ,data = Insurance_noNA)\n\n#&gt; Granger causality test\n#&gt; \n#&gt; Model 1: Precipitation ~ Lags(Precipitation, 1:1) + Lags(Claims_ln, 1:1)\n#&gt; Model 2: Precipitation ~ Lags(Precipitation, 1:1)\n#&gt;   Res.Df Df   F Pr(&gt;F)\n#&gt; 1    515              \n#&gt; 2    516 -1 0.2   0.65\n\n\nReverse testing does not confirm a statistically significant Granger causality. Hence, we do not have enough evidence to claim that insurance claims are a Granger cause of precipitation (we could expect to come to this conclusion based on our knowledge of how things work), and hence there is no evidence of feedback between losses and precipitation.\nThe function lmtest::grangertest() does not allow us to set order = 0 (to test instantaneous Granger causality), but we can do it manually. First, to show that ‘manual’ results match the lmtest::grangertest() output, repeat the test above using two nested models with lag 1.\n\nM1 &lt;- lm(Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, 1)\n         ,data = Insurance_noNA)\nM2 &lt;- lm(Claims_ln ~ dplyr::lag(Claims_ln, 1)\n         ,data = Insurance_noNA)\nanova(M1, M2)\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, \n#&gt;     1)\n#&gt; Model 2: Claims_ln ~ dplyr::lag(Claims_ln, 1)\n#&gt;   Res.Df RSS Df Sum of Sq    F Pr(&gt;F)   \n#&gt; 1    515 211                            \n#&gt; 2    516 215 -1     -4.32 10.5 0.0013 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results match.\nSecond, test the instantaneous Granger causality in the same manner.\n\nM3 &lt;- lm(Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, 1) + \n             Precipitation, data = Insurance_noNA)\nM4 &lt;- lm(Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, 1)\n         ,data = Insurance_noNA)\nanova(M3, M4)\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, \n#&gt;     1) + Precipitation\n#&gt; Model 2: Claims_ln ~ dplyr::lag(Claims_ln, 1) + dplyr::lag(Precipitation, \n#&gt;     1)\n#&gt;   Res.Df RSS Df Sum of Sq  F  Pr(&gt;F)    \n#&gt; 1    514 203                            \n#&gt; 2    515 211 -1     -7.91 20 9.5e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results show the presence of such causality.\nAs an extension to the demonstrated techniques for testing Granger causality, we may consider more complex generalized and additive models, to relax the assumptions of normality and linearity in the modeling process.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe lmtest::grangertest() options set one value to both \\(k_1\\) and \\(k_2\\) in Equation 9.17. In our example, it was \\(k_1 = k_2 = 1\\). The ‘manual’ test using the function anova() can be used for models with \\(k_1 = k_2\\) or \\(k_1 \\neq k_2\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time Series Regression with Correlated Errors</span>"
    ]
  },
  {
    "objectID": "l09_tsreg2.html#conclusion",
    "href": "l09_tsreg2.html#conclusion",
    "title": "9  Time Series Regression with Correlated Errors",
    "section": "\n9.7 Conclusion",
    "text": "9.7 Conclusion\nMultivariate models are still much more difficult to fit than univariate ones. Multiple regression remains a treacherous procedure when applied to time series data. Many observed time series exhibit nonlinear characteristics, but nonlinear models often fail to give better out-of-sample forecasts than linear models, perhaps because the latter are more robust to departures from model assumptions. It is always a good idea to end with the so-called eyeball test. Plot the forecasts on a time plot of the data and check that they look intuitively reasonable (Chatfield 2000).\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton, FL, USA\n\n\nDean RT, Dunsmuir WTM (2016) Dangers and uses of cross-correlation in analyzing time series in perception, performance, movement, and neuroscience: The importance of constructing transfer function autoregressive models. Behavior Research Methods 48:783–802. https://doi.org/10.3758/s13428-015-0611-2\n\n\nDunn PK, Smyth GK (1996) Randomized quantile residuals. Journal of Computational and Graphical Statistics 5:236–244. https://doi.org/10.2307/1390802\n\n\nGranger CWJ (1969) Investigating causal relations by econometric models and cross-spectral methods. Econometrica 37:424–438. https://doi.org/10.2307/1912791\n\n\nGupta PL, Gupta RC, Tripathi RC (1996) Analysis of zero-adjusted count data. Computational Statistics & Data Analysis 23:207–218. https://doi.org/10.1016/S0167-9473(96)00032-1\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern time series analysis. Springer-Verlag, Berlin, Germany\n\n\nLyubchich V, Nesslage G (2020) Environmental drivers of golden tilefish fisheries v1.0. Version v1.0. Zenodo\n\n\nNesslage G, Lyubchich V, Nitschke P, et al (2021) Environmental drivers of golden tilefish (Lopholatilus chamaeleonticeps) commercial landings and catch-per-unit-effort. Fisheries Oceanography 30:608–622. https://doi.org/10.1111/fog.12540\n\n\nPearl J (2009) Causality: Models, reasoning, and inference, 2nd edn. Cambridge University Press, Cambridge, UK\n\n\nPerperoglou A, Sauerbrei W, Abrahamowicz M, Schmid M (2019) A review of spline function procedures in R. BMC Medical Research Methodology 19: https://doi.org/10.1186/s12874-019-0666-3\n\n\nRebane G, Pearl J (1987) The recovery of causal poly-trees from statistical data. In: Proceedings of the third annual conference on uncertainty in artificial intelligence. pp 222–228\n\n\nShumway RH, Stoffer DS (2017) Time series analysis and its applications with R examples, 4th edn. Springer, New York, NY, USA\n\n\nSoliman M, Lyubchich V, Gel YR (2019) Complementing the power of deep learning with statistical model fusion: Probabilistic forecasting of influenza in Dallas County, Texas, USA. Epidemics 28:100345. https://doi.org/10.1016/j.epidem.2019.05.004\n\n\nStasinopoulos DM, Rigby RA (2007) Generalized additive models for location scale and shape (GAMLSS) in R. Journal of Statistical Software 23:1–46. https://doi.org/10.18637/jss.v023.i07\n\n\nZuur A, Ieno EN, Walker NJ, et al (2009) Mixed effects models and extensions in ecology with R. Springer, New York",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time Series Regression with Correlated Errors</span>"
    ]
  },
  {
    "objectID": "l12_forecast.html",
    "href": "l12_forecast.html",
    "title": "10  Forecasting and Model Evaluation",
    "section": "",
    "text": "10.1 Time series forecasts\nThis lecture covers evaluation of time series forecasting models, including both point and interval forecasts. We discuss cross-validation schemes tailored to time series and strategies to avoid overfitting and data leakage.\nObjectives\nReading materials\nAudio overview",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forecasting and Model Evaluation</span>"
    ]
  },
  {
    "objectID": "l12_forecast.html#time-series-forecasts",
    "href": "l12_forecast.html#time-series-forecasts",
    "title": "10  Forecasting and Model Evaluation",
    "section": "",
    "text": "10.1.1 Assumptions\nForecasting models typically rely on several key assumptions:\n\n\nStructural stability: relationships among variables and their dynamics are assumed to remain the same in the future as in the past (see also Chapter 1 and Chapter 9). Violations (regime shifts, interventions) undermine predictive validity.\n\nData-generating process: models embed assumptions such as linearity, stationarity, seasonality, or error structure (white noise, ARMA). Forecast intervals also assume a specific distribution of errors (often Gaussian) unless bootstrapped.\n\nTiming of the dataset: genuine forecasts must only use information available at the forecast origin; using future predictors is data leakage.\n\nExogeneity of predictors: for regression with x-variables, either use leading indicators or forecast the predictors themselves (with added uncertainty).\n\nMeasurement and preprocessing: missing values, outliers, and calendar effects should be handled consistently between training and deployment.\n\n10.1.2 How to obtain forecasts from different types of models\nDifferent model classes require different forecasting approaches.\nWhite noise and simple trend models\nFor models with deterministic trends and white noise errors (e.g., linear regression), forecasts are simply the fitted values at future time points. Prediction intervals assume constant variance or use residual resampling (bootstrap).\nFor trends fitted via lm(), mgcv::gam(), or a similar function, use predict(model, newdata = future_df) to obtain point forecasts, where future_df should include new values of the time indices used to model the trends. Check for other arguments in the predict() method for the given object class (e.g., ?predict.lm or ?mgcv::predict.gam) to specify prediction intervals (e.g., interval = \"prediction\", level = 0.95) or extract standard errors (e.g., se.fit = TRUE) to compute custom prediction intervals.\nRecursive models\nARIMA, SARIMA models and exponential smoothing methods (simple, Holt, Holt–Winters) generate forecasts recursively, with their 1-step-ahead forecasts usually being most accurate. Multi-step forecasts from these models rely on their past forecasts, so forecast uncertainty grows with the forecast horizon.\nRecursive forecasts typically require just the number of steps ahead, h, to forecast. E.g., use predict(ARIMAmodel, n.ahead = h) for ARIMA models fitted in base R (see ?predict.Arima or ?predict.ar). The R package forecast provides a unified interface: its forecast(model, h = h) works for objects from ets(), auto.arima(), Arima(), HoltWinters(), and many others, returning point forecasts and prediction intervals.\nRegression models\nWhen a regression includes explanatory variables, forecasting requires values of future predictors (see discussion of ex-ante vs ex-post forecasts below), which are passed to the function using the arguments newdata, newxreg, or xreg. If the regression includes an ARMA component for its errors, it is predicted recursively (see the relevant help files for details, such as ?predict.Arima and ?forecast::forecast.Arima).\n\n10.1.3 Types of forecasts: ex-post vs. ex-ante\nLet \\(\\hat{Y}_T(h)\\) be a forecast \\(h\\) steps ahead made at time \\(T\\). If \\(\\hat{Y}_T(h)\\) only uses information up to time \\(T\\), the resulting forecasts are called out-of-sample forecasts. Chatfield (2000) listed several ways to unfairly ‘improve’ forecasts:\n\nFitting the model to all the data including the testing set.\nFitting several models to the training set and choosing the model which gives the best ‘forecasts’ of the testing set. The selected model is then used (again) to produce forecasts of the testing set, even though the latter has already been used in the modeling process.\nUsing the known test-set values of ‘future’ observations on the explanatory variables in multivariate forecasting. This will improve forecasts of the dependent variable in the testing set, but these future values of predictors will not be known at the time the forecast is supposedly made. Economists call such forecasts ex-post forecasts to distinguish them from ex-ante forecasts. The latter, being genuinely out-of-sample, use forecasts of future values of explanatory variables, where necessary, to compute forecasts of the response variable. Ex-post forecasts can be useful for assessing the effects of explanatory variables, provided the analyst does not pretend that they are genuine out-of-sample forecasts.\n\nSo what to do if we put lots of effort to build a regression model using time series and need to forecast the response, \\(Y_t\\), which is modeled using independent variables \\(X_{t,p}\\) (\\(p=1,\\dots,P\\))? Two options are possible.\nOption 1: Use leading indicators\nIf \\(X_{t,p}\\)’s are leading indicators with lags starting at \\(l\\), we generally would not need their future values to obtain the forecasts \\(\\hat{Y}_T(h)\\), where \\(h\\leqslant l\\). For example, the model for insurance claims tested in Section 9.6 shows that precipitation with lag 1 is a good predictor for current losses, i.e., precipitation is a leading indicator. The 1-week-ahead forecast of \\(Y_{t+1}\\) can be obtained using the current precipitation \\(X_t\\) (all data are available). If \\(h&gt;l\\), we will be forced to forecast the independent variables, \\(X_{t,p}\\)’s—see the next option.\nOption 2: Start by forecasting predictors\nIf we opt for forecasting \\(X_{t,p}\\)’s, the errors (uncertainty) of the resulting forecasts for \\(Y_t\\) will be larger, because future \\(X_{t,p}\\)’s themselves will be the estimates. Nevertheless, it might be the only choice when leading indicators are not available. Building a full and comprehensive model with all diagnostics for each regressor is usually unfeasible and even problematic if we plan to consider multivariate models for regressors (the complexity of models will quickly escalate). As an alternative, it is common to use automatic or semi-automatic univariate procedures that can help to forecast each of the \\(X_{t,p}\\)’s. For example, consider exponential smoothing, Holt–Winters smoothing, and auto-selected SARIMA/ARIMA/ARMA/AR/MA models—all those can be automated for a large number of forecasts to make.\n\n10.1.4 Typical set of models to compare\nIn practice, when comparing forecasting models we often consider:\n\nBasic (naive) models: average, climatology, seasonal naive, Holt–Winters, the last value. These serve as benchmarks – more complex models should outperform them.\nCommonly or previously used model: GLM or simpler ARIMA specification that has been used in the field or in previous studies. This provides a baseline representing current practice.\nState-of-the-science or proposed model: advanced methods (e.g., ARIMAX with exogenous predictors, machine learning, or nonparametric approaches) that incorporate domain knowledge or recent methodological developments.\n\nThe insurance example below uses a set like this and quantitatively assesses forecasting performance of the different models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forecasting and Model Evaluation</span>"
    ]
  },
  {
    "objectID": "l12_forecast.html#cross-validation-schemes",
    "href": "l12_forecast.html#cross-validation-schemes",
    "title": "10  Forecasting and Model Evaluation",
    "section": "\n10.2 Cross-validation schemes",
    "text": "10.2 Cross-validation schemes\nSuppose we have used the data \\(Y_1, \\dots, Y_n\\) to build \\(M\\) forecasting models \\(\\hat{Y}^{(m)}_t\\) (\\(m = 1,\\dots,M\\)) and we now obtain future observations \\(Y_{n+1}, \\dots, Y_{n+k}\\) that were not used to fit the models (also called out-of-sample data, after-sample, or the testing set; \\(k\\) is the size of this set). The difference \\(Y_t - \\hat{Y}^{(m)}_t\\) is the forecast (or prediction) error at time \\(t\\) for the \\(m\\)th model. However, an approach like this might require us to wait for future observations to assess and compare the models. A way around this is to take the historical dataset \\(Y_1, \\dots, Y_n\\) and split it into a training set and a testing set.\nCross-validation (CV) is a standard approach to formal assessment of model performance and generalization ability. By repeating the training and evaluation process on different data splits, CV provides robust estimates of out-of-sample accuracy and helps detect overfitting. Designing CV for time series models is more complex than for independent data due to temporal dependence and the need to respect time order. Choose a CV scheme that mirrors how the model will be used in practice. For that, you may consider\n\nForecast horizon: 1 week vs. 4 weeks vs. 1 year ahead (different error dynamics and interval calibration).\nUpdate frequency: will you refit the model weekly, monthly, or never (e.g., if the model is deployed on a remote buoy with limited bandwidth/power)?\nAvailability of predictors: are they leading indicators (ex-ante usable) or must they be predicted too?\n\n\n10.2.1 Principles for designing CV for time series\n\nRespect temporal order: never train on information from the future relative to validation points.\nEvaluate at the target horizon: if you need 1-week-ahead forecasts, it is impractical to assess model performance on 1-year-ahead horizons (and vice versa).\nMatch refit cadence: if you plan monthly refits, validate with monthly refits; if the model is frozen, validate with a single fit and no refitting.\nAvoid data leakage: when using predictors, supply leading indicators or forecast predictors when necessary.\n\n10.2.2 Common schemes\n\nSingle holdout (train/test split). Fit the model once on the training period; evaluate on a future block. Appropriate when a model is deployed once for a long time without refitting.\nExpanding window (a.k.a. walk-forward). Increase the training window over time, refit the model each time, and forecast \\(h\\) steps ahead. Mirrors frequent updates with accumulating data.\nFixed window (moving window). Use a sliding window of fixed length (last \\(W\\) observations), refit the model each time, forecast \\(h\\) steps. Useful when stationarity is local and old data become less relevant.\nPeriodic refits. Refit the model only every certain number of steps (e.g., monthly) and use the same fitted model between refits. Balances compute/operational constraints with adaptation.\n\nWe may compare a great number of models using the training set and choose the best one (with the smallest errors); however, it would be unfair to use the out-of-sample errors from the testing set for demonstrating the model performance because this part of the sample was used to select the model. Thus, it is advisable to have one more chunk of the same time series that was not used for model specification, estimation, or selection. Errors of the selected model on this validation set will be closer to the true (genuine) out-of-sample errors and can be used to assess coverage of true out-of-sample forecasts when the model is finally deployed.\n\n10.2.3 R implementations\nYou may find useful the following R functions for running CV: for(), while(), sapply(), and zoo::rollapply() loops, purrr::map(), or similar constructs. Also, consider if you should run heavy computations in parallel, with functions like parallel::parSapply(), future.apply::future_sapply(), or furrr::future_map(). Several R packages provide built-in functions for time-series CV:\n\n\nforecast::tsCV() automates rolling-origin errors for a user-specified forecast function; ideal for 1-step errors and extendable for \\(h\\)-step forecasts.\n\ncaret::createTimeSlices() (and trainControl(method = \"timeslice\")) creates indices for time-slice resampling that you can loop over with your own modeling function.\n\n\n\n\n\n\n\nNoteExample: Cross-validation schemes in R\n\n\n\nExamples below use the AirPassengers dataset (monthly international airline passenger numbers, 1949–1960) available in base R.\n\nSingle holdout (train/test split). Use 70% of data for training, last 30% of data for testing.\n\n\n# Sample size\nn &lt;- length(AirPassengers)\n\n# Training and testing set sizes\nn_train &lt;- floor(0.7 * n)\nh &lt;- n - n_train\n\n# For subsetting the ts object\ntrain_end_time &lt;- time(AirPassengers)[n_train]\ntest_start_time &lt;- time(AirPassengers)[n_train + 1]\n\n# Training and testing sets\ny_train &lt;- window(AirPassengers, end = train_end_time)\ny_test &lt;- window(AirPassengers, start = test_start_time)\n\n# Fit models on training data\nm_ets &lt;- forecast::ets(y_train)\nm_arima &lt;- forecast::auto.arima(y_train)\n\n# Forecasts\nfc_ets &lt;- forecast::forecast(m_ets, h = h)\nfc_arima &lt;- forecast::forecast(m_arima, h = h)\n\n# RMSE calculation\nc(RMSE_ETS    = sqrt(mean((y_test - fc_ets$mean)^2)),\n  RMSE_ARIMA  = sqrt(mean((y_test - fc_arima$mean)^2))\n)\n\n#&gt;   RMSE_ETS RMSE_ARIMA \n#&gt;       55.1       26.2\n\n\n\nExpanding window, \\(h\\)-step forecasts with forecast::tsCV().\n\n\nlibrary(forecast)\n\n# Set the forecast horizon (6 months ahead)\nh_target &lt;- 6\n\n# Set the initial training size (e.g., first 10 years)\nn_train_initial &lt;- 120  # 10 years * 12 months\n\n# Compute forecast errors using tsCV()\ne_ets_h   &lt;- tsCV(AirPassengers, initial = n_train_initial,\n                  forecastfunction = function(z, h) \n                      forecast(ets(z), h = h), h = h_target)\ne_arima_h &lt;- tsCV(AirPassengers, initial = n_train_initial, \n                  forecastfunction = function(z, h)\n                      forecast(auto.arima(z), h = h), h = h_target)\n\n# RMSE calculation for each model and horizon\ndata.frame(\n    RMSE_ETS_h   = apply(e_ets_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE))),\n    RMSE_ARIMA_h = apply(e_arima_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE)))\n)\n\n#&gt;     RMSE_ETS_h RMSE_ARIMA_h\n#&gt; h=1       20.5         17.4\n#&gt; h=2       26.4         19.7\n#&gt; h=3       31.0         20.0\n#&gt; h=4       38.7         21.2\n#&gt; h=5       40.2         20.9\n#&gt; h=6       39.7         21.3\n\n\n\nFixed window (moving window) with a custom loop.\n\n\n# Set the forecast horizon (6 months ahead)\nh_target &lt;- 6\n\n# Set the fixed training size (e.g., the last 10 years)\nn_train &lt;- 120  # 10 years * 12 months\n\n# Initialize error storage\ne_ets_h &lt;- e_arima_h &lt;- matrix(NA, nrow = 0, ncol = h_target)\n\n# Loop over time points with seq(); can change the argument \"by\" to skip some points\nfor (i in seq(n_train, length(AirPassengers) - h_target, by = 1)) {\n    y_train &lt;- window(AirPassengers, \n                      start = time(AirPassengers)[i - n_train + 1], \n                      end = time(AirPassengers)[i])\n    y_test  &lt;- window(AirPassengers, \n                      start = time(AirPassengers)[i + 1], \n                      end = time(AirPassengers)[i + h_target])\n    \n    # Fit models\n    m_ets   &lt;- forecast::ets(y_train)\n    m_arima &lt;- forecast::auto.arima(y_train)\n    \n    # Forecasts\n    fc_ets   &lt;- forecast::forecast(m_ets, h = h_target)\n    fc_arima &lt;- forecast::forecast(m_arima, h = h_target)\n    \n    # Forecast errors\n    e_ets_h   &lt;- rbind(e_ets_h, y_test - fc_ets$mean)\n    e_arima_h &lt;- rbind(e_arima_h, y_test - fc_arima$mean)\n}\n\n# RMSE calculation for each model and horizon\ndata.frame(\n    RMSE_ETS_h   = apply(e_ets_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE))),\n    RMSE_ARIMA_h = apply(e_arima_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE)))\n)\n\n#&gt;   RMSE_ETS_h RMSE_ARIMA_h\n#&gt; 1       20.9         18.2\n#&gt; 2       25.7         20.1\n#&gt; 3       30.9         23.3\n#&gt; 4       38.6         23.6\n#&gt; 5       39.9         25.2\n#&gt; 6       38.7         25.0\n\n\n\nPeriodic refits (e.g., yearly), \\(h\\)-step-ahead. Modify the loop above with if() statement and time_since_last_refit counter to refit only every refit_every steps.\n\n\n# Set the forecast horizon (6 months ahead)\nh_target &lt;- 6\n\n# Set the fixed training size (e.g., the last 10 years)\nn_train &lt;- 120  # 10 years * 12 months\n\n# How often to refit (in loop steps). Example: refit every 12 months\nrefit_every &lt;- 12\ntime_since_last_refit &lt;- refit_every  # Force refit at first iteration\n\n# Initialize error storage\ne_ets_h &lt;- e_arima_h &lt;- matrix(NA, nrow = 0, ncol = h_target)\n\n# Loop over time points with seq()\nfor (i in seq(n_train, length(AirPassengers) - h_target, by = 1)) {\n    y_train &lt;- window(AirPassengers, \n                      start = time(AirPassengers)[i - n_train + 1], \n                      end = time(AirPassengers)[i])\n    y_test  &lt;- window(AirPassengers, \n                      start = time(AirPassengers)[i + 1], \n                      end = time(AirPassengers)[i + h_target])\n    \n    if (time_since_last_refit &gt;= refit_every) {\n        # Fit models\n        m_ets   &lt;- forecast::ets(y_train)\n        m_arima &lt;- forecast::auto.arima(y_train)\n        \n        # Reset counter\n        time_since_last_refit &lt;- 0 \n        \n        print(paste(\"Refitted at time index:\", i))\n    }\n    \n    # Forecasts\n    # If not refitted, use the last fitted model and forecast further ahead\n    fc_ets   &lt;- forecast::forecast(m_ets, h = h_target + time_since_last_refit)\n    fc_arima &lt;- forecast::forecast(m_arima, h = h_target + time_since_last_refit)\n    \n    # Select only the relevant forecasts (last h_target steps)\n    last_h_target &lt;- (time_since_last_refit + 1):(time_since_last_refit + h_target)\n    \n    # Forecast errors\n    e_ets_h   &lt;- rbind(e_ets_h, y_test - fc_ets$mean[last_h_target])\n    e_arima_h &lt;- rbind(e_arima_h, y_test - fc_arima$mean[last_h_target])\n    \n    # Increment counter\n    time_since_last_refit &lt;- time_since_last_refit + 1\n}\n\n#&gt; [1] \"Refitted at time index: 120\"\n#&gt; [1] \"Refitted at time index: 132\"\n\n# RMSE calculation for each model and horizon\ndata.frame(\n    RMSE_ETS_h   = apply(e_ets_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE))),\n    RMSE_ARIMA_h = apply(e_arima_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE)))\n)\n\n#&gt;   RMSE_ETS_h RMSE_ARIMA_h\n#&gt; 1       44.3         40.2\n#&gt; 2       47.3         43.4\n#&gt; 3       48.3         46.0\n#&gt; 4       47.7         45.9\n#&gt; 5       50.7         51.1\n#&gt; 6       53.5         55.1",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forecasting and Model Evaluation</span>"
    ]
  },
  {
    "objectID": "l12_forecast.html#metrics-for-model-comparison",
    "href": "l12_forecast.html#metrics-for-model-comparison",
    "title": "10  Forecasting and Model Evaluation",
    "section": "\n10.3 Metrics for model comparison",
    "text": "10.3 Metrics for model comparison\n\n10.3.1 Point forecast metrics\nFor each model, compute the prediction mean square error (PMSE) \\[\nPMSE_m = k^{-1}\\sum_{t=n+1}^{n+k}\\left(Y_t - \\hat{Y}^{(m)}_t\\right)^2\n\\tag{10.1}\\] or prediction root mean square error (PRMSE) \\[\nPRMSE_m = \\sqrt{PMSE_m},\n\\tag{10.2}\\] prediction mean absolute error (PMAE) \\[\nPMAE_m = k^{-1}\\sum_{t=n+1}^{n+k}\\left|Y_t - \\hat{Y}^{(m)}_t\\right|,\n\\tag{10.3}\\] or prediction mean absolute percentage error (PMAPE, if \\(Y_t \\neq 0\\) in the testing period), etc. We choose the model with the smallest error. If two models produce approximately the same errors, we choose the model that is simpler (involves fewer variables). This is called the law of parsimony.\n\n\n\n\n\n\nNote\n\n\n\nPMSE and PRMSE penalize larger errors more heavily than PMAE due to squaring the errors. Most fitting algorithms (e.g., least squares) minimize squared errors, so PMSE/PRMSE often align better with the model’s optimization criterion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Interval forecast metrics\nTo assess the quality of interval forecasts from model \\(m\\), compute the empirical coverage, \\(\\hat{C}_m\\), as the proportion of observations in the testing set that are within – covered by – corresponding prediction intervals for a given confidence \\(C\\), e.g., \\(C = 0.95\\) or 95%): \\[\n\\hat{C}_m = k^{-1}\\sum_{t=n+1}^{n+k} \\mathbf{1}\\{Y_t \\in PI_t^{(m)}\\},\n\\] where \\(PI_t^{(m)} = \\left[PI_{t,lower}^{(m)}, PI_{t,upper}^{(m)}\\right]\\) is the prediction interval from model \\(m\\) at time \\(t\\), and \\(\\mathbf{1}\\{ \\cdot \\}\\) is the indicator function, it equals 1 if the condition is true and 0 otherwise. To select the best coverage, one can calculate the absolute differences between each empirical coverage \\(\\hat{C}_m\\) and the nominal coverage \\(C\\): \\[\n\\Delta_m = |\\hat{C}_m - C|.\n\\] Hence, we select the model with the smallest \\(\\Delta_m\\), not the largest coverage \\(\\hat{C}_m\\).\nAdditionally, compute the average interval width \\[\n\\text{Width}_m = k^{-1}\\sum_{t=n+1}^{n+k} \\left(PI_{t,upper}^{(m)} - PI_{t,lower}^{(m)}\\right)\n\\] and prefer models with narrower intervals, all else equal.\nPrediction intervals are well-calibrated if their empirical coverage is close to \\(C\\) (more important) while the intervals are not too wide (less important). The trade-off between coverage and width is crucial: overly wide intervals may achieve nominal coverage but provide little practical value.\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to obtain different prediction intervals from the same model. For example, we can calculate prediction intervals based on normal, t, and bootstrapped distributions. In this case, point forecasts are the same, but interval forecasts differ.\n\n\n\n\n\n\n\n\nNoteExample: Insurance claims forecasting\n\n\n\nWe evaluate forecasts of weekly home insurance data using point and interval metrics using alternative models: seasonal naive, ARIMA, and GAMLSS. For GAMLSS (developed in Section 9.5), we develop ex-post forecasts using the known precipitation data for the forecast week, and ex-ante forecasts where we predict precipitation with a simple univariate model. Overall, we compare four combinations of models and forecasts.\nFor the cross-validation scheme, we use an expanding window with weekly refits and 4-week-ahead forecasts. With 10 years of data, we use the first 9 years for initial training and the last 1 year for testing.\n\nlibrary(gamlss)\n\n# Read and prepare a weekly time series\nlogconstant &lt;- 1\nY &lt;- read.csv(\"data/insurance_weekly.csv\") %&gt;%\n    dplyr::select(Claims, Precipitation) %&gt;% \n    mutate(Precipitation_lag1 = dplyr::lag(Precipitation, 1),\n           Week = 1:n(),\n           Year = rep(2002:2011, each = 52),\n           Claims_ln = log(Claims + logconstant)) %&gt;% \n    mutate(Claims_ln_lag1 = dplyr::lag(Claims_ln, 1))\nn &lt;- nrow(Y)\n\n# Set the forecast horizon (4 weeks ahead)\nh_target &lt;- 4\n\n# Set the initial training size (the first 9 years)\nn_train_initial &lt;- 9 * 52\n\n# Set confidence level for prediction intervals\nC &lt;- 0.95\nlower_p &lt;- (1 - C) / 2\nupper_p &lt;- 1 - lower_p\n\n# Initialize storage for performance metrics\ne_naive_h &lt;- e_arima_h &lt;- e_gamlss_xp_h &lt;- e_gamlss_xa_h &lt;- matrix(NA, nrow = 0, ncol = h_target)\nc_naive_h &lt;- c_arima_h &lt;- c_gamlss_xp_h &lt;- c_gamlss_xa_h &lt;- matrix(NA, nrow = 0, ncol = h_target)\nw_naive_h &lt;- w_arima_h &lt;- w_gamlss_xp_h &lt;- w_gamlss_xa_h &lt;- matrix(NA, nrow = 0, ncol = h_target)\n\n# Loop over time points with seq(); can change the argument \"by\" to skip some points\nfor (i in seq(n_train_initial, n - h_target, by = 1)) {\n    # 1. Training and testing sets\n    Y_train &lt;- Y[1:i, ]\n    Y_train_noNA &lt;- na.omit(Y_train)\n    y_train &lt;- ts(Y_train$Claims, frequency = 52)\n    Y_test &lt;- Y[(i + 1):(i + h_target), ]\n    y_test &lt;- Y_test$Claims\n    \n    # 2. Fit models\n    m_arima &lt;- forecast::auto.arima(y_train)\n    m_gamlss &lt;- gamlss(Claims ~ ps(Precipitation) + Week + ps(Precipitation_lag1) + Claims_ln_lag1\n                       ,sigma.formula = ~ps(Week)\n                       ,family = NBI\n                       ,control = gamlss.control(c.crit = 0.01, trace = FALSE)\n                       ,data = Y_train_noNA)\n    \n    # 3. Forecasts\n    # Seasonal naive forecast (\"climatology\" for the number of claims)\n    fc_naive &lt;- forecast::snaive(y_train, level = C * 100, h = h_target)\n    \n    # ARIMA forecast\n    fc_arima &lt;- forecast::forecast(m_arima, h = h_target, level = C * 100)\n    \n    # GAMLSS ex-post forecast, see ?gamlss::predict.gamlss\n    pred_mu &lt;- predict(m_gamlss, \n                       newdata = Y_test, \n                       what = \"mu\", \n                       type = \"response\")\n    pred_sigma &lt;- predict(m_gamlss, \n                          newdata = Y_test, \n                          what = \"sigma\", \n                          type = \"response\")\n    lower_bound &lt;- qNBI(lower_p, mu = pred_mu, sigma = pred_sigma)\n    upper_bound &lt;- qNBI(upper_p, mu = pred_mu, sigma = pred_sigma)\n    fc_gamlss_xp &lt;- data.frame(mean = pred_mu,\n                               lower = lower_bound,\n                               upper = upper_bound)\n    \n    # GAMLSS ex-ante forecast\n    # Simple precipitation forecast: use naive seasonal forecast\n    last_precip &lt;- tail(Y_train$Precipitation, 1)\n    new_precip &lt;- forecast::snaive(ts(Y_train$Precipitation, frequency = 52), \n                                   h = h_target)$mean %&gt;% \n        as.vector()\n    Y_test_exante &lt;- Y_test %&gt;%\n        mutate(Precipitation = new_precip) %&gt;% \n        mutate(Precipitation_lag1 = dplyr::lag(Precipitation, 1, default = last_precip))\n    # Since we have the lagged claims as one of the predictors (unknown when forecasting),  \n    # we need to do recursive forecasts for GAMLSS ex-ante\n    lower_bound_xa &lt;- upper_bound_xa &lt;- pred_mu_xa &lt;- rep(NA, h_target)\n    for (j in 1:h_target) {\n        pred_mu_1 &lt;- predict(m_gamlss, \n                             newdata = Y_test_exante[j, , drop = FALSE],\n                             what = \"mu\", \n                             type = \"response\")\n        pred_sigma_1 &lt;- predict(m_gamlss, \n                              newdata = Y_test_exante[j, , drop = FALSE], \n                              what = \"sigma\", \n                              type = \"response\")\n        pred_mu_xa[j] &lt;- pred_mu_1\n        lower_bound_xa[j] &lt;- qNBI(lower_p, mu = pred_mu_1, sigma = pred_sigma_1)\n        upper_bound_xa[j] &lt;- qNBI(upper_p, mu = pred_mu_1, sigma = pred_sigma_1)\n        # Update lagged claims for next step\n        if (j &lt; h_target) {\n            Y_test_exante$Claims_ln_lag1[j + 1] &lt;- log(pred_mu_1 + logconstant)\n        }\n    }\n    fc_gamlss_xa &lt;- data.frame(mean = pred_mu_xa,\n                               lower = lower_bound_xa,\n                               upper = upper_bound_xa)\n    \n    # 4.1 Forecast errors\n    e_naive_h   &lt;- rbind(e_naive_h, y_test - fc_naive$mean)\n    e_arima_h &lt;- rbind(e_arima_h, y_test - fc_arima$mean)\n    e_gamlss_xp_h &lt;- rbind(e_gamlss_xp_h, y_test - fc_gamlss_xp$mean)\n    e_gamlss_xa_h &lt;- rbind(e_gamlss_xa_h, y_test - fc_gamlss_xa$mean)\n    \n    # 4.2 Forecast coverage\n    c_naive_h &lt;- rbind(c_naive_h, \n                         (y_test &gt;= as.vector(fc_naive$lower)) & \n                           (y_test &lt;= as.vector(fc_naive$upper)))\n    c_arima_h &lt;- rbind(c_arima_h, \n                       (y_test &gt;= as.vector(fc_arima$lower)) & \n                           (y_test &lt;= as.vector(fc_arima$upper)))\n    c_gamlss_xp_h &lt;- rbind(c_gamlss_xp_h, \n                           (y_test &gt;= fc_gamlss_xp$lower) & (y_test &lt;= fc_gamlss_xp$upper))\n    c_gamlss_xa_h &lt;- rbind(c_gamlss_xa_h, \n                           (y_test &gt;= fc_gamlss_xa$lower) & (y_test &lt;= fc_gamlss_xa$upper))\n    \n    # 4.3 Forecast interval width\n    w_naive_h &lt;- rbind(w_naive_h, as.vector(fc_naive$upper) - as.vector(fc_naive$lower))\n    w_arima_h &lt;- rbind(w_arima_h, as.vector(fc_arima$upper) - as.vector(fc_arima$lower))\n    w_gamlss_xp_h &lt;- rbind(w_gamlss_xp_h, fc_gamlss_xp$upper - fc_gamlss_xp$lower)\n    w_gamlss_xa_h &lt;- rbind(w_gamlss_xa_h, fc_gamlss_xa$upper - fc_gamlss_xa$lower)\n    \n}\n\n\nCode# RMSE for each model and horizon\ndata.frame(\n    RMSE_naive_h = apply(e_naive_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE))),\n    RMSE_arima_h = apply(e_arima_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE))),\n    RMSE_gamlss_xp_h = apply(e_gamlss_xp_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE))),\n    RMSE_gamlss_xa_h = apply(e_gamlss_xa_h, 2, function(x) sqrt(mean(x^2, na.rm = TRUE)))\n)\n\n#&gt;   RMSE_naive_h RMSE_arima_h RMSE_gamlss_xp_h RMSE_gamlss_xa_h\n#&gt; 1         8.12         7.44             6.60             7.93\n#&gt; 2         8.13         7.90             6.65             8.43\n#&gt; 3         8.14         7.90             6.67             8.50\n#&gt; 4         7.78         7.58             6.38             8.16\n\n\nThe lowest RMSE across all horizons belongs to the ex-post forecasts from the GAMLSS, however, these forecasts are unfairly better because they use future precipitation data and future (lagged) claim counts. The ex-ante GAMLSS forecasts have considerably higher RMSE – even higher than the naive seasonal model. The RMSE of ARIMA model were second-lowest.\n\nCode# Coverage for each model and horizon\n(COV &lt;- data.frame(\n    Coverage_naive_h = apply(c_naive_h, 2, function(x) mean(x, na.rm = TRUE)),\n    Coverage_arima_h = apply(c_arima_h, 2, function(x) mean(x, na.rm = TRUE)),\n    Coverage_gamlss_xp_h = apply(c_gamlss_xp_h, 2, function(x) mean(x, na.rm = TRUE)),\n    Coverage_gamlss_xa_h = apply(c_gamlss_xa_h, 2, function(x) mean(x, na.rm = TRUE))\n))\n\n#&gt;   Coverage_naive_h Coverage_arima_h Coverage_gamlss_xp_h Coverage_gamlss_xa_h\n#&gt; 1             0.98            0.959                0.959                0.939\n#&gt; 2             0.98            0.959                0.939                0.918\n#&gt; 3             0.98            0.959                0.939                0.918\n#&gt; 4             0.98            0.959                0.939                0.918\n\nCode# Difference from the nominal level\nabs(COV - C)\n\n#&gt;   Coverage_naive_h Coverage_arima_h Coverage_gamlss_xp_h Coverage_gamlss_xa_h\n#&gt; 1           0.0296          0.00918              0.00918               0.0112\n#&gt; 2           0.0296          0.00918              0.01122               0.0316\n#&gt; 3           0.0296          0.00918              0.01122               0.0316\n#&gt; 4           0.0296          0.00918              0.01122               0.0316\n\n\nThe empirical coverage of ARIMA intervals is the closest to the nominal 95% level, followed by ex-post GAMLSS and the naive model. The ex-ante GAMLSS intervals are under-covering the true observations by about 3 percentage points.\n\nCode# Average interval width calculation for each model and horizon\ndata.frame(\n    Width_naive_h = apply(w_naive_h, 2, function(x) mean(x, na.rm = TRUE)),\n    Width_arima_h = apply(w_arima_h, 2, function(x) mean(x, na.rm = TRUE)),\n    Width_gamlss_xp_h = apply(w_gamlss_xp_h, 2, function(x) mean(x, na.rm = TRUE)),\n    Width_gamlss_xa_h = apply(w_gamlss_xa_h, 2, function(x) mean(x, na.rm = TRUE))\n)\n\n#&gt;   Width_naive_h Width_arima_h Width_gamlss_xp_h Width_gamlss_xa_h\n#&gt; 1          46.7          31.7              16.4              16.1\n#&gt; 2          46.7          32.0              16.5              16.7\n#&gt; 3          46.7          32.0              16.6              17.1\n#&gt; 4          46.7          32.0              16.6              17.2\n\n\nThe GAMLSS intervals of two types are the narrowest, followed by ARIMA, then the naive model. Except the naive model, the uncertainty increases with the forecast horizon for all models, as expected.\nOverall, while the GAMLSS provides explanatory power for the incurred weather-related home insurance losses, the model is not that suitable for out-of-sample forecasting when future precipitation is unknown. We attempted to forecast precipitation with a simple seasonal naive model, which likely contributed to the poor performance of the ex-ante GAMLSS forecasts. Perhaps, more sophisticated precipitation forecasts (not necessarily by us, can be from an external dataset of weather predictions) could improve the ex-ante GAMLSS results, but this remains to be tested. For truly out-of-sample forecasting, ARIMA model was the best among the considered alternatives. ARIMA delivered RMSE just below that of the naive model, but the coverage and width of ARIMA prediction intervals were considerably better.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forecasting and Model Evaluation</span>"
    ]
  },
  {
    "objectID": "l12_forecast.html#conclusion",
    "href": "l12_forecast.html#conclusion",
    "title": "10  Forecasting and Model Evaluation",
    "section": "\n10.4 Conclusion",
    "text": "10.4 Conclusion\nThis lecture covered comprehensive evaluation of time series forecasting models:\n\nWe illustrated how to assess point forecasts via RMSE, MAE, and interval forecasts via empirical coverage and average widths. See forecast::accuracy() for automatic calculation of some of these and other metrics.\nWe demonstrated data leakage by comparing ex-post and ex-ante forecasts by GAMLSS and compared it against univariate methods.\nCross-validation schemes should mirror operational use: match the target horizon, refit cadence, and predictor availability to deployment conditions.\nPrefer rolling-origin cross-validation to reduce model selection bias and to diagnose overfitting; keep models parsimonious (AICc helps) and always check that assumptions are plausible for future deployment.\nWhen multiple models perform similarly on point metrics, interval calibration and operational constraints (computation, interpretability) guide the final choice.\n\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton, FL, USA\n\n\nHastie TJ, Tibshirani RJ, Friedman JH (2009) The elements of statistical learning: Data mining, inference, and prediction, 2nd edn. Springer, New York, NY, USA",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forecasting and Model Evaluation</span>"
    ]
  },
  {
    "objectID": "l13_spectral.html",
    "href": "l13_spectral.html",
    "title": "\n11  Spectral Analysis\n",
    "section": "",
    "text": "11.1 Introduction\nThe goal of this lecture is to learn how to view time series from the frequency perspective. You should be able to recognize features of time series from a periodogram similar to how we did it using plots of the autocorrelation function (ACF).\nObjectives\nReading materials\nAudio overview\nBy now, we have been working in the time domain, such that our analysis could be seen as a regression of the present on the past (for example, ARIMA models). We have been using many time series plots with time on the \\(x\\)-axis.\nAn alternative approach is to analyze time series in a spectral domain, such that use a regression of present on a linear combination of sine and cosine functions. In this type of analysis, we often use periodogram plots, with frequency or period on the \\(x\\)-axis.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spectral Analysis</span>"
    ]
  },
  {
    "objectID": "l13_spectral.html#regression-on-sinusoidal-components",
    "href": "l13_spectral.html#regression-on-sinusoidal-components",
    "title": "\n11  Spectral Analysis\n",
    "section": "\n11.2 Regression on sinusoidal components",
    "text": "11.2 Regression on sinusoidal components\nThe simplest form of spectral analysis consists of regression on a periodic component: \\[\nY_t = A\\cos \\omega t + B \\sin \\omega t + C + \\epsilon_t,\n\\tag{11.1}\\] where \\(t =1,\\dots,T\\) and \\(\\epsilon_t \\sim \\mathrm{WN}(0, \\sigma^2)\\). Without loss of generality, we assume \\(0 \\leqslant \\omega \\leqslant \\pi\\). In fact, for discrete data, frequencies outside this range are aliased into this range. For example, suppose we have a negative frequency \\(\\omega = -\\delta\\), where \\(0 \\leqslant \\delta \\leqslant \\pi\\). Then, for any integer \\(t\\), \\[\n\\cos(\\omega t) = \\cos(-\\delta t) = \\cos(\\delta t).\n\\] because the cosine function is an even function (meaning \\(\\cos(−x) = \\cos(x)\\)).         Hence, a sampled sinusoid with a negative frequency \\(\\omega = -\\delta\\) appears to coincide with a sinusoid with a positive frequency \\(\\delta\\) in the interval \\([0, \\pi]\\).\n\n\n\n\n\n\nNote\n\n\n\nThe term \\(A\\cos \\omega t + B \\sin \\omega t\\) is a periodic function with the period \\(2\\pi / \\omega\\). The period \\(2\\pi / \\omega\\) represents the number of time units that it takes for the function to take the same value again, i.e., to complete a cycle. The frequency, measured in cycles per time unit, is given by the inverse \\(\\omega / (2 \\pi)\\). The angular frequency, measured in radians per time unit, is given by \\(\\omega\\). Because of its convenience, the angular frequency \\(\\omega\\) will be used to describe the periodicity of the function, and its name is shortened to frequency when there is no danger of confusion.\n\n\nConsider monthly data that exhibit a 12-month seasonality. Hence, the period \\(2\\pi / \\omega = 12\\), which implies the angular frequency \\(\\omega = \\pi / 6\\). The frequency, measured in cycles per time unit, is given by the inverse \\[\n\\frac{\\omega}{2\\pi} = \\frac{1}{12} \\approx 0.08.\n\\]\nThe formulas to estimate parameters of regression in Equation 11.1 take a much simpler form if \\(\\omega\\) is one of the Fourier frequencies, defined by \\[\n\\omega_j=\\frac{2\\pi j}{T}, \\quad  j=0,\\dots, \\frac{T}{2},\n\\] then \\[\n\\begin{split}\n\\hat{A}&=\\frac{2}{T}\\sum_t Y_t\\cos \\omega_jt,\\\\\n\\hat{B}&=\\frac{2}{T}\\sum_t Y_t\\sin \\omega_jt,\\\\\n\\hat{C}&=\\overline{Y}=\\frac{1}{T}\\sum_tY_t.\n\\end{split}\n\\]\nA suitable way of testing the significance of the sinusoidal component with frequency \\(\\omega_j\\) is using its contribution to the sum of squares \\[\nR_T(\\omega_j)=\\frac{T}{2}\\left( \\hat{A}^2+\\hat{B}^2 \\right).\n\\] If the \\(\\epsilon_t \\sim N(0, \\sigma^2)\\), then it follows that \\(\\hat{A}\\) and \\(\\hat{B}\\) are also independent normal, each with the variance \\(2\\sigma^2/T\\), so under the null hypothesis of \\(A = B = 0\\) we find that \\[\\frac{R_T(\\omega_j)}{\\sigma^2}\\sim \\chi_2^2\\] or equivalently that \\(R_T(\\omega_j)/(2\\sigma^2)\\) has an exponential distribution with mean 1. The above theory can be extended to the simultaneous estimation of several periodic components.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spectral Analysis</span>"
    ]
  },
  {
    "objectID": "l13_spectral.html#periodogram",
    "href": "l13_spectral.html#periodogram",
    "title": "\n11  Spectral Analysis\n",
    "section": "\n11.3 Periodogram",
    "text": "11.3 Periodogram\nThe Fourier transform uses Fourier series, such as the pairs of sines and cosines with different periods, to describe the frequencies present in the original time series.\nThe Fourier transform applied to an equally-spaced time series \\(Y_t\\) (where \\(t = 1,\\dots,T\\)) is also called the discrete Fourier transform (DFT) because the time is discrete (not the values \\(Y_t\\)).\nTo reduce the computational complexity of DFT and speed up the computations, one of the fast Fourier transform (FFT) algorithms is typically used.\nThe results of the Fourier transform are shown in a periodogram that describes the spectral properties of the signal.\nThe periodogram is defined as \\[\nI_T(\\omega) = \\frac{1}{2\\pi T}\\left| \\sum_{t=1}^T Y_te^{i\\omega t} \\right|^2,\n\\] which is an approximately unbiased estimator of the spectral density \\(f\\).\nSome undesirable features of the periodogram:\n\n\n\\(I_T(\\omega)\\) for fixed \\(\\omega\\) is not a consistent estimate of \\(f(\\omega)\\), since \\[\nI_T(\\omega_j) \\sim \\frac{f(\\omega_j)}{2} \\chi^2_2.\n\\] Therefore, the variance of \\(f^2(\\omega)\\) does not tend to 0 as \\(T \\rightarrow \\infty\\).\nThe independence of periodogram ordinates at different Fourier frequencies suggests that the sample periodogram plotted as a function of \\(\\omega\\) will be extremely irregular.\n\nSuppose that \\(\\gamma(h)\\) is the autocovariance function of a stationary process and that \\(f(\\omega)\\) is the spectral density for the same process (\\(h\\) is the time lag and \\(\\omega\\) is the frequency). The autocovariance \\(\\gamma(h)\\) and the spectral density \\(f(\\omega)\\) are related: \\[\n\\gamma(h) = \\int_{-1/2}^{1/2} e^{2\\pi i \\omega h} f(\\omega) d \\omega,\n\\] and \\[\nf(\\omega) = \\sum_{h=-\\infty}^{+\\infty} \\gamma(h) e^{-2\\pi i \\omega h}.\n\\]\nIn the language of advanced calculus, the autocovariance and spectral density are Fourier transform pairs. These Fourier transform equations show that there is a direct link between the time domain representation and the frequency domain representation of a time series.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spectral Analysis</span>"
    ]
  },
  {
    "objectID": "l13_spectral.html#smoothing",
    "href": "l13_spectral.html#smoothing",
    "title": "\n11  Spectral Analysis\n",
    "section": "\n11.4 Smoothing",
    "text": "11.4 Smoothing\nThe idea behind smoothing is to take weighted averages over neighboring frequencies to reduce the variability associated with individual periodogram values. However, such an operation necessarily introduces some bias into the estimation procedure. Theoretical studies focus on the amount of smoothing that is required to obtain an optimum trade-off between bias and variance. In practice, this usually means that the choice of a kernel and amount of smoothing is somewhat subjective.\nThe main form of a smoothed estimator is given by \\[\n\\hat{f}(\\lambda) = \\int^{\\pi}_{-\\pi}\\frac{1}{h}K\\left( \\frac{\\omega-\\lambda}{h}\\right)I_T(\\omega)d\\omega,\n\\] where \\(I_T(\\cdot)\\) is the periodogram based on \\(T\\) observations, \\(K(\\cdot)\\) is a kernel function, and \\(h\\) is the bandwidth. We usually take \\(K(\\cdot)\\) to be a nonnegative function, symmetric about 0 and integrating to 1. Thus, any symmetric density, such as the normal density, will work. In practice, however, it is more usual to take a kernel of finite range, such as the Epanechnikov kernel \\[\nK(x)=\\frac{3}{4\\sqrt{5}}\\left(1-\\frac{x^2}{5} \\right), \\quad -\\sqrt{5}\\leqslant x \\leqslant \\sqrt{5},\n\\] which is 0 outside the interval \\([-\\sqrt{5}, \\sqrt{5}]\\). This choice of kernel function has some optimality properties. However, in practice, this optimality is less important than the choice of bandwidth \\(h\\), which effectively controls the range over which the periodogram is smoothed.\nThere are some additional difficulties with the performance of the sample periodogram in the presence of a sinusoidal variation whose frequency is not one of the Fourier frequencies. This effect is known as leakage. The reason of leakage is that we always consider a truncated periodogram. Truncation implicitly assumes that the time series is periodic with a period \\(T\\), which, of course, is not always true. So we artificially introduce non-existent periodicities into the estimated spectrum, i.e., cause ‘leakage’ of the spectrum. (If the time series is perfectly periodic over \\(T\\) then there is no leakage.) The leakage can be treated using an operation of tapering on the periodogram, i.e., by choosing appropriate periodogram windows.\n\n\n\n\n\n\nNote\n\n\n\nWhen we work with periodograms, we lose all phase (relative location/time origin) information: the periodogram will be the same if all the data were circularly rotated to a new time origin, i.e., the observed data are treated as perfectly periodic.\n\n\nAnother popular choice to smooth a periodogram is the Daniell kernel (with parameter \\(m\\)). For a time series, it is a centered moving average of values between the times \\(t - m\\) and \\(t  + m\\) (inclusive). For example, the smoothing formula for a Daniell kernel with \\(m = 2\\) is \\[\n\\begin{split}\n\\hat{x}_t &= \\frac{x_{t-2}+x_{t-1}+x_t + x_{t+1}+x_{t+2}}{5} \\\\\n&= 0.2x_{t-2} + 0.2x_{t-1} +0.2x_t + 0.2x_{t+1} +0.2x_{t+2}.\n\\end{split}\n\\]\nThe weighting coefficients for a Daniell kernel with \\(m = 2\\) can be checked using the function kernel(). In the output, the indices in coef[ ] refer to the time difference from the center of the average at the time \\(t\\).\n\nkernel(\"daniell\", m = 2)\n\n#&gt; Daniell(2) \n#&gt; coef[-2] = 0.2\n#&gt; coef[-1] = 0.2\n#&gt; coef[ 0] = 0.2\n#&gt; coef[ 1] = 0.2\n#&gt; coef[ 2] = 0.2\n\n\nThe modified Daniell kernel is such that the two endpoints in the averaging receive half the weight that the interior points do. For a modified Daniell kernel with \\(m = 2\\), the smoothing is \\[\n\\begin{split}\n\\hat{x}_t &= \\frac{x_{t-2}+2x_{t-1}+2x_t + 2x_{t+1} + x_{t+2}}{8} \\\\\n& = 0.125x_{t-2} +0.25x_{t-1}+0.25x_t+0.25x_{t+1}+0.125x_{t+2}\n\\end{split}\n\\]\nList the weighting coefficients:\n\nkernel(\"modified.daniell\", m = 2)\n\n#&gt; mDaniell(2) \n#&gt; coef[-2] = 0.125\n#&gt; coef[-1] = 0.250\n#&gt; coef[ 0] = 0.250\n#&gt; coef[ 1] = 0.250\n#&gt; coef[ 2] = 0.125\n\n\nEither the Daniell kernel or the modified Daniell kernel can be convoluted (repeated) so that the smoothing is applied again to the smoothed values. This produces a more extensive smoothing by averaging over a wider time interval. For instance, to repeat a Daniell kernel with \\(m = 2\\) on the smoothed values that resulted from a Daniell kernel with \\(m = 2\\), the formula would be \\[\n\\hat{\\hat{x}}_t = \\frac{\\hat{x}_{t-2}+\\hat{x}_{t-1}+\\hat{x}_t +\\hat{x}_{t+1}+\\hat{x}_{t+2}}{5}.\n\\]\nThe weights for averaging the original data for convoluted Daniell kernels with \\(m = 2\\) in both smooths will be\n\nkernel(\"daniell\", m = c(2, 2))\n\n#&gt; Daniell(2,2) \n#&gt; coef[-4] = 0.04\n#&gt; coef[-3] = 0.08\n#&gt; coef[-2] = 0.12\n#&gt; coef[-1] = 0.16\n#&gt; coef[ 0] = 0.20\n#&gt; coef[ 1] = 0.16\n#&gt; coef[ 2] = 0.12\n#&gt; coef[ 3] = 0.08\n#&gt; coef[ 4] = 0.04\n\nkernel(\"modified.daniell\", m = c(2, 2))\n\n#&gt; mDaniell(2,2) \n#&gt; coef[-4] = 0.0156\n#&gt; coef[-3] = 0.0625\n#&gt; coef[-2] = 0.1250\n#&gt; coef[-1] = 0.1875\n#&gt; coef[ 0] = 0.2188\n#&gt; coef[ 1] = 0.1875\n#&gt; coef[ 2] = 0.1250\n#&gt; coef[ 3] = 0.0625\n#&gt; coef[ 4] = 0.0156\n\n\nThe center values are weighted slightly more heavily in the modified Daniell kernel than in the unmodified one.\nWhen we smooth a periodogram, we are smoothing across frequencies rather than across times.\n\n\n\n\n\n\nNoteExample: Spectral analysis of the airline passenger data\n\n\n\nRecall the airline passenger time series (Figure 3.12) that has an increasing trend and strong multiplicative seasonality.\nThe series should be detrended before a spectral analysis. For demonstration purposes, compute periodogram for the raw data and log-transformed and detrended.\n\n# Fit a quadratic trend to log-transformed data\nt &lt;- as.vector(time(AirPassengers))\nmod &lt;- lm(log10(AirPassengers) ~ poly(t, degree = 2))\nres &lt;- ts(mod$residuals)\nattributes(res) &lt;- attributes(AirPassengers)\n\n# Compute spectra\nspec_raw &lt;- spec.pgram(AirPassengers, detrend = FALSE, plot = FALSE)\nspec_log_detrend &lt;- spec.pgram(res, detrend = FALSE, plot = FALSE)\n\nThe \\(x\\)-axes of the periodograms correspond to \\(\\omega / (2\\pi)\\) or the number of cycles per the time series period specified in the ts object (12 months). If the frequency of 12 was not specified for this time series, the \\(x\\)-axes would be different, and the first spectrum peak would correspond to \\(\\approx 0.08\\).\nFigure 11.1 B shows that the spectrum of the raw data is dominated by low frequencies (trend). After detrending the data (Figure 11.1 C), the spectrum at frequency 1 (meaning one cycle per year) is the dominant one.\n\nCodepAirPassengers &lt;- forecast::autoplot(AirPassengers, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Airline passengers (thousand)\") +\n    ggtitle(\"Raw series\")\np2 &lt;- data.frame(Spectrum = spec_raw$spec, \n                 Frequency = spec_raw$freq) %&gt;% \n    ggplot(aes(x = Frequency, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_line() + \n    ggtitle(\"\")\np3 &lt;- forecast::autoplot(res, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Airline passengers residuals (lg[thousand])\") + \n    ggtitle(\"Detrended and log-transformed series\")\np4 &lt;- data.frame(Spectrum = spec_log_detrend$spec, \n                 Frequency = spec_log_detrend$freq) %&gt;% \n    ggplot(aes(x = Frequency, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_line() + \n    ggtitle(\"\")\n(pAirPassengers + p2) / (p3 + p4) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 11.1: Monthly AirPassengers time series, log-transformed and detrended, and the corresponding fast Fourier transforms.\n\n\n\n\nNote that the function spec.pgram() has the option of removing a simpler (linear) trend and showing the results as a base-R plot. The confidence band in the upper right corner of this plot helps to identify the statistical significance of the peaks (Figure 11.2).\n\nCodepar(mfrow = c(2, 2))\nspec.pgram(res, spans = c(3))\nspec.pgram(res, spans = c(3, 3))\nspec.pgram(res, spans = c(7, 7))\nspec.pgram(res, spans = c(11, 11))\n\n\n\n\n\n\nFigure 11.2: Smoothed periodograms of monthly log-transformed and detrended AirPassengers time series.\n\n\n\n\nAnother method of testing the reality of a peak is to look at its harmonics. It is extremely unlikely that a true cycle will be shaped perfectly as a sine curve, hence at least a few of the first harmonics will show up as well. For example, in monthly data with annual seasonality (period of 12 months), we often observe peaks at 6, 4, and 3 months. This is exactly the pattern that we see in the figures above.\nWe can also approximate our data with an AR model and then plot the approximating periodogram of the AR model (Figure 11.3).\n\nCodespectrum(res, method = \"ar\")\n\n\n\n\n\n\nFigure 11.3: Periodogram of AR process approximating the monthly log-transformed and detrended AirPassengers time series.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spectral Analysis</span>"
    ]
  },
  {
    "objectID": "l13_spectral.html#periodogram-for-unequally-spaced-time-series",
    "href": "l13_spectral.html#periodogram-for-unequally-spaced-time-series",
    "title": "\n11  Spectral Analysis\n",
    "section": "\n11.5 Periodogram for unequally-spaced time series",
    "text": "11.5 Periodogram for unequally-spaced time series\nUnequally/unevenly-spaced time series or gaps in observations (missing data) can be handled with the Lomb–Scargle periodogram calculation. This method was originally developed for the analysis of unevenly-spaced astronomical signals (Lomb 1976; Scargle 1982) but found application in many other domains, including environmental science (e.g., Ruf 1999; Campos et al. 2008; Siskey et al. 2016).\n\n\n\n\n\n\nNoteExample: Ammonium concentration in wastewater system\n\n\n\nFor example, consider a time series imputeTS::tsNH4 of ammonium (NH\\(_4\\)) concentration in a wastewater system (Figure 11.4). This time series contains observations from regular 10-minute intervals, but some of the observations are missing.\n\nCodeforecast::autoplot(imputeTS::tsNH4) +\n    xlab(\"Day\") +\n    ylab(bquote(NH[4]~concentration))\n\n\n\n\n\n\nFigure 11.4: Time series of ammonium concentration in a wastewater system.\n\n\n\n\nSave the data in a format acceptable by the Lomb–Scargle function.\n\nD &lt;- data.frame(NH4 = as.vector(imputeTS::tsNH4),\n                Time = as.vector(time(imputeTS::tsNH4)))\n\nIf needed, na.omit(D) can be used to remove the rows with missing values.\nFigure 11.5 and Figure 11.6 show periodograms calculated based on these data. Note that the function lomb::lsp() has arguments adjusting the span of the frequencies and significance level.\n\nCodepar(mfrow = c(2, 2))\nlomb::lsp(D$NH4, times = D$Time, type = \"frequency\", alpha = 0.05, main = \"\")\n\n\n\n\n\n\nFigure 11.5: Lomb–Scargle periodograms computed for the unequally-spaced time series of ammonium concentration.\n\n\n\n\n\nCodelomb::lsp(D$NH4, times = D$Time, type = \"period\", alpha = 0.05, main = \"\")\n\n\n\n\n\n\nFigure 11.6: Lomb–Scargle periodograms computed for the unequally-spaced time series of ammonium concentration.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spectral Analysis</span>"
    ]
  },
  {
    "objectID": "l13_spectral.html#frequency-estimation-in-time",
    "href": "l13_spectral.html#frequency-estimation-in-time",
    "title": "\n11  Spectral Analysis\n",
    "section": "\n11.6 Frequency estimation in time",
    "text": "11.6 Frequency estimation in time\nThe assumption of a whole time series having a constant spectrum might be very limiting for the analysis of long time series. An estimation of frequencies locally in time allows us to study how the spectrum changes in time, and also detect smaller (short-lived) signals that would be averaged out otherwise. A straightforward solution is to separate the time series into windows (sub-periods) and estimate a spectrum in each such window.\nThe windowed analysis makes sense when in each window we have enough observations to estimate the frequencies. As a guide, we should have at least two observations per period to be able to represent the signal in the discrete Fourier transform. For example, we cannot estimate the seasonality if we sample once per year – this is our sampling rate or sampling frequency \\(F_s\\). To start identifying seasonal periodicity, our sampling rate should be at least 2 samples per year. The highest frequency we can work with is limited by the Nyquist frequency \\[\nF_N = \\frac{F_s}{2},\n\\] which is half of the sampling frequency.\nAfter applying the FFT in each window, the results can be visualized in a spectrogram. The spectrogram combines information from multiple periodograms: it shows time on the \\(x\\)-axis, frequency on the \\(y\\)-axis, and the corresponding spectrum power as the color.\n\n\n\n\n\n\nNoteExample: Spectrograms for the NAO\n\n\n\nConsider a long time series of the North Atlantic Oscillation (NAO) index obtained from an ensemble of 100 model-constrained NAO reconstructions (Figure 11.7).\n\nCodeNAOdf &lt;- readr::read_csv(\"data/NAO.csv\", skip = 12, show_col_types = FALSE) %&gt;% \n    rename(NAOproxy = nao_mc_mean) %&gt;% \n    select(Year, NAOproxy) %&gt;% \n    arrange(Year)\nNAO &lt;- ts(NAOdf$NAOproxy, start = NAOdf$Year[1])\nspec_raw &lt;- spec.pgram(NAO, detrend = FALSE, plot = FALSE, spans = 3)\n\n# Find the frequency with the max power\nfmax &lt;- spec_raw$freq[which.max(spec_raw$spec)]\n\n\n\nCodep1 &lt;- forecast::autoplot(NAO) +\n    xlab(\"Year\") +\n    ylab(\"NAO\")\np2 &lt;- data.frame(Spectrum = spec_raw$spec, \n                 Frequency = spec_raw$freq) %&gt;% \n    ggplot(aes(x = Frequency, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_vline(xintercept = fmax, col = \"blue\", lty = 2) + \n    geom_line() + \n    ggtitle(\"\")\np3 &lt;- data.frame(Spectrum = spec_raw$spec, \n                 Period = 1/spec_raw$freq) %&gt;% \n    ggplot(aes(x = Period, y = Spectrum)) +\n    scale_y_continuous(trans = 'log10') +\n    geom_vline(xintercept = 1/fmax, col = \"blue\", lty = 2) +\n    geom_line() + \n    ggtitle(\"\")\np1 / (p2 + p3) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure 11.7: North Atlantic Oscillation (NAO) winter index (December, January, and February) calculated as the mean of 100 model-constrained NAO reconstructions, along with its smoothed periodograms. The dashed lines denote the maximal magnitude of the spectrum.\n\n\n\n\nThis is an annual time series, hence the sampling frequency \\(F_s = 1\\). From the global FFT results, the frequency with the maximal power is 0.019 year\\(^{-1}\\), which is equivalent to the period of about 53.3 years. We will set the window for the FFT and calculate the spectrogram. Note that we can use overlapping windows for a smoother picture.\n\n# Set the window size (years), for applying the FFT in each window\nwindow_size = 30\n\n# Compute the spectrogram\nSP &lt;- signal::specgram(x = NAO,\n                       n = window_size,\n                       overlap = window_size/3,\n                       Fs = 1)\n\nSee the spectrograms in Figure 11.8 and compare them with the time series plot in Figure 11.7 A.\n\nCodepar(mfrow = c(1, 2))\n\n# Plot the spectrogram without decorations\nprint(SP, main = \"A) Raw results\")\n\n# Discard phase information\nP &lt;- abs(SP$S)\n\n# Normalize the spectrum to the range [0, 1]\nP &lt;- P - min(P)\nP &lt;- P/max(P)\n\n# Extract time\nYears &lt;- time(NAO)[SP$t]\n\n# Plot the spectrogram after processing\noce::imagep(x = Years,\n            y = SP$f,\n            z = t(P),\n            col = oce::oce.colorsViridis,\n            ylab = expression(Frequency~(y^-1 )),\n            xlab = \"Year\",\n            main = \"B) After normalization and adding colors\",\n            drawPalette = TRUE,\n            decimate = FALSE)\n\n\n\n\n\n\nFigure 11.8: Spectrograms for North Atlantic Oscillation (NAO) winter index.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spectral Analysis</span>"
    ]
  },
  {
    "objectID": "l13_spectral.html#conclusion",
    "href": "l13_spectral.html#conclusion",
    "title": "\n11  Spectral Analysis\n",
    "section": "\n11.7 Conclusion",
    "text": "11.7 Conclusion\nFor challenging problems, smoothing, multitapering, linear filtering, (repeated) pre-whitening, and Lomb–Scargle can be used together. Beware that aperiodic but autoregressive processes produce peaks in spectral densities. Harmonic analysis is a complicated art rather than a straightforward procedure.\nIt is extremely difficult to derive the significance of a weak periodicity from harmonic analysis. Do not believe analytical estimates (e.g., exponential probability), as they rarely apply to real data. It is essential to make simulations, typically permuting or bootstrapping the data keeping the observing times fixed. Simulations of the final model with the observation times are also advised.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spectral Analysis</span>"
    ]
  },
  {
    "objectID": "l13_spectral.html#appendix",
    "href": "l13_spectral.html#appendix",
    "title": "\n11  Spectral Analysis\n",
    "section": "\n11.8 Appendix",
    "text": "11.8 Appendix\nWavelet analysis\nAn alternative to the windowed FFT is the wavelet analysis, which also estimates the strength of time series variations for different frequencies and times. Wavelet is a ‘small wave’ or function \\(\\psi(t)\\) approximating the variations. Popular wavelet functions are Meyer, Morlet, and Mexican hat.\nFigure 11.9 shows the results of using the Morlet wavelet to process the NAO time series.\n\nCodelibrary(dplR)\nwv &lt;- morlet(y1 = NAOdf$NAOproxy, x1 = NAOdf$Year)\nlevs &lt;- quantile(wv$Power, probs = c(0, 0.5, 0.75, 0.9, 0.95, 1))\nwavelet.plot(wv, wavelet.levels = levs)\n\n\n\n\n\n\nFigure 11.9: Wavelet analysis of the North Atlantic Oscillation (NAO) winter index.\n\n\n\n\n\n\n\n\nCampos MC, Costa JL, Quintella BR, et al (2008) Activity and movement patterns of the Lusitanian toadfish inferred from pressure-sensitive data-loggers in the Mira estuary (Portugal). Fisheries Management and Ecology 15:449–458. https://doi.org/10.1111/j.1365-2400.2008.00629.x\n\n\nLomb NR (1976) Least-squares frequency analysis of unequally spaced data. Astrophysics and Space Science 39:447–462. https://doi.org/10.1007/BF00648343\n\n\nNason GP (2008) Wavelet methods in statistics with R. Springer, New York, NY, USA\n\n\nRuf T (1999) The Lomb–Scargle periodogram in biological rhythm research: Analysis of incomplete and unequally spaced time-series. Biological Rhythm Research 30:178–201. https://doi.org/10.1076/brhm.30.2.178.1422\n\n\nScargle JD (1982) Studies in astronomical time series analysis. II – statistical aspects of spectral analysis of unevenly spaced data. Astrophysical Journal 263:835–853. https://doi.org/10.1086/160554\n\n\nShumway RH, Stoffer DS (2017) Time series analysis and its applications with R examples, 4th edn. Springer, New York, NY, USA\n\n\nSiskey MR, Lyubchich V, Liang D, et al (2016) Periodicity of strontium:calcium across annuli further validates otolith-ageing for Atlantic bluefin tuna (Thunnus thynnus). Fisheries Research 177:13–17. https://doi.org/10.1016/j.fishres.2016.01.004",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spectral Analysis</span>"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Examples were calculated in R version 4.5.1 (2025-06-13 ucrt) with an effort to use the most recent versions of R packages.\nThe codes load silently only a few packages:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\ntheme_set(theme_light())\n\nAll other packages are named before the function as in forecast::ggAcf() (this code calls the function ggAcf() from the package forecast) or called immediately before the necessary function use:\n\nlibrary(fable)\nm &lt;- as_tsibble(Y) %&gt;%\n    model(ARIMA(Y ~ 1, ic = \"bic\"))\nreport(m)\n\nThe R packages used in this book include (in alphabetic order):\n\n\nastsa (Stoffer 2025)\n\n\ncaret (Kuhn 2024)\n\n\ndownlit (Wickham 2024)\n\n\ndplR (Bunn et al. 2025)\n\n\ndplyr (Wickham et al. 2023)\n\n\ndynlm (Zeileis 2019)\n\n\nEcdat (Croissant and Graves 2025)\n\n\nfable (O’Hara-Wild et al. 2024)\n\n\nfGarch (Wuertz et al. 2024)\n\n\nFinTS (Graves 2024)\n\n\nfma (Hyndman 2023)\n\n\nforecast (Hyndman et al. 2025)\n\n\nfuntimes (Lyubchich et al. 2023)\n\n\ngamlss (Stasinopoulos and Rigby 2025)\n\n\ngamlss.ggplots (Stasinopoulos et al. 2024)\n\n\ngamlss.util (Stasinopoulos et al. 2016)\n\n\nGGally (Schloerke et al. 2025)\n\n\nggplot2 (Wickham et al. 2025a)\n\n\nggpubr (Kassambara 2025)\n\n\nggtime (O’Hara-Wild et al. 2025)\n\n\nKendall (McLeod 2022)\n\n\nknitr (Xie 2025)\n\n\nlawstat (Gastwirth et al. 2023)\n\n\nlmtest (Hothorn et al. 2022)\n\n\nlomb (Ruf 2024)\n\n\nmgcv (Wood 2025)\n\n\nmgcViz (Fasiolo and Nedellec 2025)\n\n\nnlme (Pinheiro et al. 2025)\n\n\noce (Kelley and Richards 2024)\n\n\npatchwork (Pedersen 2025)\n\n\nplotly (Sievert et al. 2025)\n\n\npracma (Borchers 2023)\n\n\nrandtests (Caeiro and Mateus 2024)\n\n\nreadr (Wickham et al. 2024)\n\n\nrmarkdown (Allaire et al. 2025)\n\n\nsignal (Ligges et al. 2024)\n\n\ntseries (Trapletti and Hornik 2024)\n\n\nTSstudio (Krispin 2023)\n\n\nurca (Pfaff 2024)\n\n\nxml2 (Wickham et al. 2025b)\n\n\n\n\n\n\nAllaire J, Xie Y, Dervieux C, et al (2025) Rmarkdown: Dynamic documents for r. R package version 2.30, https://github.com/rstudio/rmarkdown\n\n\nBorchers HW (2023) Pracma: Practical numerical math functions. R package version 2.4.4, https://CRAN.R-project.org/package=pracma\n\n\nBunn A, Korpela M, Biondi F, et al (2025) dplR: Dendrochronology program library in r. R package version 1.7.8, https://github.com/OpenDendro/dplR\n\n\nCaeiro F, Mateus A (2024) Randtests: Testing randomness in r. R package version 1.0.2, https://CRAN.R-project.org/package=randtests\n\n\nCroissant Y, Graves S (2025) Ecdat: Data sets for econometrics. R package version 0.4.7, https://www.r-project.org\n\n\nFasiolo M, Nedellec R (2025) mgcViz: Visualisations for generalized additive models. R package version 0.2.1, https://github.com/mfasiolo/mgcViz\n\n\nGastwirth JL, Gel YR, Hui WLW, et al (2023) Lawstat: Tools for biostatistics, public policy, and law. R package version 3.6, https://CRAN.R-project.org/package=lawstat\n\n\nGraves S (2024) FinTS: Companion to tsay (2005) analysis of financial time series. R package version 0.4-9, https://geobosh.github.io/FinTSDoc/\n\n\nHothorn T, Zeileis A, Farebrother RW, Cummins C (2022) Lmtest: Testing linear regression models. R package version 0.9-40, https://CRAN.R-project.org/package=lmtest\n\n\nHyndman R (2023) Fma: Data sets from \"forecasting: Methods and applications\" by makridakis, wheelwright & hyndman (1998). R package version 2.5, https://pkg.robjhyndman.com/fma/\n\n\nHyndman R, Athanasopoulos G, Bergmeir C, et al (2025) Forecast: Forecasting functions for time series and linear models. R package version 8.24.0, https://pkg.robjhyndman.com/forecast/\n\n\nKassambara A (2025) Ggpubr: ggplot2 based publication ready plots. R package version 0.6.1, https://rpkgs.datanovia.com/ggpubr/\n\n\nKelley D, Richards C (2024) Oce: Analysis of oceanographic data. R package version 1.8-3, https://dankelley.github.io/oce/\n\n\nKrispin R (2023) TSstudio: Functions for time series analysis and forecasting. R package version 0.1.7, https://github.com/RamiKrispin/TSstudio\n\n\nKuhn M (2024) Caret: Classification and regression training. R package version 7.0-1, https://github.com/topepo/caret/\n\n\nLigges U, Short T, Kienzle P (2024) Signal: Signal processing. R package version 1.8-1, https://signal.R-forge.R-project.org\n\n\nLyubchich V, Gel YR, Vishwakarma S (2023) Funtimes: Functions for time series analysis. R package version 9.1, https://CRAN.R-project.org/package=funtimes\n\n\nMcLeod AI (2022) Kendall: Kendall rank correlation and mann-kendall trend test. R package version 2.2.1, http://www.stats.uwo.ca/faculty/aim\n\n\nO’Hara-Wild M, Huang CA, Kay M, Hyndman R (2025) Ggtime: Grammar of graphics and plot helpers for time series visualization. R package version 0.1.0, https://CRAN.R-project.org/package=ggtime\n\n\nO’Hara-Wild M, Hyndman R, Wang E (2024) Fable: Forecasting models for tidy time series. R package version 0.4.1, https://fable.tidyverts.org\n\n\nPedersen TL (2025) Patchwork: The composer of plots. R package version 1.3.2, https://patchwork.data-imaginist.com\n\n\nPfaff B (2024) Urca: Unit root and cointegration tests for time series data. R package version 1.3-4, https://CRAN.R-project.org/package=urca\n\n\nPinheiro J, Bates D, R Core Team (2025) Nlme: Linear and nonlinear mixed effects models. R package version 3.1-168, https://svn.r-project.org/R-packages/trunk/nlme/\n\n\nRuf T (2024) Lomb: Lomb-scargle periodogram. R package version 2.5.0, https://CRAN.R-project.org/package=lomb\n\n\nSchloerke B, Cook D, Larmarange J, et al (2025) GGally: Extension to ggplot2. R package version 2.4.0, https://ggobi.github.io/ggally/\n\n\nSievert C, Parmer C, Hocking T, et al (2025) Plotly: Create interactive web graphics via plotly.js. R package version 4.11.0, https://plotly-r.com\n\n\nStasinopoulos M, Rigby B, Eilers P (2016) Gamlss.util: GAMLSS utilities. R package version 4.3-4, http://www.gamlss.org/\n\n\nStasinopoulos M, Rigby R (2025) Gamlss: Generalized additive models for location scale and shape. R package version 5.5-0, https://www.gamlss.com/\n\n\nStasinopoulos M, Rigby R, De Bastiani F (2024) Gamlss.ggplots: Plotting functions for generalized additive model for location scale and shape. R package version 2.1-12, https://www.gamlss.com/\n\n\nStoffer D (2025) Astsa: Applied statistical time series analysis. R package version 2.3, https://dsstoffer.github.io/\n\n\nTrapletti A, Hornik K (2024) Tseries: Time series analysis and computational finance. R package version 0.10-58, https://CRAN.R-project.org/package=tseries\n\n\nWickham H (2024) Downlit: Syntax highlighting and automatic linking. R package version 0.4.4, https://downlit.r-lib.org/\n\n\nWickham H, Chang W, Henry L, et al (2025a) ggplot2: Create elegant data visualisations using the grammar of graphics. R package version 4.0.0, https://ggplot2.tidyverse.org\n\n\nWickham H, François R, Henry L, et al (2023) Dplyr: A grammar of data manipulation. R package version 1.1.4, https://dplyr.tidyverse.org\n\n\nWickham H, Hester J, Bryan J (2024) Readr: Read rectangular text data. R package version 2.1.5, https://readr.tidyverse.org\n\n\nWickham H, Hester J, Ooms J (2025b) xml2: Parse XML. R package version 1.4.0, https://xml2.r-lib.org\n\n\nWood S (2025) Mgcv: Mixed GAM computation vehicle with automatic smoothness estimation. R package version 1.9-3, https://CRAN.R-project.org/package=mgcv\n\n\nWuertz D, Chalabi Y, Setz T, et al (2024) fGarch: Rmetrics - autoregressive conditional heteroskedastic modelling. R package version 4033.92, https://geobosh.github.io/fGarchDoc/\n\n\nXie Y (2025) Knitr: A general-purpose package for dynamic report generation in r. R package version 1.50, https://yihui.org/knitr/\n\n\nZeileis A (2019) Dynlm: Dynamic linear regression. R package version 0.3-6, https://CRAN.R-project.org/package=dynlm",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire J, Xie Y, Dervieux C, et al (2025) Rmarkdown: Dynamic documents\nfor r. R package version 2.30, https://github.com/rstudio/rmarkdown\n\n\nBenjamin MA, Rigby RA, Stasinopoulos DM (2003) Generalized\nautoregressive moving average models. Journal of the American\nStatistical Association 98:214–223. https://doi.org/10.1198/016214503388619238\n\n\nBenson B, Magnuson J, Sharma S (2020) Global lake and river ice\nphenology database. Version 1 (G01377). National\nSnow and Ice Data Center, Boulder, CO, USA\n\n\nBerk RA (2016) Statistical learning\nfrom a regression perspective, 2nd edn. Springer, Switzerland\n\n\nBickel PJ, Götze F, Zwet WR van (1997) Resampling fewer than n observations: Gains, losses, and\nremedies for losses. Statistica Sinica 7:1–31\n\n\nBollerslev T (1986) Generalized autoregressive conditional\nheteroskedasticity. Journal of Econometrics 31:307–327. https://doi.org/10.1016/0304-4076(86)90063-1\n\n\nBollerslev T (2009) Glossary to\nARCH (GARCH). In: Volatility and time\nseries econometrics: Essays in honour of Robert F. Engle.\nSSRN\n\n\nBorchers HW (2023) Pracma: Practical numerical math functions. R package\nversion 2.4.4, https://CRAN.R-project.org/package=pracma\n\n\nBox GEP, Jenkins GM (1976) Time series analysis: Forecasting and\ncontrol. Holden-Day, San Francisco, CA, USA\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and\nforecasting, 2nd edn. Springer, New York, NY, USA\n\n\nBrooks C, Burke SP (2003) Information criteria for GARCH\nmodel selection. The European Journal of Finance 9:557–580. https://doi.org/10.1080/1351847021000029188\n\n\nBühlmann P (2002) Bootstraps for time series. Statistical Science\n17:52–72. https://doi.org/10.1214/ss/1023798998\n\n\nBunn A, Korpela M, Biondi F, et al (2025) dplR: Dendrochronology program\nlibrary in r. R package version 1.7.8, https://github.com/OpenDendro/dplR\n\n\nCabilio P, Zhang Y, Chen X (2013) Bootstrap rank tests for trend in time\nseries. Environmetrics 24:537–549. https://doi.org/10.1002/env.2250\n\n\nCaeiro F, Mateus A (2024) Randtests: Testing randomness in r. R package\nversion 1.0.2, https://CRAN.R-project.org/package=randtests\n\n\nCampbell SD, Diebold FX (2005) Weather forecasting for weather\nderivatives. Journal of the American Statistical Association 100:6–16.\nhttps://doi.org/10.1198/016214504000001051\n\n\nCampos MC, Costa JL, Quintella BR, et al (2008) Activity and movement\npatterns of the Lusitanian toadfish inferred from\npressure-sensitive data-loggers in the Mira estuary\n(Portugal). Fisheries Management and Ecology 15:449–458. https://doi.org/10.1111/j.1365-2400.2008.00629.x\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton, FL,\nUSA\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn.\nJohn Wiley & Sons, Hoboken, NJ, USA\n\n\nChatterjee S, Simonoff JS (2013) Handbook of regression analysis. John\nWiley & Sons, Hoboken, NJ, USA\n\n\nCleveland WS (1979) Robust locally weighted regression and smoothing\nscatterplots. Journal of the American Statistical Association\n74:829–836. https://doi.org/10.1080/01621459.1979.10481038\n\n\nCochrane D, Orcutt GH (1949) Application of least squares regression to\nrelationships containing auto-correlated error terms. Journal of the\nAmerican Statistical Association 44:32–61. https://doi.org/10.2307/2280349\n\n\nCripps E, Dunsmuir WTM (2003) Modeling the variability of\nSydney Harbor wind measurements. Journal of\nApplied Meteorology 42:1131–1138. https://doi.org/10.1175/1520-0450(2003)042&lt;1131:MTVOSH&gt;2.0.CO;2\n\n\nCroissant Y, Graves S (2025) Ecdat: Data sets for econometrics. R\npackage version 0.4.7, https://www.r-project.org\n\n\nDavison AC, Hinkley DV (1997) Bootstrap methods and their application.\nCambridge University Press, Cambridge, UK\n\n\nDean RT, Dunsmuir WTM (2016) Dangers and uses of cross-correlation in\nanalyzing time series in perception, performance, movement, and\nneuroscience: The importance of constructing transfer function\nautoregressive models. Behavior Research Methods 48:783–802. https://doi.org/10.3758/s13428-015-0611-2\n\n\nDegras D, Xu Z, Zhang T, Wu WB (2012) Testing for parallelism among\ntrends in multiple time series. IEEE Transactions on Signal\nProcessing 60:1087–1097. https://doi.org/10.1109/TSP.2011.2177831\n\n\nDickey DA, Fuller WA (1979) Distribution of the estimators for\nautoregressive time series with a unit root. Journal of the American\nStatistical Association 74:427–431. https://doi.org/10.2307/2286348\n\n\nDuguay CR, Brown L, Kang K-K, Kheyrollah Pour H (2013) State of the\nclimate in 2012: Lake ice. Bulletin of the American Meteorological\nSociety 94:S124–S126. https://doi.org/10.1175/2013BAMSStateoftheClimate.1\n\n\nDunn PK, Smyth GK (1996) Randomized quantile residuals. Journal of\nComputational and Graphical Statistics 5:236–244. https://doi.org/10.2307/1390802\n\n\nEfron B (1979) Bootstrap methods: Another look at the jackknife. The\nAnnals of Statistics 7:1–26. https://doi.org/10.1214/aos/1176344552\n\n\nEngle RF (1982) Autoregressive conditional heteroscedasticity with\nestimates of the variance of United Kingdom\ninflation. Econometrica 50:987–1007. https://doi.org/10.2307/1912773\n\n\nEngle RF, Granger CWJ (1987) Co-integration and error correction:\nRepresentation, estimation, and testing. Econometrica 55:251–276. https://doi.org/10.2307/1913236\n\n\nEsterby SR (1996) Review of methods for the detection and estimation of\ntrends with emphasis on water quality applications. Hydrological\nProcesses 10:127–149. https://doi.org/10.1002/(SICI)1099-1085(199602)10:2&lt;127::AID-HYP354&gt;3.0.CO;2-8\n\n\nEun CS, Lee J (2010) Mean-variance convergence around the world. Journal\nof Banking & Finance 34:856–870. https://doi.org/10.1016/j.jbankfin.2009.09.016\n\n\nFasiolo M, Nedellec R (2025) mgcViz: Visualisations for generalized\nadditive models. R package version 0.2.1, https://github.com/mfasiolo/mgcViz\n\n\nGastwirth JL, Gel YR, Hui WLW, et al (2023) Lawstat: Tools for\nbiostatistics, public policy, and law. R package version 3.6, https://CRAN.R-project.org/package=lawstat\n\n\nGranger CWJ (1969) Investigating causal relations by econometric models\nand cross-spectral methods. Econometrica 37:424–438. https://doi.org/10.2307/1912791\n\n\nGraves S (2024) FinTS: Companion to tsay (2005) analysis of financial\ntime series. R package version 0.4-9, https://geobosh.github.io/FinTSDoc/\n\n\nGupta PL, Gupta RC, Tripathi RC (1996) Analysis of zero-adjusted count\ndata. Computational Statistics & Data Analysis 23:207–218. https://doi.org/10.1016/S0167-9473(96)00032-1\n\n\nHall P, Van Keilegom I (2003) Using difference-based methods for\ninference in nonparametric regression with time series errors. Journal\nof the Royal Statistical Society: Series B (Statistical Methodology)\n65:443–456. https://doi.org/10.1111/1467-9868.00395\n\n\nHansen PR, Lunde A (2005) A comparison of volatility models: Does\nanything beat a GARCH(1, 1)? Journal of Applied\nEconometrics 20:873–889. https://doi.org/10.1002/jae.800\n\n\nHärdle W, Horowitz J, Kreiss J-P (2003) Bootstrap methods for time\nseries. International Statistical Review 71:435–459. https://doi.org/10.1111/j.1751-5823.2003.tb00485.x\n\n\nHastie TJ, Tibshirani RJ, Friedman JH (2009) The elements of\nstatistical learning: Data mining, inference, and prediction, 2nd\nedn. Springer, New York, NY, USA\n\n\nHirsch RM, Slack JR, Smith RA (1982) Techniques of trend analysis for\nmonthly water quality data. Water Resources Research 18:107–121. https://doi.org/10.1029/WR018i001p00107\n\n\nHothorn T, Zeileis A, Farebrother RW, Cummins C (2022) Lmtest: Testing\nlinear regression models. R package version 0.9-40, https://CRAN.R-project.org/package=lmtest\n\n\nHyndman R (2023) Fma: Data sets from \"forecasting: Methods and\napplications\" by makridakis, wheelwright & hyndman (1998). R package\nversion 2.5, https://pkg.robjhyndman.com/fma/\n\n\nHyndman R, Athanasopoulos G, Bergmeir C, et al (2025) Forecast:\nForecasting functions for time series and linear models. R package\nversion 8.24.0, https://pkg.robjhyndman.com/forecast/\n\n\nKassambara A (2025) Ggpubr: ggplot2 based publication ready plots. R\npackage version 0.6.1, https://rpkgs.datanovia.com/ggpubr/\n\n\nKelley D, Richards C (2024) Oce: Analysis of oceanographic data. R\npackage version 1.8-3, https://dankelley.github.io/oce/\n\n\nKendall MG (1975) Rank correlation methods, 4th edn. Charles Griffin,\nLondon, UK\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern\ntime series analysis. Springer-Verlag, Berlin, Germany\n\n\nKohn R, Schimek MG, Smith M (2000) Spline and kernel\nregression for dependent data. In: Schimek MG (ed) Smoothing and\nregression: Approaches, computation, and application. John Wiley &\nSons, Inc., New York, pp 135–158\n\n\nKreiss J-P, Paparoditis E, Politis DN (2011) On the range of validity of\nthe autoregressive sieve bootstrap. Annals of Statistics 39:2103–2130.\nhttps://doi.org/10.1214/11-AOS900\n\n\nKrispin R (2023) TSstudio: Functions for time series analysis and\nforecasting. R package version 0.1.7, https://github.com/RamiKrispin/TSstudio\n\n\nKuhn M (2024) Caret: Classification and regression training. R package\nversion 7.0-1, https://github.com/topepo/caret/\n\n\nLatifovic R, Pouliot D (2007) Analysis of climate change impacts on lake\nice phenology in Canada using the historical satellite data\nrecord. Remote Sensing of Environment 106:492–507. https://doi.org/10.1016/j.rse.2006.09.015\n\n\nLi WK (1994) Time series models based on generalized linear models: Some\nfurther results. Biometrics 50:506–511. https://doi.org/10.2307/2533393\n\n\nLigges U, Short T, Kienzle P (2024) Signal: Signal processing. R package\nversion 1.8-1, https://signal.R-forge.R-project.org\n\n\nLjung GM, Box GEP (1978) On a measure of lack of fit in time series\nmodels. Biometrika 65:297–303. https://doi.org/10.1093/biomet/65.2.297\n\n\nLomb NR (1976) Least-squares frequency analysis of unequally spaced\ndata. Astrophysics and Space Science 39:447–462. https://doi.org/10.1007/BF00648343\n\n\nLyubchich V (2016) Detecting time series trends and their\nsynchronization in climate data. Intelligence Innovations Investments\n12:132–137. https://www.researchgate.net/publication/318283780_Detecting_time_series_trends_and_their_synchronization_in_climate_data\n\n\nLyubchich V, Gel YR (2016) A local factor nonparametric test for trend\nsynchronism in multiple time series. Journal of Multivariate Analysis\n150:91–104. https://doi.org/10.1016/j.jmva.2016.05.004\n\n\nLyubchich V, Gel YR, El‐Shaarawi A (2013) On detecting non‐monotonic\ntrends in environmental time series: A fusion of local regression and\nbootstrap. Environmetrics 24:209–226. https://doi.org/10.1002/env.2212\n\n\nLyubchich V, Gel YR, Vishwakarma S (2023) Funtimes: Functions for time\nseries analysis. R package version 9.1, https://CRAN.R-project.org/package=funtimes\n\n\nLyubchich V, Nesslage G (2020) Environmental drivers of golden tilefish fisheries\nv1.0. Version v1.0. Zenodo\n\n\nLyubchich V, Wang X, Heyes A, Gel YR (2016) A distribution-free m-out-of-n bootstrap approach to testing\nsymmetry about an unknown median. Computational Statistics & Data\nAnalysis 104:1–9. https://doi.org/10.1016/j.csda.2016.05.004\n\n\nMarinova D, McAleer M (2003) Modelling trends and volatility in\necological patents in the USA. Environmental Modelling\n& Software 18:195–203. https://doi.org/10.1016/S1364-8152(02)00079-8\n\n\nMcLeod AI (2022) Kendall: Kendall rank correlation and mann-kendall\ntrend test. R package version 2.2.1, http://www.stats.uwo.ca/faculty/aim\n\n\nNason GP (2008) Wavelet methods in\nstatistics with R. Springer, New York, NY, USA\n\n\nNesslage G, Lyubchich V, Nitschke P, et al (2021) Environmental drivers\nof golden tilefish (Lopholatilus\nchamaeleonticeps) commercial landings and catch-per-unit-effort.\nFisheries Oceanography 30:608–622. https://doi.org/10.1111/fog.12540\n\n\nNoguchi K, Gel YR, Duguay CR (2011) Bootstrap-based tests for trends in\nhydrological time series, with application to ice phenology data.\nJournal of Hydrology 410:150–161. https://doi.org/10.1016/j.jhydrol.2011.09.008\n\n\nO’Hara-Wild M, Huang CA, Kay M, Hyndman R (2025) Ggtime: Grammar of\ngraphics and plot helpers for time series visualization. R package\nversion 0.1.0, https://CRAN.R-project.org/package=ggtime\n\n\nO’Hara-Wild M, Hyndman R, Wang E (2024) Fable: Forecasting models for\ntidy time series. R package version 0.4.1, https://fable.tidyverts.org\n\n\nPark C, Hannig J, Kang K-H (2014) Nonparametric comparison of multiple\nregression curves in scale-space. Journal of Computational and Graphical\nStatistics 23:657–677. https://doi.org/10.1080/10618600.2013.822816\n\n\nPark C, Vaughan A, Hannig J, Kang K-H (2009) SiZer analysis\nfor the comparison of time series. Journal of Statistical Planning and\nInference 139:3974–3988. https://doi.org/10.1016/j.jspi.2009.05.003\n\n\nPearl J (2009) Causality: Models, reasoning, and inference, 2nd edn.\nCambridge University Press, Cambridge, UK\n\n\nPedersen TL (2025) Patchwork: The composer of plots. R package version\n1.3.2, https://patchwork.data-imaginist.com\n\n\nPerperoglou A, Sauerbrei W, Abrahamowicz M, Schmid M (2019) A review of\nspline function procedures in R. BMC Medical Research\nMethodology 19: https://doi.org/10.1186/s12874-019-0666-3\n\n\nPfaff B (2024) Urca: Unit root and cointegration tests for time series\ndata. R package version 1.3-4, https://CRAN.R-project.org/package=urca\n\n\nPinheiro J, Bates D, R Core Team (2025) Nlme: Linear and nonlinear mixed\neffects models. R package version 3.1-168, https://svn.r-project.org/R-packages/trunk/nlme/\n\n\nPowell AM, Xu J (2011) Abrupt climate regime shifts, their potential\nforcing and fisheries impacts. Atmospheric and Climate Sciences 1:33. https://doi.org/10.4236/acs.2011.12004\n\n\nRebane G, Pearl J (1987) The recovery of causal poly-trees from\nstatistical data. In: Proceedings of the third annual conference on\nuncertainty in artificial intelligence. pp 222–228\n\n\nRice J (1984) Bandwidth choice for nonparametric regression. The Annals\nof Statistics 12:1215–1230. https://doi.org/10.1214/aos/1176346788\n\n\nRuf T (1999) The Lomb–Scargle periodogram in biological\nrhythm research: Analysis of incomplete and unequally spaced\ntime-series. Biological Rhythm Research 30:178–201. https://doi.org/10.1076/brhm.30.2.178.1422\n\n\nRuf T (2024) Lomb: Lomb-scargle periodogram. R package version 2.5.0, https://CRAN.R-project.org/package=lomb\n\n\nRydberg TH (2000) Realistic statistical modelling of financial data.\nInternational Statistical Review 68:233–258. https://doi.org/10.2307/1403412\n\n\nScargle JD (1982) Studies in astronomical time series analysis.\nII – statistical aspects of spectral analysis of unevenly\nspaced data. Astrophysical Journal 263:835–853. https://doi.org/10.1086/160554\n\n\nSchloerke B, Cook D, Larmarange J, et al (2025) GGally: Extension to\nggplot2. R package version 2.4.0, https://ggobi.github.io/ggally/\n\n\nSeidel DJ, Lanzante JR (2004) An assessment of three alternatives to\nlinear trends for characterizing global atmospheric temperature changes.\nJournal of Geophysical Research: Atmospheres 109: https://doi.org/10.1029/2003JD004414\n\n\nShumway RH, Stoffer DS (2011) Time series analysis\nand its applications with R examples, 3rd edn.\nSpringer, New York, NY, USA\n\n\nShumway RH, Stoffer DS (2014) Time series analysis and its applications\nwith R examples, 3-EZ. Free Texts in Statistics\n\n\nShumway RH, Stoffer DS (2017) Time series analysis\nand its applications with R examples, 4th edn.\nSpringer, New York, NY, USA\n\n\nSievert C, Parmer C, Hocking T, et al (2025) Plotly: Create interactive\nweb graphics via plotly.js. R package version 4.11.0, https://plotly-r.com\n\n\nSiskey MR, Lyubchich V, Liang D, et al (2016) Periodicity of strontium:calcium across annuli further validates\notolith-ageing for Atlantic bluefin tuna\n(Thunnus thynnus). Fisheries Research 177:13–17. https://doi.org/10.1016/j.fishres.2016.01.004\n\n\nSoliman M, Lyubchich V, Gel YR (2019) Complementing the power of deep\nlearning with statistical model fusion: Probabilistic forecasting of\ninfluenza in Dallas County, Texas, USA. Epidemics\n28:100345. https://doi.org/10.1016/j.epidem.2019.05.004\n\n\nStasinopoulos DM, Rigby RA (2007) Generalized additive models for\nlocation scale and shape (GAMLSS) in R.\nJournal of Statistical Software 23:1–46. https://doi.org/10.18637/jss.v023.i07\n\n\nStasinopoulos M, Rigby B, Eilers P (2016) Gamlss.util: GAMLSS utilities.\nR package version 4.3-4, http://www.gamlss.org/\n\n\nStasinopoulos M, Rigby R (2025) Gamlss: Generalized additive models for\nlocation scale and shape. R package version 5.5-0, https://www.gamlss.com/\n\n\nStasinopoulos M, Rigby R, De Bastiani F (2024) Gamlss.ggplots: Plotting\nfunctions for generalized additive model for location scale and shape. R\npackage version 2.1-12, https://www.gamlss.com/\n\n\nStoffer D (2025) Astsa: Applied statistical time series analysis. R\npackage version 2.3, https://dsstoffer.github.io/\n\n\nTaylor JW, Buizza R (2004) A comparison of temperature density forecasts\nfrom GARCH and atmospheric models. Journal of Forecasting\n23:337–355. https://doi.org/10.1002/for.917\n\n\nTrapletti A, Hornik K (2024) Tseries: Time series analysis and\ncomputational finance. R package version 0.10-58, https://CRAN.R-project.org/package=tseries\n\n\nTsay RS (2005) Analysis of financial time series, 2nd edn. John Wiley\n& Sons, Hoboken, NJ, USA\n\n\nVilar-Fernández JM, González-Manteiga W (2004) Nonparametric comparison\nof curves with dependent errors. Statistics 38:81–99. https://doi.org/10.1080/02331880310001634656\n\n\nVogelsang TJ, Franses PH (2005) Testing for common deterministic trend\nslopes. Journal of Econometrics 126:1–24. https://doi.org/10.1016/j.jeconom.2004.02.004\n\n\nWang L, Akritas MG, Van Keilegom I (2008) An ANOVA-type\nnonparametric diagnostic test for heteroscedastic regression models.\nJournal of Nonparametric Statistics 20:365–382. https://doi.org/10.1080/10485250802066112\n\n\nWickham H (2024) Downlit: Syntax highlighting and automatic linking. R\npackage version 0.4.4, https://downlit.r-lib.org/\n\n\nWickham H, Chang W, Henry L, et al (2025a) ggplot2: Create elegant data\nvisualisations using the grammar of graphics. R package version 4.0.0,\nhttps://ggplot2.tidyverse.org\n\n\nWickham H, François R, Henry L, et al (2023) Dplyr: A grammar of data\nmanipulation. R package version 1.1.4, https://dplyr.tidyverse.org\n\n\nWickham H, Hester J, Bryan J (2024) Readr: Read rectangular text data. R\npackage version 2.1.5, https://readr.tidyverse.org\n\n\nWickham H, Hester J, Ooms J (2025b) xml2: Parse XML. R package version\n1.4.0, https://xml2.r-lib.org\n\n\nWood S (2025) Mgcv: Mixed GAM computation vehicle with automatic\nsmoothness estimation. R package version 1.9-3, https://CRAN.R-project.org/package=mgcv\n\n\nWood SN (2006) Generalized additive models: An introduction with r.\nChapman; Hall/CRC, New York, NY, USA\n\n\nWooldridge JM (2013) Introductory econometrics: A modern approach, 5th\nedn. Cengage Learning, Mason, OH, USA\n\n\nWuertz D, Chalabi Y, Setz T, et al (2024) fGarch: Rmetrics -\nautoregressive conditional heteroskedastic modelling. R package version\n4033.92, https://geobosh.github.io/fGarchDoc/\n\n\nXie Y (2025) Knitr: A general-purpose package for dynamic report\ngeneration in r. R package version 1.50, https://yihui.org/knitr/\n\n\nZeger SL, Qaqish B (1988) Markov regression models for time series: A\nquasi-likelihood approach. Biometrics 44:1019–1031. https://doi.org/10.2307/2531732\n\n\nZeileis A (2019) Dynlm: Dynamic linear regression. R package version\n0.3-6, https://CRAN.R-project.org/package=dynlm\n\n\nZhang T (2013) Clustering high-dimensional time series based on\nparallelism. Journal of the American Statistical Association\n108:577–588. https://doi.org/10.1080/01621459.2012.760458\n\n\nZuur A, Ieno EN, Walker NJ, et al (2009) Mixed effects models\nand extensions in ecology with R. Springer, New York",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "s1_wls.html",
    "href": "s1_wls.html",
    "title": "Appendix A — Weighted least squares",
    "section": "",
    "text": "We can often hypothesize that the standard deviation of residuals in the model \\[\ny_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\n\\tag{A.1}\\] is proportional to the predictor \\(X\\), so \\[\n\\mathrm{var}(\\varepsilon_i)=k^2x^2_i, \\;\\; k&gt;0.\n\\]\nIn the weighted least squares (WLS) method, we can stabilize the variance by dividing both sides of Equation A.1 by \\(x_i\\): \\[\n\\frac{y_i}{x_i}=\\frac{\\beta_0}{x_i}+\\beta_1+\\frac{\\varepsilon_i}{x_i},\n\\tag{A.2}\\] then \\(\\mathrm{var}\\left(\\frac{\\varepsilon_i}{x_i}\\right)=k^2\\), i.e., it is now stabilized.\n\n\n\n\n\n\nNoteExample: WLS applied ‘manually’\n\n\n\nConsider a simulated example of a linear model \\(y=3-2x\\) with noise, which is a function of \\(x\\).\n\nCodeset.seed(111)\nk = 0.5\nn = 100\nx &lt;- rnorm(n, 0, 5)\ny &lt;- 3 - 2 * x + rnorm(n, 0, k*x^2)\n\n\nThe coefficients estimated using ordinary least squares (OLS):\n\nfit_ols &lt;- lm(y ~ x)\nsummary(fit_ols)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -47.41  -8.01  -2.32   1.58 286.47 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)    6.131      3.402    1.80    0.075 .  \n#&gt; x             -3.571      0.639   -5.59    2e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 34 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.242,  Adjusted R-squared:  0.234 \n#&gt; F-statistic: 31.3 on 1 and 98 DF,  p-value: 2.04e-07\n\n\nBased on Figure A.1, the OLS assumption of homoskedasticity is violated, because the observations deviate farther from the regression line at its ends (i.e., the variability of regression residuals is higher at the low and high values of the predictor).\n\nCodep1 &lt;- ggplot(data.frame(x, y), aes(x = x, y = y)) + \n    geom_abline(intercept = 3, slope = -2, col = \"gray50\", lwd = 1.5) +\n    geom_abline(intercept = fit_ols$coefficients[1], \n                slope = fit_ols$coefficients[2], lty = 2) +\n    geom_point()\np2 &lt;- ggplot(data.frame(x, y), aes(x = x, y = rstandard(fit_ols))) +\n    geom_hline(yintercept = 0, col = \"gray50\") +\n    geom_point() +\n    xlab(\"x\") +\n    ylab(\"Standardized residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure A.1: Simulated data example with heteroskedasticity. The gray line represents the underlying model; the dashed line is obtained from the OLS fit.\n\n\n\n\nTo stabilize the variance ‘manually,’ transform the variables according to Equation A.2 and refit the model:\n\nY.t &lt;- y/x\nX.t &lt;- 1/x\nfit_wls &lt;- lm(Y.t ~ X.t)\nsummary(fit_wls)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Y.t ~ X.t)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -20.021  -0.635   0.251   1.322   5.676 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   -2.154      0.293   -7.36  5.7e-11 ***\n#&gt; X.t            3.008      0.168   17.95  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.9 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.767,  Adjusted R-squared:  0.764 \n#&gt; F-statistic:  322 on 1 and 98 DF,  p-value: &lt;2e-16\n\n\nCheck Equation A.2 to see the correspondence of the coefficients, see the results in Figure A.2.\n\nCodep1 &lt;- ggplot(data.frame(x, y), aes(x = x, y = y)) + \n    geom_abline(intercept = 3, slope = -2, col = \"gray50\", lwd = 1.5) +\n    geom_abline(intercept = fit_wls$coefficients[2], \n                slope = fit_wls$coefficients[1], lty = 2) +\n    geom_point()\np2 &lt;- ggplot(data.frame(x, y), aes(x = x, y = rstandard(fit_wls))) +\n    geom_hline(yintercept = 0, col = \"gray50\") +\n    geom_point() +\n    xlab(\"x\") +\n    ylab(\"Standardized residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure A.2: Simulated data example with heteroskedasticity. The gray line represents the underlying model; the dashed line is obtained from the WLS fit.\n\n\n\n\n\n\nInstead of minimizing the residual sum of squares (using the original or transformed data in Equation A.1 and Equation A.2), \\[\nRSS(\\beta) = \\sum_{i=1}^n(y_i - x_i\\beta)^2,\n\\] we minimize the weighted sum of squares, where \\(w_i\\) are the weights: \\[\nWSS(\\beta; w) = \\sum_{i=1}^nw_i(y_i - x_i\\beta)^2.\n\\] This includes OLS as the special case when all the weights \\(w_i = 1\\) (\\(i=1,\\dots,n\\)). In the example above, \\(w_i=1/x^2_i\\).\nIn matrix form, \\[\n\\hat{\\boldsymbol{\\beta}}=(\\boldsymbol{X}^{\\top}\\boldsymbol{W}\\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\top}\\boldsymbol{W}\\boldsymbol{Y}.\n\\tag{A.3}\\]\nTo apply Equation A.3 in R, specify the argument weights, and remember to take an inverse. Note that the coefficients are now labeled as expected.\n\nfit_wls2 &lt;- lm(y ~ x, weights = 1/x^2)\nsummary(fit_wls2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, weights = 1/x^2)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -7.122 -0.842  0.018  1.238 20.021 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)    3.008      0.168   17.95  &lt; 2e-16 ***\n#&gt; x             -2.154      0.293   -7.36  5.7e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.9 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.356,  Adjusted R-squared:  0.349 \n#&gt; F-statistic: 54.2 on 1 and 98 DF,  p-value: 5.72e-11\n\n\nChatterjee and Hadi (2006) in Chapter 7 consider two more cases for applying WLS, both related to grouping. We skip those cases for now and revisit our data example from Chapter 1.\n\n\n\n\n\n\nNoteExample: Dishwasher shipments WLS model\n\n\n\nFirst, use OLS to estimate the simple linear regression exploring dishwasher shipments (DISH) and private residential investments (RES) for several years.\n\nD &lt;- read.delim(\"data/dish.txt\") %&gt;% \n    rename(Year = YEAR)\nmodDish_ols &lt;- lm(DISH ~ RES, data = D)\n\nThe plot in Figure A.3 indicates that the variance might be decreasing with higher investments.\n\nCodeggplot(D, aes(x = RES, y = rstandard(modDish_ols))) +\n    geom_hline(yintercept = 0, col = \"gray50\") +\n    geom_point() +\n    xlab(\"Residential investments\") +\n    ylab(\"Standardized residuals\")\n\n\n\n\n\n\nFigure A.3: OLS residuals vs. the predictor.\n\n\n\n\nApply the WLS:\n\nmodDish_wls &lt;- lm(DISH ~ RES, data = D, weights = RES^2)\n\nIn Figure A.4 we see minor changes in the slope (better fit?).\n\nCodep1 &lt;- ggplot(D, aes(x = RES, y = DISH)) + \n    geom_abline(intercept = modDish_wls$coefficients[1], \n                slope = modDish_wls$coefficients[2], lty = 2) +\n    geom_abline(intercept = modDish_ols$coefficients[1], \n                slope = modDish_ols$coefficients[2], \n                col = \"gray50\") +\n    geom_point() +\n    xlab(\"Residential investments\") + \n    ylab(\"Dishwasher shipments\")\np2 &lt;- ggplot(D, aes(x = RES, y = rstandard(modDish_wls))) +\n    geom_hline(yintercept = 0, col = \"gray50\") +\n    geom_point() +\n    xlab(\"Residential investments\") +\n    ylab(\"Standardized residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure A.4: The regression fits (OLS – solid line; WLS – dashed line) and the WLS residuals vs. the predictor.\n\n\n\n\nHowever, the residuals are still autocorrelated, which violates another assumption of the OLS and WLS methods:\n\nlawstat::runs.test(rstandard(modDish_wls))\n\n#&gt; \n#&gt;  Runs Test - Two sided\n#&gt; \n#&gt; data:  rstandard(modDish_wls)\n#&gt; Standardized Runs Statistic = -2, p-value = 0.05\n\n\n\n\nSee Appendix B on the method of generalized least squares (GLS) that allows accounting for autocorrelation in regression modeling.\n\n\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn. John Wiley & Sons, Hoboken, NJ, USA",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Weighted least squares</span>"
    ]
  },
  {
    "objectID": "s2_gls.html",
    "href": "s2_gls.html",
    "title": "Appendix B — Generalized least squares",
    "section": "",
    "text": "Here we use time series data (ordered by \\(t\\)), thus, Equation A.1 will be written with the time indices \\(t\\) as \\[\ny_t=\\beta_0+\\beta_1x_t+\\varepsilon_t,\n\\tag{B.1}\\] where the regression errors at times \\(t\\) and \\(t-1\\) are \\[\n\\begin{split}\n\\varepsilon_t&=y_t-\\beta_0-\\beta_1x_t,\\\\\n\\varepsilon_{t-1}&=y_{t-1}-\\beta_0-\\beta_1x_{t-1}.\n\\end{split}\n\\tag{B.2}\\]\nAn AR(1) model for the errors will yield \\[\n\\begin{split}\ny_t-\\beta_0-\\beta_1x_t & = \\rho\\varepsilon_{t-1} + w_t, \\\\\ny_t-\\beta_0-\\beta_1x_t & = \\rho(y_{t-1}-\\beta_0-\\beta_1x_{t-1})+w_t,\n\\end{split}\n\\tag{B.3}\\] where \\(w_t\\) are uncorrelated errors.\nRewrite it as \\[\n\\begin{split}\ny_t-\\rho y_{t-1}&=\\beta_0(1-\\rho)+\\beta_1(x_t-\\rho x_{t-1})+w_t,\\\\\ny_t^* &= \\beta_0^* + \\beta_1 x_t^*+w_t,\n\\end{split}\n\\tag{B.4}\\] where \\(y_t^* = y_t-\\rho y_{t-1}\\); \\(\\beta_0^* = \\beta_0(1-\\rho)\\); \\(x_t^* = x_t-\\rho x_{t-1}\\). Notice the errors \\(w_t\\) in the final Equation B.4 for the transformed variables \\(y_t^*\\) and \\(x_t^*\\) are uncorrelated.\nTo get from Equation B.1 to Equation B.4, we can use an iterative procedure by Cochrane and Orcutt (1949) as in the example below.\n\n\n\n\n\n\nNoteExample: Dishwasher shipments model accounting for autocorrelation\n\n\n\n\nEstimate the model in Equation B.1 using OLS.\n\n\nCodeD &lt;- read.delim(\"data/dish.txt\") %&gt;% \n    rename(Year = YEAR)\nmodDish_ols &lt;- lm(DISH ~ RES, data = D)\n\n\n\nCalculate residuals \\(\\hat{\\varepsilon}_t\\) and estimate \\(\\rho\\) as \\[\n\\hat{\\rho}=\\frac{\\sum_{t=2}^n\\hat{\\varepsilon}_t\\hat{\\varepsilon}_{t-1}}{\\sum_{t=1}^n\\hat{\\varepsilon}^2_t}.\n\\]\n\n\n\nCodee &lt;- modDish_ols$residuals\nrho &lt;- sum(e[-1] * e[-length(e)]) / sum(e^2)\nrho\n\n#&gt; [1] 0.694\n\n\n\nCalculate transformed variables \\(x^*_t\\) and \\(y^*_t\\) and fit model in Equation B.4.\n\n\nCodey.star &lt;- D$DISH[-1] - rho * D$DISH[-length(D$DISH)]\nx.star &lt;- D$RES[-1] - rho * D$RES[-length(D$RES)]\nmodDish_ar1 &lt;- lm(y.star ~ x.star)\nsummary(modDish_ar1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y.star ~ x.star)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -479.7 -117.8   32.9  120.7  536.1 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)    26.76     130.69    0.20     0.84    \n#&gt; x.star         50.99       7.74    6.59    1e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 252 on 23 degrees of freedom\n#&gt; Multiple R-squared:  0.654,  Adjusted R-squared:  0.639 \n#&gt; F-statistic: 43.4 on 1 and 23 DF,  p-value: 1.01e-06\n\n\n\nExamine the residuals of the newly fitted equation (Figure B.1) and repeat the procedure, if needed.\n\n\nCodep1 &lt;- ggplot(D, aes(x = Year, y = modDish_ols$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ggtitle(\"OLS model modDish_ols\") +\n    ylab(\"Residuals\")\np2 &lt;- ggplot(D[-1,], aes(x = Year, y = modDish_ar1$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ggtitle(\"Transformed model modDish_ar1\") +\n    ylab(\"Residuals\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure B.1: Residual plots of the original OLS model and the model transformed to account for autocorrelation in residuals.\n\n\n\n\nBased on the runs test, there is not enough evidence of autocorrelation in the new residuals:\n\nlawstat::runs.test(rstandard(modDish_ar1))\n\n#&gt; \n#&gt;  Runs Test - Two sided\n#&gt; \n#&gt; data:  rstandard(modDish_ar1)\n#&gt; Standardized Runs Statistic = -0.6, p-value = 0.5\n\n\n\n\nWhat we have just applied is the method of generalized least squares (GLS): \\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\boldsymbol{X}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{Y},\n\\tag{B.5}\\] where \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix. The method of weighted least squares (WLS; Appendix A) is just a special case of the GLS. In the WLS approach, all the off-diagonal entries of \\(\\boldsymbol{\\Sigma}\\) are 0.\nWe can use the function nlme::gls() and specify the correlation structure to avoid iterating the steps from the previous example manually:\n\nmodDish_ar1_v2 &lt;- nlme::gls(DISH ~ RES\n                            ,correlation = nlme::corAR1(form = ~Year)\n                            ,data = D)\nsummary(modDish_ar1_v2)\n\n#&gt; Generalized least squares fit by REML\n#&gt;   Model: DISH ~ RES \n#&gt;   Data: D \n#&gt;   AIC BIC logLik\n#&gt;   342 347   -167\n#&gt; \n#&gt; Correlation Structure: AR(1)\n#&gt;  Formula: ~Year \n#&gt;  Parameter estimate(s):\n#&gt; Phi \n#&gt;   1 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Value Std.Error t-value p-value\n#&gt; (Intercept) -137.5   3714137    0.00       1\n#&gt; RES           45.7         6    7.35       0\n#&gt; \n#&gt;  Correlation: \n#&gt;     (Intr)\n#&gt; RES 0     \n#&gt; \n#&gt; Standardized residuals:\n#&gt;       Min        Q1       Med        Q3       Max \n#&gt; -0.000249 -0.000014  0.000135  0.000232  0.000338 \n#&gt; \n#&gt; Residual standard error: 3714137 \n#&gt; Degrees of freedom: 26 total; 24 residual\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the function nlme::gls() we can also specify weights to accommodate heteroskedastic errors, but the syntax differs from the weights specification in the function stats::lm() (Appendix A). See ?nlme::varFixed.\n\n\n\n\n\n\nCochrane D, Orcutt GH (1949) Application of least squares regression to relationships containing auto-correlated error terms. Journal of the American Statistical Association 44:32–61. https://doi.org/10.2307/2280349",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Generalized least squares</span>"
    ]
  },
  {
    "objectID": "s4_trendsync.html",
    "href": "s4_trendsync.html",
    "title": "Appendix C — Synchrony of parametric trends",
    "section": "",
    "text": "The problem of detecting joint trend dynamics in time series is essential in a variety of applications, ranging from the analysis of macroeconomic indicators (Vogelsang and Franses 2005; Eun and Lee 2010) to assessing patterns in ice phenology measurements from multiple locations (Latifovic and Pouliot 2007; Duguay et al. 2013) to evaluating yields of financial instruments at various maturity levels (Park et al. 2009) and cell phone download activity at different area codes (Degras et al. 2012).\nThe extensive research on comparing trend patterns follows two main directions:\n\nTesting for joint mean functions.\nAnalysis of joint stochastic trends, which is closely linked to the cointegration notion by Engle and Granger (1987) (Section 8.3).\n\nHere we explore the first direction, that is, assess whether several observed time series follow the same hypothesized parametric trend.\nThere exist many tests for comparing mean functions, but most of the developed methods assume independent errors. Substantially less is known about testing for joint deterministic trends in a time series framework.\nOne of the methods developed for time series is by Degras et al. (2012) and Zhang (2013) who extended the integrated square error (ISE) based approach of Vilar-Fernández and González-Manteiga (2004) to a case of multiple time series with weakly dependent (non)stationary errors. For a comprehensive literature review of available methodology for comparing mean functions embedded into independent errors in a time series framework, see Degras et al. (2012) and Park et al. (2014). Most of these methods, however, either focus on aligning only two curves or require us to select multiple hyperparameters, such as the bandwidth, level of smoothness, and window size for a long-run variance function. As mentioned by Park et al. (2014), the choice of such multiple nuisance parameters is challenging for a comparison of curves (even under the independent and identically distributed setup) and often leads to inadequate performance, especially in samples of moderate size.\nAs an alternative, consider an extension of the WAVK test (Section 7.4.2) to a case of multiple time series (Lyubchich and Gel 2016). Let us observe \\(N\\) time series \\[\nY_{it} = \\mu_i(t/T) + \\epsilon_{it},\n\\] where \\(i = 1, \\dots, N\\) (\\(N\\) is the number of time series), \\(t=1, \\dots, T\\) (\\(T\\) is the length of the time series), \\(\\mu_i(u)\\) (\\(0&lt;u\\leqslant 1\\)) are unknown smooth trend functions, and the noise \\(\\epsilon_{it}\\) can be represented as a finite-order AR(\\(p\\)) process or infinite-order AR(\\(\\infty\\)) process with i.i.d. innovations \\({e}_{it}\\).\nWe are interested in testing whether these \\(N\\) observed time series have the same trend of some pre-specified smooth parametric form \\(f(\\theta, u)\\):\n\\(H_0\\): \\(\\mu_i(u)= c_i + f(\\theta, u)\\)\\(H_1\\): there exists \\(i\\), such that \\(\\mu_i(u)\\neq c_i + f(\\theta, u)\\),\nwhere the reference curve \\(f(\\theta, u)\\) with a vector of parameters \\(\\theta\\) belongs to a known family of smooth parametric functions, and \\(1\\leqslant i \\leqslant N\\). For identifiability, assume that \\(\\sum_{i=1}^N c_i=0\\). Notice that the hypotheses include (but are not limited to) the special cases of\\(f(\\theta,u)\\equiv 0\\) (testing for no trend);\\(f(\\theta,u)=\\theta_0+\\theta_1 u\\) (testing for a common linear trend);\\(f(\\theta,u)=\\theta_0+\\theta_1 u+\\theta_2u^2\\) (testing for a common quadratic trend).\nThis hypothesis testing approach allows us to answer the following questions:\n\nDo trends in temperature (or wind speeds, or precipitation) reproduced by a climate model correspond to the historical observations? I.e., is the model generally correct?\nDo different instruments (sensors) capture changes similarly, or deviate, for example, due to aging of some of the instruments?\nDo trends estimated at different locations (Canada and USA, lower and mid-troposphere, etc.) follow some hypothesized global trend?\n\nTest the null hypothesis by following these steps (Lyubchich and Gel 2016):\n\nEstimate the joint hypothetical trend \\(f({\\theta}, u)\\) using the aggregated sample \\(\\left\\{\\overline{Y}_{\\cdot t}\\right\\}_{t=1}^T\\) (i.e., a time series obtained by averaging across all \\(N\\) time series).\nFor each time series, subtract the estimated trend, then apply the autoregressive filter to obtain residuals \\(\\hat{e}_{it}\\), which under the \\(H_0\\) behave asymptotically like the independent and identically distributed \\({e}_{it}\\): \\[\n\\begin{split}\n\\hat{e}_{it}&= \\hat{\\epsilon}_{it}-\\sum_{j=1}^{p_i(T)}{\\hat{\\phi}_{ij}\\hat{\\epsilon}_{i,t-j}} \\\\\n&=\n\\left\\{ Y_{it} - f(\\hat{\\theta},u_{t}) \\right\\} -\n\\left\\{ \\sum_{j=1}^{p_i(T)}{\\hat{\\phi}_{ij}{Y}_{i,t-j}} - \\sum_{j=1}^{p_i(T)}{\\hat{\\phi}_{ij}f(\\hat{\\theta},u_{t-j})} \\right\\}.\n\\end{split}\n\\]\n\nConstruct a sequence of \\(N\\) statistics \\({\\rm WAVK}_{1}(k_{1}), \\dots, {\\rm WAVK}_{N}(k_{N})\\). Then, the synchrony test statistic is \\[\nS_T = \\sum_{i=1}^N k_{i}^{-1/2}{\\rm WAVK}_{i}(k_{i}),\n\\] where \\(k_{i}\\) is the local window size for the WAVK statistic.\nEstimate the variance of \\(\\hat{e}_{it}\\), e.g., using the robust difference-based estimator by Rice (1984): \\[\ns_i^2= \\frac{\\sum_{t=2}^T(\\hat{e}_{it}-\\hat{e}_{i,t-1})^2}{2(T-1)}.\n\\]\n\nSimulate \\(BN\\) times \\(T\\)-dimensional vectors \\(e^*_{iT}\\) from the multivariate normal distribution \\(MVN\\left(0, s_i^2\\boldsymbol{I}\\right)\\), where \\(B\\) is the number of bootstrap replications, \\(\\boldsymbol{I}\\) is a \\(T\\times T\\) identity matrix.\nCompute \\(B\\) bootstrapped statistics on \\(e^*_{iT}\\): \\[S^*_T=\\sum_{i=1}^N k^{-1/2}_{i} {\\rm WAVK}^*_{i}(k_{i}).\\]\n\nThe bootstrap \\(p\\)-value for testing the \\(H_0\\) is the proportion of \\(|S^*_T|\\) that exceed \\(|S_T|\\).\n\nSee the application of both the WAVK and synchrony tests in Lyubchich (2016).\nIf the null hypothesis is rejected, the method does not tell, however, what was the reason, and which particular time series caused the rejection of the \\(H_0\\). One can remove the time series (or several time series at once) with the largest WAVK statistic(s) and apply the test again, although repeated testing increases the probability of Type I error. For an application of this method in trend clustering, see this vignette.\n\n\n\n\n\n\nNoteExample: CMIP5 vs. observations\n\n\n\nReplicate the test for synchrony of trends found in two time series (Lyubchich 2016):\n\na multi-model average of temperatures from the 5th phase of the Coupled Model Intercomparison Project (CMIP5) and\nobserved global temperature anomalies relative to the base period of 1981–2010.\n\n\nCodeD &lt;- read.csv(\"data/CMIP5.csv\") %&gt;% \n    filter(1948 &lt;= Year & Year &lt;= 2013) %&gt;%\n    mutate(Temp_CMIP5 = Temp_CMIP5 - 273.15)\n\n\nSee Figure C.1 showing the time series plots after converting the CMIP data to degrees Celsius.\n\nCodep1 &lt;- D %&gt;% ggplot(aes(x = Year, y = Temp_CMIP5)) +\n    geom_line()\np2 &lt;- D %&gt;% ggplot(aes(x = Year, y = Temp_obs)) +\n    geom_line()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure C.1: Global annual mean temperature (°C) in 1948–2013: CMIP5 multi-model average and observed anomalies relative to the base period of 1981–2010.\n\n\n\n\nTest the synchrony of parametric linear trends in these time series:\n\nset.seed(123)\nfuntimes::sync_test(D[, c(\"Temp_CMIP5\", \"Temp_obs\")] ~ t)\n\n#&gt; \n#&gt;  Nonparametric test for synchronism of parametric trends\n#&gt; \n#&gt; data:  D[, c(\"Temp_CMIP5\", \"Temp_obs\")] \n#&gt; Test statistic = 0.04, p-value = 0.03\n#&gt; alternative hypothesis: common trend is not of the form D[, c(\"Temp_CMIP5\", \"Temp_obs\")] ~ t.\n#&gt; sample estimates:\n#&gt; $common_trend_estimates\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)    -1.57     0.0962   -16.4 1.44e-24\n#&gt; t               3.10     0.1647    18.8 8.39e-28\n#&gt; \n#&gt; $ar.order_used\n#&gt;          Temp_CMIP5 Temp_obs\n#&gt; ar.order          1        1\n#&gt; \n#&gt; $Window_used\n#&gt;        Temp_CMIP5 Temp_obs\n#&gt; Window          3        3\n#&gt; \n#&gt; $all_considered_windows\n#&gt;  Window Statistic p-value Asympt. p-value\n#&gt;       3    0.0406   0.034          0.0246\n#&gt;       4    0.0373   0.042          0.0386\n#&gt;       6    0.0413   0.020          0.0222\n#&gt; \n#&gt; $wavk_obs\n#&gt; [1] 0.0143 0.0263\n\n\nThe \\(p\\)-value below the usual significance level \\(\\alpha = 0.05\\) allows us to reject the null hypothesis, however, as Lyubchich (2016) pointed out, the decision would differ if more confidence is required (e.g., \\(\\alpha = 0.01\\)). Note that the \\(p\\)-value of 0.012 reported by Lyubchich (2016) differs from the one reported above due to the function settings and randomness due to the bootstrapping. We should save the random number generator state with set.seed() for replicability (so the test results are exactly the same every time the test is applied), and use a larger number of bootstrap replications B for consistency (so the test leads to the same conclusions when the function set.seed() is not used).\nNow test the synchrony of parametric quadratic trends in these time series:\n\nset.seed(123)\nfuntimes::sync_test(D[, c(\"Temp_CMIP5\", \"Temp_obs\")] ~ poly(t, 2))\n\n#&gt; \n#&gt;  Nonparametric test for synchronism of parametric trends\n#&gt; \n#&gt; data:  D[, c(\"Temp_CMIP5\", \"Temp_obs\")] \n#&gt; Test statistic = 0.05, p-value = 0.002\n#&gt; alternative hypothesis: common trend is not of the form D[, c(\"Temp_CMIP5\", \"Temp_obs\")] ~ poly(t, 2).\n#&gt; sample estimates:\n#&gt; $common_trend_estimates\n#&gt;             Estimate Std. Error  t value Pr(&gt;|t|)\n#&gt; (Intercept) 6.72e-16     0.0324 2.07e-14 1.00e+00\n#&gt; poly(t, 2)1 7.27e+00     0.2632 2.76e+01 6.20e-37\n#&gt; poly(t, 2)2 2.28e+00     0.2632 8.65e+00 2.62e-12\n#&gt; \n#&gt; $ar.order_used\n#&gt;          Temp_CMIP5 Temp_obs\n#&gt; ar.order          2        0\n#&gt; \n#&gt; $Window_used\n#&gt;        Temp_CMIP5 Temp_obs\n#&gt; Window          3        3\n#&gt; \n#&gt; $all_considered_windows\n#&gt;  Window Statistic p-value Asympt. p-value\n#&gt;       3    0.0518   0.002        0.000282\n#&gt;       4    0.0350   0.028        0.014236\n#&gt;       6    0.0222   0.070        0.120094\n#&gt; \n#&gt; $wavk_obs\n#&gt; [1] -0.000673  0.052504\n\n\nNote these results differ substantially based on the window used for computing the WAVK statistic. The function funtimes::sync_test() automatically selects the optimal window based on the heuristic approach of comparing distances between bootstrap distributions (Lyubchich 2016; Lyubchich and Gel 2016).\n\n\n\n\n\n\nDegras D, Xu Z, Zhang T, Wu WB (2012) Testing for parallelism among trends in multiple time series. IEEE Transactions on Signal Processing 60:1087–1097. https://doi.org/10.1109/TSP.2011.2177831\n\n\nDuguay CR, Brown L, Kang K-K, Kheyrollah Pour H (2013) State of the climate in 2012: Lake ice. Bulletin of the American Meteorological Society 94:S124–S126. https://doi.org/10.1175/2013BAMSStateoftheClimate.1\n\n\nEngle RF, Granger CWJ (1987) Co-integration and error correction: Representation, estimation, and testing. Econometrica 55:251–276. https://doi.org/10.2307/1913236\n\n\nEun CS, Lee J (2010) Mean-variance convergence around the world. Journal of Banking & Finance 34:856–870. https://doi.org/10.1016/j.jbankfin.2009.09.016\n\n\nLatifovic R, Pouliot D (2007) Analysis of climate change impacts on lake ice phenology in Canada using the historical satellite data record. Remote Sensing of Environment 106:492–507. https://doi.org/10.1016/j.rse.2006.09.015\n\n\nLyubchich V (2016) Detecting time series trends and their synchronization in climate data. Intelligence Innovations Investments 12:132–137. https://www.researchgate.net/publication/318283780_Detecting_time_series_trends_and_their_synchronization_in_climate_data\n\n\nLyubchich V, Gel YR (2016) A local factor nonparametric test for trend synchronism in multiple time series. Journal of Multivariate Analysis 150:91–104. https://doi.org/10.1016/j.jmva.2016.05.004\n\n\nPark C, Hannig J, Kang K-H (2014) Nonparametric comparison of multiple regression curves in scale-space. Journal of Computational and Graphical Statistics 23:657–677. https://doi.org/10.1080/10618600.2013.822816\n\n\nPark C, Vaughan A, Hannig J, Kang K-H (2009) SiZer analysis for the comparison of time series. Journal of Statistical Planning and Inference 139:3974–3988. https://doi.org/10.1016/j.jspi.2009.05.003\n\n\nRice J (1984) Bandwidth choice for nonparametric regression. The Annals of Statistics 12:1215–1230. https://doi.org/10.1214/aos/1176346788\n\n\nVilar-Fernández JM, González-Manteiga W (2004) Nonparametric comparison of curves with dependent errors. Statistics 38:81–99. https://doi.org/10.1080/02331880310001634656\n\n\nVogelsang TJ, Franses PH (2005) Testing for common deterministic trend slopes. Journal of Econometrics 126:1–24. https://doi.org/10.1016/j.jeconom.2004.02.004\n\n\nZhang T (2013) Clustering high-dimensional time series based on parallelism. Journal of the American Statistical Association 108:577–588. https://doi.org/10.1080/01621459.2012.760458",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Synchrony of parametric trends</span>"
    ]
  },
  {
    "objectID": "s7_garma.html",
    "href": "s7_garma.html",
    "title": "Appendix D — Generalized autoregressive moving average (GARMA) models",
    "section": "",
    "text": "Let \\(Y_t\\) be the observed time series and \\(\\boldsymbol{X_t}\\) exogenous regressors. Then, we can model the conditional distribution of \\(Y_t\\), given \\(Y_1,\\dots,Y_{t-1}\\), \\(\\boldsymbol{X_1,\\dots,X_{t}}\\) as \\[\ng(\\mu_t)=\\boldsymbol{X}'_t\\beta+\\sum_{j=1}^p\\phi_j\\{g(Y_{t-j})- \\boldsymbol{X}'_{t-j}\\beta\\}\n+\\sum_{j=1}^q\\theta_j \\{g(Y_{t-j})-g(\\mu_{t-j})\\},\n\\tag{D.1}\\] where \\(g(\\cdot)\\) is an appropriate link function; \\(\\mu_t\\) is a conditional mean of the dependent variable; \\(\\boldsymbol{\\beta}\\) is a vector of regression coefficients; \\(\\phi_j\\), \\(j=1,\\dots, p\\), are the autoregressive coefficients; \\(\\theta_j\\), \\(j=1,\\dots,q\\), are the moving average coefficients, while \\(p\\) and \\(q\\) are the autoregressive and moving average orders, respectively.\nIn certain cases, the function \\(g(\\cdot)\\) requires some transformation of the original series \\(Y_{t-j}\\) to avoid the non-existence of \\(g(Y_{t-j})\\) (Benjamin et al. 2003).\nThe generalized autoregressive moving average model (Equation D.1), GARMA(\\(p,q\\)), represents a flexible observation-driven modification of the classical Box–Jenkins methodology and GLMs for integer-valued time series. GARMA further advances the classical Gaussian ARMA model to a case where the distribution of the dependent variable is not only non-Gaussian but can be discrete. The dependent variable is assumed to belong to a conditional exponential family distribution given the past information of the process and thus the GARMA can be used to model a variety of discrete distributions (Benjamin et al. 2003). The GARMA model also extends the work of Zeger and Qaqish (1988), who proposed an autoregressive exponential family model, and Li (1994), who introduced its moving average counterpart.\n\n\n\n\n\n\nNoteExample: Insurance claims GARMA model\n\n\n\nHere, we revisit the insurance claims example from Section 9.5.\n\nCodelogconstant &lt;- 1\nInsurance &lt;- read.csv(\"data/insurance_weekly.csv\") %&gt;%\n    dplyr::select(Claims, Precipitation) %&gt;% \n    mutate(Precipitation_lag1 = dplyr::lag(Precipitation, 1),\n           Week = 1:nrow(.),\n           Year = rep(2002:2011, each = 52),\n           Claims_ln = log(Claims + logconstant))\nsummary(Insurance)\n\n#&gt;      Claims       Precipitation    Precipitation_lag1      Week    \n#&gt;  Min.   :  0.00   Min.   : 0.000   Min.   : 0.00      Min.   :  1  \n#&gt;  1st Qu.:  1.00   1st Qu.: 0.775   1st Qu.: 0.75      1st Qu.:131  \n#&gt;  Median :  3.00   Median : 3.800   Median : 3.80      Median :260  \n#&gt;  Mean   :  3.61   Mean   : 7.713   Mean   : 7.72      Mean   :260  \n#&gt;  3rd Qu.:  4.00   3rd Qu.:10.000   3rd Qu.:10.00      3rd Qu.:390  \n#&gt;  Max.   :170.00   Max.   :77.300   Max.   :77.30      Max.   :520  \n#&gt;                                    NA's   :1                       \n#&gt;       Year        Claims_ln    \n#&gt;  Min.   :2002   Min.   :0.000  \n#&gt;  1st Qu.:2004   1st Qu.:0.693  \n#&gt;  Median :2006   Median :1.386  \n#&gt;  Mean   :2006   Mean   :1.252  \n#&gt;  3rd Qu.:2009   3rd Qu.:1.609  \n#&gt;  Max.   :2011   Max.   :5.142  \n#&gt; \n\n\nFit a GARMA model relating the weekly number of insurance claims to the total precipitation during that and previous weeks. For fitting the model, you will need the gamlss.util package, which is archived on CRAN. Follow the link to download the latest version, then install it from the archive.\n\n# The model function doesn't accept NAs, so remove them\nInsurance_noNA &lt;- na.omit(Insurance)\n\nlibrary(gamlss.util)\nm00zip &lt;- garmaFit(Claims ~ Precipitation + Week + Precipitation_lag1\n                   ,family = ZIP\n                   ,data = Insurance_noNA)\n\n#&gt; deviance of linear model=  3014\n\nm00nbi &lt;- garmaFit(Claims ~ Precipitation + Week + Precipitation_lag1\n                   ,family = NBI\n                   ,data = Insurance_noNA)\n\n#&gt; deviance of linear model=  2324\n\nAIC(m00zip, m00nbi)\n\n#&gt;        df  AIC\n#&gt; m00nbi  5 2334\n#&gt; m00zip  5 3024\n\n\nBased on the smallest AIC, select negative binomial distribution (NBI) for the GARMA model. Obtain ACF and PACF plots of the model residuals to select ARMA order (Figure D.1).\n\nCodep1 &lt;- forecast::ggAcf(m00nbi$residuals) +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np2 &lt;- forecast::ggAcf(m00nbi$residuals, type = \"partial\") +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure D.1: ACF and PACF plots of residuals of the base model GARMA(0,0) based on NBI distribution.\n\n\n\n\nBased on the observed ACF and PACF patterns (Figure D.1), an appropriate model for the temporal dependence could be ARMA(1,0), ARMA(3,0), or ARMA(1,1), among the most parsimonious options. Alternatively, we can select the orders based on AIC:\n\n(res_arma &lt;- forecast::auto.arima(m00nbi$residuals, \n                                  d = 0, D = 0, \n                                  stepwise = FALSE))\n\n#&gt; Series: m00nbi$residuals \n#&gt; ARIMA(1,0,2) with zero mean \n#&gt; \n#&gt; Coefficients:\n#&gt;         ar1     ma1     ma2\n#&gt;       0.876  -0.679  -0.089\n#&gt; s.e.  0.067   0.080   0.051\n#&gt; \n#&gt; sigma^2 = 0.879:  log likelihood = -702\n#&gt; AIC=1411   AICc=1411   BIC=1428\n\n# Extract the orders, see the value 'arma' in ?stats::arima\np &lt;- res_arma$arma[1]\nq &lt;- res_arma$arma[2]\n\nRefit the GARMA model specifying these orders. Then verify that the temporal dependence in residuals was removed (Figure D.2), and assess other assumptions (Figure D.3), including homogeneity and normality of the quantile residuals.\n\nset.seed(12345)\nm10nbi &lt;- garmaFit(Claims ~ Precipitation + Week + Precipitation_lag1\n                   ,order = c(1, 0)\n                   ,family = NBI\n                   ,data = Insurance_noNA)\n\n#&gt; deviance of linear model=  2324 \n#&gt; deviance of  garma model=  2306\n\nsummary(m10nbi)\n\n#&gt; \n#&gt; Family:  c(\"NBI\", \"Negative Binomial type I\") \n#&gt; Fitting method: \"nlminb\" \n#&gt; \n#&gt; Call:  garmaFit(formula = Claims ~ Precipitation + Week + Precipitation_lag1,  \n#&gt;     order = c(1, 0), data = Insurance_noNA, family = NBI) \n#&gt; \n#&gt; \n#&gt; Coefficient(s):\n#&gt;                            Estimate  Std. Error  t value   Pr(&gt;|t|)    \n#&gt; beta.(Intercept)        0.491446753 0.104282235  4.71266 2.4450e-06 ***\n#&gt; beta.Precipitation      0.024214583 0.003281726  7.37861 1.5987e-13 ***\n#&gt; beta.Week               0.001578533 0.000297431  5.30722 1.1131e-07 ***\n#&gt; beta.Precipitation_lag1 0.012197659 0.003880431  3.14338  0.0016701 ** \n#&gt; phi                     0.091454371 0.034440316  2.65545  0.0079204 ** \n#&gt; sigma                   0.489354010 0.057665066  8.48614 &lt; 2.22e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Degrees of Freedom for the fit: 6 Residual Deg. of Freedom   513 \n#&gt; Global Deviance:     2306 \n#&gt;             AIC:     2318 \n#&gt;             SBC:     2344\n\n\n\nCodep1 &lt;- forecast::ggAcf(m10nbi$residuals) +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np2 &lt;- forecast::ggAcf(m10nbi$residuals, type = \"partial\") +\n    ggtitle(\"\") +\n    xlab(\"Lag (weeks)\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\nFigure D.2: ACF and PACF plots of the GARMA(1,0) model residuals based on NBI distribution.\n\n\n\n\n\nCodeplot(m10nbi, ts = FALSE)\n\n#&gt; ******************************************************************\n#&gt;   Summary of the Randomised Quantile Residuals\n#&gt;                            mean   =  0.0569 \n#&gt;                        variance   =  0.854 \n#&gt;                coef. of skewness  =  0.532 \n#&gt;                coef. of kurtosis  =  7.32 \n#&gt; Filliben correlation coefficient  =  0.98 \n#&gt; ******************************************************************\n\n\n\n\n\n\n\nFigure D.3: Default diagnostics of the GARMA(1,0) model residuals based on NBI distribution.\n\n\n\n\nThe residual diagnostics look somewhat adequate. The issue is with the residuals showing high kurtosis (recall that kurtosis of normal distribution equals 3) and some outliers in the right tail. Also, consider Section 9.5 for another analysis of this dataset.\n\n\n\n\n\n\nBenjamin MA, Rigby RA, Stasinopoulos DM (2003) Generalized autoregressive moving average models. Journal of the American Statistical Association 98:214–223. https://doi.org/10.1198/016214503388619238\n\n\nLi WK (1994) Time series models based on generalized linear models: Some further results. Biometrics 50:506–511. https://doi.org/10.2307/2533393\n\n\nZeger SL, Qaqish B (1988) Markov regression models for time series: A quasi-likelihood approach. Biometrics 44:1019–1031. https://doi.org/10.2307/2531732",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Generalized autoregressive moving average (GARMA) models</span>"
    ]
  },
  {
    "objectID": "s_practice.html",
    "href": "s_practice.html",
    "title": "Appendix E — Practice exercises",
    "section": "",
    "text": "E.1 Intro practice\nAnswer whether each of these statements is true or false.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Practice exercises</span>"
    ]
  },
  {
    "objectID": "s_practice.html#sec-intropractice",
    "href": "s_practice.html#sec-intropractice",
    "title": "Appendix E — Practice exercises",
    "section": "",
    "text": "The highest autocorrelation is observed when each next observation is exactly the same as the previous.\n‘Time series is autocorrelated’ means there is a trend.\nAutocorrelation goes away if we smooth the data, for example, with a moving average.\n‘Random variables \\(X\\) and \\(Y\\) are uncorrelated’ means \\(X\\) and \\(Y\\) are independent.\nTime series is an uninterrupted sequence of observations. Missing observations break the sequence into multiple separate time series.\nThe most appropriate statistical tool to detect a trend is the simple Student’s \\(t\\)-test.\nIf there is no autocorrelation at the first lag, i.e., \\(\\mathrm{cor}(X_t, X_{t-1}) = 0\\), then \\(\\mathrm{cor}(X_t, X_{t-2}) = 0\\).\nPrediction mean absolute error (PMAE) measures the quality of point forecasts, whereas prediction mean squared error (PMSE) measures the quality of interval forecasts.\nIf a time series \\(X_t\\) (\\(t = 1, \\dots, T\\)) is stationary, then all predictions for times \\(T+1\\), \\(T+2\\), \\(\\dots\\) are the same.\nWhite noise is a sequence of weakly correlated random variables.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Practice exercises</span>"
    ]
  },
  {
    "objectID": "s_practice.html#sec-acfpractice",
    "href": "s_practice.html#sec-acfpractice",
    "title": "Appendix E — Practice exercises",
    "section": "\nE.2 ARMA practice",
    "text": "E.2 ARMA practice\nBelow are several examples of time series with their ACF and PACF plots. For each example time series, use the plots to decide whether an ARMA(\\(p, q\\)) model is appropriate, and if so, suggest the orders \\(p\\) and \\(q\\). Use Table 4.1 for help.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Practice exercises</span>"
    ]
  },
  {
    "objectID": "s_practice.html#sec-trendpractice",
    "href": "s_practice.html#sec-trendpractice",
    "title": "Appendix E — Practice exercises",
    "section": "\nE.3 Trend practice",
    "text": "E.3 Trend practice\nAnswer whether each of these statements is true or false.\n\nIf ACF values at the first ten lags are statistically significant, then the time series is not stationary.\nIf \\(X_t\\) is an ARIMA\\((p,d,q)\\) process, then \\((1-B)^d X_t\\) is an ARMA\\((p,q)\\) process.\nSlowly decaying ACF is a sign of nonstationarity.\nIf the hypothesis \\(H_1\\) of a linear trend was accepted for the series \\(U_t\\), \\(t = 1, \\dots, T\\), it will be also accepted for the subsets \\(U_{t'}\\), where \\(t' = j, \\dots, k\\); \\(j&lt;k\\), and \\(j,k &lt; T\\).\nUnit root tests can be applied to determine the appropriate order of differencing \\(d\\).\nA time series that exhibits a quadratic-looking trend can be made stationary (detrended) using the Box–Cox transformation with the power parameter \\(\\lambda = 2\\).\nARIMA(0,1,0) model is a random walk.\nFor the backshift operator \\(B\\), \\((1-B)^dX_t = (1-B^d)X_t\\).\nA linear time trend can be eliminated by differencing the time series once or twice.\nTrend functions (e.g., \\(X_t = 0.35 + 0.11t + \\epsilon_t\\), where \\(\\epsilon_t\\) are uncorrelated errors) express the changes in the process \\(X_t\\) caused by time.\nTime series should be differenced just enough times to remove a stochastic trend. Differencing too many times leads to problems.\nAutocorrelation in observations affects results of the \\(t\\)-test and Mann–Kendall test.\nThe Mann–Kendall test focuses on a more general class of trends than the \\(t\\)-test does.\nThe null hypothesis of the augmented Dickey–Fuller test is no unit root (stationarity).\nARIMA\\((p,d,q)\\) is a difference-stationary process.\nBootstrapping allows us to replicate the finite-sample distribution of the test statistic.\nTo detrend a time series, the difference operator should be applied with the same lag(s) at which the sample ACF has statistically significant values.\nOne of the correct ways to run regression on the time series \\(Y_t\\) and \\(X_t\\) with trends is to detrend these time series before fitting the regression model.\nIn practice, it is seldom necessary to go beyond second-order differences for detrending a time series.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Practice exercises</span>"
    ]
  }
]